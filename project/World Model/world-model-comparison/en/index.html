<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="author" content="Pebblous Data Communication Team">
    <meta name="language" content="English">
    <meta http-equiv="content-language" content="en">
    <meta name="robots" content="index, follow">
    <meta name="revisit-after" content="7 days">

    <title>Three World Models for Next-Generation AI: Jeff Hawkins, Yann LeCun, Fei-Fei Li | Pebblous</title>
    <meta name="description" content="A comparative analysis of Jeff Hawkins' Thousand Brains Theory, Yann LeCun's JEPA, and Fei-Fei Li's Spatial Intelligence. Charting the direction of next-generation AI world models that go beyond the limits of LLMs to understand the physical world.">
    <meta name="keywords" content="World Model, Jeff Hawkins, Yann LeCun, Fei-Fei Li, Thousand Brains Theory, JEPA, Joint Embedding Predictive Architecture, Spatial Intelligence, World Labs, Numenta, Meta AI, Cortical Column, Reference Frame, SDR, Sparse Distributed Representations, Marble, RTFM, Robotics, AGI, Artificial General Intelligence, LLM Limitations, Generative AI, Digital Twin, Physical AI, Pebblous, Energy-Based Models, EBM, Sensorimotor Integration, Predictive Coding">

    <link rel="canonical" href="https://blog.pebblous.ai/project/World%20Model/world-model-comparison/en/">
    <link rel="alternate" hreflang="ko" href="https://blog.pebblous.ai/project/World%20Model/world-model-comparison/ko/">
    <link rel="alternate" hreflang="en" href="https://blog.pebblous.ai/project/World%20Model/world-model-comparison/en/">
    <link rel="alternate" hreflang="x-default" href="https://blog.pebblous.ai/project/World%20Model/world-model-comparison/en/">

    <!-- Open Graph -->
    <meta property="og:title" content="Three World Models for Next-Generation AI: Jeff Hawkins, Yann LeCun, Fei-Fei Li">
    <meta property="og:description" content="A comparative analysis of the Thousand Brains Theory, JEPA, and Spatial Intelligence -- charting the direction of next-generation AI world models beyond LLMs.">
    <meta property="og:type" content="article">
    <meta property="og:url" content="https://blog.pebblous.ai/project/World%20Model/world-model-comparison/en/">
    <meta property="og:image" content="https://blog.pebblous.ai/project/World%20Model/image/world-model-comparison.png">
    <meta property="og:image:width" content="1200">
    <meta property="og:image:height" content="630">
    <meta property="og:site_name" content="Pebblous Blog">
    <meta property="og:locale" content="en_US">

    <!-- Twitter Card -->
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="Three World Models for Next-Generation AI">
    <meta name="twitter:description" content="Comparative analysis of world model theories by Jeff Hawkins, Yann LeCun, and Fei-Fei Li">
    <meta name="twitter:image" content="https://blog.pebblous.ai/project/World%20Model/image/world-model-comparison.png">

    <!-- Favicon -->
    <link rel="icon" href="/image/favicon.ico" sizes="any">
    <link rel="icon" href="/image/Pebblous_BM_Orange_RGB.png" type="image/png">

    <!-- Google Tag Manager -->
    <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
    new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
    j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
    'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','GTM-57L9F58B');</script>

    <!-- JSON-LD: Article -->
    <script type="application/ld+json">
    {
        "@context": "https://schema.org",
        "@type": "Article",
        "headline": "Three World Models for Next-Generation AI: Jeff Hawkins, Yann LeCun, Fei-Fei Li",
        "description": "A comparative analysis of Jeff Hawkins' Thousand Brains Theory, Yann LeCun's JEPA, and Fei-Fei Li's Spatial Intelligence. Charting the direction of next-generation AI world models beyond LLMs.",
        "url": "https://blog.pebblous.ai/project/World%20Model/world-model-comparison/en/",
        "datePublished": "2026-01-02",
        "dateModified": "2026-01-12",
        "author": {
            "@type": "Organization",
            "name": "Pebblous",
            "url": "https://www.pebblous.ai"
        },
        "publisher": {
            "@type": "Organization",
            "name": "Pebblous",
            "logo": {
                "@type": "ImageObject",
                "url": "https://www.pebblous.ai/image/Pebblous_BM_Orange_RGB.png"
            }
        }
    }
    </script>

    <!-- Tailwind CSS -->
    <link rel="stylesheet" href="/styles/tailwind-build.css">

    <!-- Fonts -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Noto+Sans+KR:wght@300;400;500;700&display=swap" rel="stylesheet">

    <!-- Common Styles -->
    <link rel="stylesheet" href="/styles/common-styles.css?v=20260107">

    <style>
        :root {
            --bg-primary: #ffffff;
            --bg-secondary: #f8fafc;
            --bg-card: rgba(255, 255, 255, 0.95);
            --text-primary: #0f172a;
            --text-secondary: #334155;
            --text-muted: #64748b;
            --accent-color: #F86825;
            --teal-color: #0d9488;
            --border-color: #e2e8f0;
        }

        [data-theme="dark"] {
            --bg-primary: #0f172a;
            --bg-secondary: #1e293b;
            --bg-card: rgba(30, 41, 59, 0.95);
            --text-primary: #f1f5f9;
            --text-secondary: #cbd5e1;
            --text-muted: #94a3b8;
            --accent-color: #F86825;
            --teal-color: #14b8a6;
            --border-color: #334155;
        }

        [data-theme="beige"] {
            --bg-primary: #f5f1e8;
            --bg-secondary: #ebe3d5;
            --bg-card: rgba(245, 241, 232, 0.95);
            --text-primary: #2d2a26;
            --text-secondary: #5a534a;
            --text-muted: #78716c;
            --accent-color: #F86825;
            --teal-color: #0d9488;
            --border-color: #d6cec0;
        }

        body {
            font-family: 'Noto Sans KR', sans-serif;
            font-size: 18px;
            letter-spacing: -0.01em;
            background-color: var(--bg-primary);
            color: var(--text-primary);
            transition: background-color 0.3s ease, color 0.3s ease;
            scroll-behavior: smooth;
        }

        /* Share Button Styles */
        .share-container {
            display: flex;
            gap: 1rem;
            align-items: center;
            justify-content: center;
            flex-wrap: wrap;
        }

        .share-label {
            font-size: 0.875rem;
            color: #94a3b8;
            font-weight: 500;
        }

        .share-btn {
            display: inline-flex;
            align-items: center;
            gap: 0.375rem;
            padding: 0;
            background: none;
            border: none;
            color: #64748b;
            cursor: pointer;
            transition: color 0.2s;
            font-size: 0.875rem;
            text-decoration: none;
        }

        .share-btn:hover {
            color: #F86825;
        }

        .share-btn svg {
            width: 1.25rem;
            height: 1.25rem;
        }

        /* Typography enhancements for academic content */
        main p {
            line-height: 1.85;
            margin-bottom: 1.25rem;
        }

        main h3 {
            margin-top: 2.5rem;
        }

        main h4 {
            margin-top: 2rem;
        }

        main ul, main ol {
            margin-bottom: 1.25rem;
        }

        main li {
            margin-bottom: 0.5rem;
            line-height: 1.75;
        }

        /* Table Styles */
        main table {
            width: 100%;
            border-collapse: collapse;
            margin: 1.5rem 0;
            font-size: 0.95rem;
        }

        main table th {
            background-color: var(--bg-secondary);
            font-weight: 600;
            text-align: left;
            padding: 0.75rem 1rem;
            border: 1px solid var(--border-color);
        }

        main table td {
            padding: 0.75rem 1rem;
            border: 1px solid var(--border-color);
            vertical-align: top;
        }

        main table tbody tr:hover {
            background-color: rgba(248, 104, 37, 0.05);
        }

        /* Blockquote */
        main blockquote {
            border-left: 4px solid var(--accent-color);
            padding-left: 1.5rem;
            margin: 1.5rem 0;
            font-style: italic;
            color: var(--text-secondary);
        }

        /* Reference links */
        .ref-link {
            color: var(--teal-color);
            text-decoration: none;
            font-weight: 500;
        }

        .ref-link:hover {
            text-decoration: underline;
        }

        /* Scholar Cards */
        .scholar-card {
            background: var(--bg-card);
            border: 1px solid var(--border-color);
            border-radius: 0.75rem;
            padding: 1.5rem;
            transition: all 0.3s cubic-bezier(0.4, 0, 0.2, 1);
            position: relative;
            overflow: hidden;
        }

        .scholar-card::before {
            content: '';
            position: absolute;
            top: 0;
            left: 0;
            width: 4px;
            height: 100%;
            transition: background 0.3s;
        }

        .scholar-card.hawkins::before { background: linear-gradient(180deg, #f97316, #ea580c); }
        .scholar-card.lecun::before { background: linear-gradient(180deg, #0d9488, #0f766e); }
        .scholar-card.feifei::before { background: linear-gradient(180deg, #8b5cf6, #7c3aed); }

        .scholar-card:hover {
            transform: translateY(-4px);
            box-shadow: 0 12px 24px rgba(0, 0, 0, 0.1);
        }

        /* Key Concept Box */
        .key-concept-box {
            background: var(--bg-secondary);
            border-left: 4px solid var(--teal-color);
            border-radius: 0 0.5rem 0.5rem 0;
            padding: 1.25rem 1.5rem;
            margin: 1.5rem 0;
        }

        .key-concept-box.orange {
            border-left-color: var(--accent-color);
        }

        .key-concept-box.purple {
            border-left-color: #8b5cf6;
        }

        /* Process Flow Cards */
        .process-flow {
            display: flex;
            flex-direction: column;
            gap: 1rem;
            margin: 1.5rem 0;
        }

        @media (min-width: 768px) {
            .process-flow {
                flex-direction: row;
            }
        }

        .process-step {
            flex: 1;
            background: var(--bg-card);
            border: 1px solid var(--border-color);
            border-radius: 0.5rem;
            padding: 1rem;
            position: relative;
            text-align: center;
        }

        .process-step .step-number {
            position: absolute;
            top: -0.75rem;
            left: 50%;
            transform: translateX(-50%);
            width: 1.5rem;
            height: 1.5rem;
            background: var(--teal-color);
            color: white;
            border-radius: 50%;
            font-size: 0.75rem;
            font-weight: 700;
            display: flex;
            align-items: center;
            justify-content: center;
        }

        .process-step .step-title {
            font-weight: 600;
            margin-bottom: 0.5rem;
            color: var(--text-primary);
        }

        .process-step .step-desc {
            font-size: 0.875rem;
            color: var(--text-secondary);
        }

        /* Fusion Diagram */
        .fusion-diagram {
            display: grid;
            grid-template-columns: repeat(3, 1fr);
            gap: 1rem;
            margin: 1.5rem 0;
        }

        @media (max-width: 767px) {
            .fusion-diagram {
                grid-template-columns: 1fr;
            }
        }

        .fusion-item {
            background: var(--bg-card);
            border: 2px solid var(--border-color);
            border-radius: 0.75rem;
            padding: 1.5rem;
            text-align: center;
            transition: all 0.3s;
        }

        .fusion-item:hover {
            border-color: var(--accent-color);
        }

        .fusion-item .fusion-icon {
            font-size: 2rem;
            margin-bottom: 0.75rem;
        }

        .fusion-item .fusion-label {
            font-size: 0.75rem;
            text-transform: uppercase;
            letter-spacing: 0.05em;
            color: var(--text-muted);
            margin-bottom: 0.25rem;
        }

        .fusion-item .fusion-title {
            font-weight: 700;
            font-size: 1.125rem;
            color: var(--text-primary);
            margin-bottom: 0.5rem;
        }

        .fusion-item .fusion-author {
            font-size: 0.875rem;
            color: var(--teal-color);
        }
    </style>
</head>

<body>
    <!-- Google Tag Manager (noscript) -->
    <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-57L9F58B"
    height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>

    <!-- Header Placeholder -->
    <div id="header-placeholder"></div>

    <!-- Main Content -->
    <div class="container mx-auto px-4 sm:px-6 lg:px-8 py-8 md:py-12 max-w-[1400px]">
        <div class="lg:flex lg:gap-8 lg:justify-center lg:items-start">

            <!-- TOC Sidebar -->
            <nav class="hidden lg:block lg:w-[240px] lg:shrink-0 sticky top-20 self-start">
                <h3 class="font-bold themeable-heading mb-4 text-lg">Table of Contents</h3>
                <ul id="toc-links" class="space-y-3 text-sm border-l-2 themeable-toc-border">
                    <li class="pl-4 hover:text-orange-500 transition-colors">
                        <a href="#executive-summary" class="themeable-text-secondary hover:text-orange-500">Executive Summary</a>
                    </li>
                    <li class="pl-4 hover:text-orange-500 transition-colors">
                        <a href="#intro" class="themeable-text-secondary hover:text-orange-500">1. Introduction: Beyond Language</a>
                    </li>
                    <li class="pl-4 hover:text-orange-500 transition-colors">
                        <a href="#hawkins" class="themeable-text-secondary hover:text-orange-500">2. Jeff Hawkins: Thousand Brains</a>
                    </li>
                    <li class="pl-4 hover:text-orange-500 transition-colors">
                        <a href="#lecun" class="themeable-text-secondary hover:text-orange-500">3. Yann LeCun: JEPA</a>
                    </li>
                    <li class="pl-4 hover:text-orange-500 transition-colors">
                        <a href="#feifei" class="themeable-text-secondary hover:text-orange-500">4. Fei-Fei Li: Spatial Intelligence</a>
                    </li>
                    <li class="pl-4 hover:text-orange-500 transition-colors">
                        <a href="#comparison" class="themeable-text-secondary hover:text-orange-500">5. Comparative Analysis</a>
                    </li>
                    <li class="pl-4 hover:text-orange-500 transition-colors">
                        <a href="#conclusion" class="themeable-text-secondary hover:text-orange-500">6. Conclusion: Future of World Models</a>
                    </li>
                    <li class="pl-4 hover:text-orange-500 transition-colors">
                        <a href="#faq" class="themeable-text-secondary hover:text-orange-500">FAQ</a>
                    </li>
                    <li class="pl-4 hover:text-orange-500 transition-colors">
                        <a href="#references" class="themeable-text-secondary hover:text-orange-500">References</a>
                    </li>
                    <li class="pl-4 hover:text-orange-500 transition-colors">
                        <a href="#pdf-download" class="themeable-text-secondary hover:text-orange-500">PDF Download</a>
                    </li>
                </ul>
            </nav>

            <!-- Main Article -->
            <main class="max-w-[800px] px-4 sm:px-6">
                <!-- Hero Section -->
                <header class="text-left mb-16">
                    <h1 id="page-h1-title" class="text-4xl md:text-5xl font-bold themeable-heading mb-6 leading-tight" style="line-height: 1.4;">
                        <!-- Automatically filled by PebblousPage.applyConfig() -->
                    </h1>

                    <div class="flex items-center gap-4 text-sm themeable-muted mb-4">
                        <span id="publish-date"></span>
                        <span>•</span>
                        <span id="publisher"></span>
                        <span>•</span>
                        <span id="reading-time">Reading time: approx. 20 min</span>
                        <span>•</span>
                        <a href="../ko/" class="text-orange-400 hover:text-orange-300 transition-colors">한국어</a>
                    </div>

                    <div id="share-buttons-placeholder" class="mb-8"></div>
                </header>

                <!-- Executive Summary -->
                <section id="executive-summary" class="mb-16 fade-in-card">
                    <h2 class="text-3xl font-bold themeable-heading mb-8">Executive Summary</h2>
                    <div class="key-insight">
                        <p class="themeable-text leading-relaxed mb-4">
                            Large Language Models (LLMs) have demonstrated remarkable achievements in text generation, yet they face a fundamental limitation: they do not truly understand the physical world. This report provides a comparative analysis of the theories of three leading scholars who are driving 'World Models,' the emerging core of next-generation AI beyond LLMs.
                        </p>
                        <p class="themeable-text leading-relaxed mb-4">
                            Jeff Hawkins reverse-engineers the neocortical structure of the brain, presenting a biological blueprint of distributed modeling across 150,000 cortical columns and reference-frame-based information storage. Yann LeCun critiques the inefficiency of pixel generation and proposes the JEPA architecture with energy-based models for efficient reasoning in abstract representation spaces. Fei-Fei Li combines 3D spatial understanding and generation into 'Spatial Intelligence,' leading a Sim2Real revolution in robotics through Marble and RTFM.
                        </p>
                        <p class="themeable-text leading-relaxed">
                            The three theories are not mutually exclusive but complementary. When Hawkins' structural framework, LeCun's reasoning engine, and Li's environment generation converge, we move one step closer to a true AGI that understands and acts upon physical reality.
                        </p>
                    </div>
                </section>

                <!-- Section 1: Introduction -->
                <section id="intro" class="mb-12">
                    <h2 class="text-3xl font-bold themeable-heading mb-6 pb-3 border-b-2 themeable-border">
                        1. Introduction: Expanding Beyond Language to the World
                    </h2>

                    <p class="themeable-text leading-relaxed">
                        Over the past decade, artificial intelligence (AI) technology has reached an unprecedented inflection point in intellectual history through the dramatic advancement of deep neural networks, particularly Large Language Models (LLMs). Generative AI, exemplified by OpenAI's ChatGPT series, has demonstrated near-human or even superhuman performance in understanding and generating text. However, the fundamental limitations of this language-centric AI are also becoming increasingly clear. <strong>Text is merely a symbol that describes the world -- it is not the physical world itself</strong>. While LLMs excel at predicting the next word by learning statistical correlations in vast text data, they lack causal inference -- understanding "why" things happen -- and spatial understanding of how their actions might affect the physical environment. The so-called "hallucination" phenomenon is an inevitable byproduct of AI relying on probabilistic plausibility rather than grounding itself in the laws of the real world <a href="#ref-1" class="ref-link">[1]</a>.
                    </p>

                    <p class="themeable-text leading-relaxed">
                        Against this backdrop, the frontier of AI research is rapidly shifting from 'language models' to '<strong>World Models</strong>.' A world model is a cognitive framework that enables an agent (human or machine) to internally simulate the structure and operating principles of the external environment, predict the outcomes of actions, and formulate plans. It goes beyond merely classifying or generating data -- it is the core mechanism that enables the essence of intelligence: 'understanding' and 'survival.'
                    </p>

                    <p class="themeable-text leading-relaxed">
                        This report provides an in-depth analysis of the theories and technical approaches of three world-renowned scholars leading world model research: <strong>Jeff Hawkins</strong> (Numenta), <strong>Yann LeCun</strong> (Meta), and <strong>Fei-Fei Li</strong> (World Labs). Jeff Hawkins reverse-engineers the mechanisms by which the human neocortex models the world, grounded in neuroscientific findings. Yann LeCun critiques the inefficiency of current generative AI and proposes a cognitive architecture emphasizing prediction and planning in abstract representation space. Fei-Fei Li aims to fuse the digital and physical worlds through 'Spatial Intelligence' -- the ability to construct and manipulate 3D space beyond mere visual recognition.
                    </p>

                    <p class="themeable-text leading-relaxed">
                        These three perspectives may seem contradictory, but in reality they offer answers at different layers toward the grand goal of Artificial General Intelligence (AGI). This report aims to chart the direction for next-generation AI by closely comparing their theoretical backgrounds, the technical details of their architectures, and the blueprints for future AI that each model presents.
                    </p>

                    <!-- Three Scholars Cards -->
                    <div class="grid grid-cols-1 md:grid-cols-3 gap-4 mt-8">
                        <div class="scholar-card hawkins">
                            <h4 class="font-bold themeable-heading text-lg mb-2">Jeff Hawkins</h4>
                            <p class="text-sm text-orange-500 font-medium mb-2">Founder of Numenta</p>
                            <p class="text-sm themeable-text-secondary">Provides the structural foundation for AGI by reverse-engineering the neocortex through the neuroscience-based <strong>Thousand Brains Theory</strong></p>
                        </div>
                        <div class="scholar-card lecun">
                            <h4 class="font-bold themeable-heading text-lg mb-2">Yann LeCun</h4>
                            <p class="text-sm text-teal-500 font-medium mb-2">Chief AI Scientist at Meta</p>
                            <p class="text-sm themeable-text-secondary">Implements efficient reasoning and planning through <strong>JEPA</strong>, emphasizing prediction in abstract representation space</p>
                        </div>
                        <div class="scholar-card feifei">
                            <h4 class="font-bold themeable-heading text-lg mb-2">Fei-Fei Li</h4>
                            <p class="text-sm text-purple-500 font-medium mb-2">Founder of World Labs</p>
                            <p class="text-sm themeable-text-secondary">Driving digital-physical world convergence through <strong>Spatial Intelligence</strong> via 3D spatial understanding and generation</p>
                        </div>
                    </div>
                </section>

                <!-- Section 2: Jeff Hawkins -->
                <section id="hawkins" class="mb-12">
                    <h2 class="text-3xl font-bold themeable-heading mb-6 pb-3 border-b-2 themeable-border">
                        2. Jeff Hawkins and Numenta: Biological Realism and the Thousand Brains Theory
                    </h2>

                    <!-- Key Concept Box -->
                    <div class="key-concept-box orange">
                        <p class="font-semibold themeable-heading mb-1">Key Concept: Thousand Brains Theory</p>
                        <p class="text-sm themeable-text-secondary">The brain is not a single hierarchical processing system but a distributed system where <strong>150,000 independent cortical columns</strong> each form their own world models and reach consensus through 'voting.' All information is stored through <strong>reference frames (coordinate systems)</strong>, which apply not only to physical objects but also to abstract concepts.</p>
                    </div>

                    <p class="themeable-text leading-relaxed">
                        Jeff Hawkins is famous as the founder of Palm Computing, but his true passion over the past several decades has been to elucidate how the brain works and to build genuine intelligence based on those principles. His company, <strong>Numenta</strong>, has continued researching the translation of neuroscientific discoveries into mathematical algorithms, culminating in the '<strong>Thousand Brains Theory</strong>.'
                    </p>

                    <h3 class="text-2xl font-semibold themeable-heading mt-8 mb-4">2.1 The Core of the Thousand Brains Theory</h3>

                    <p class="themeable-text leading-relaxed">
                        Conventional neuroscience and AI theory assumed that the brain processes sensory input hierarchically to form a single object model at the top level. That is, visual information passes through V1, V2, V4, and so on until reaching the IT (Inferior Temporal) cortex to finally recognize an object as a 'cup.' However, Hawkins rejects this hierarchical integration hypothesis and proposes a <strong>Distributed Modeling</strong> hypothesis <a href="#ref-3" class="ref-link">[3]</a>.
                    </p>

                    <h4 class="text-xl font-semibold themeable-heading mt-6 mb-3">2.1.1 Independence of Cortical Columns</h4>

                    <p class="themeable-text leading-relaxed">
                        According to Hawkins' theory, each of the approximately 150,000 <strong>cortical columns</strong> that make up the neocortex is a complete sensorimotor modeling system. Rather than only specific brain regions handling modeling, every cortical column independently learns models of the world based on its sensory input. For example, the five fingers holding a coffee cup each receive different sensory inputs, but the cortical columns connected to each finger independently attempt to form the model "this is a coffee cup" <a href="#ref-5" class="ref-link">[5]</a>.
                    </p>

                    <h4 class="text-xl font-semibold themeable-heading mt-6 mb-3">2.1.2 Voting Mechanism and Solving the Binding Problem</h4>

                    <p class="themeable-text leading-relaxed">
                        How then do tens of thousands of independent models form a single unified perception? Hawkins explains this through '<strong>voting</strong>.' Cortical columns exchange information with each other through long-range connections in the neocortex.
                    </p>

                    <!-- Voting Mechanism Flow -->
                    <div class="process-flow mt-6">
                        <div class="process-step">
                            <span class="step-number">1</span>
                            <p class="step-title mt-2">Sensory Input</p>
                            <p class="step-desc">Index finger: "smooth curve"<br>Thumb: "handle"</p>
                        </div>
                        <div class="process-step">
                            <span class="step-number">2</span>
                            <p class="step-title mt-2">Individual Judgment</p>
                            <p class="step-desc">Index column: Cup? Ball? Apple?<br>Thumb column: Cup? Kettle?</p>
                        </div>
                        <div class="process-step">
                            <span class="step-number">3</span>
                            <p class="step-title mt-2">Vote & Consensus</p>
                            <p class="step-desc">Common denominator = <strong>"Cup"</strong><br>-> Unified perception formed</p>
                        </div>
                    </div>

                    <p class="themeable-text leading-relaxed">
                        As shown above, each cortical column makes its own judgment, then exchanges information through long-range neural connections to reach consensus <a href="#ref-6" class="ref-link">[6]</a>.
                    </p>

                    <p class="themeable-text leading-relaxed">
                        This mechanism explains why the brain can function robustly even when damaged or in noisy environments. Even if some columns are damaged, accurate recognition remains possible through the voting of the remaining columns.
                    </p>

                    <h3 class="text-2xl font-semibold themeable-heading mt-8 mb-4">2.2 Reference Frames: The Coordinate System of Intelligence</h3>

                    <p class="themeable-text leading-relaxed">
                        The most revolutionary aspect of Hawkins' theory is the discovery that the fundamental format in which the brain stores and processes information is the '<strong>reference frame</strong>.' He argues that the brain's 'grid cells' and 'place cells' exist not only in the hippocampus but throughout the entire neocortex <a href="#ref-8" class="ref-link">[8]</a>.
                    </p>

                    <h4 class="text-xl font-semibold themeable-heading mt-6 mb-3">2.2.1 Allocentric Representation</h4>

                    <p class="themeable-text leading-relaxed">
                        Just as the structure of a room does not change when we move within it, the brain models objects using an <strong>object-centered (allocentric)</strong> coordinate system rather than an observer-centered one.
                    </p>

                    <ul class="list-disc pl-6 themeable-text-secondary mb-6">
                        <li><strong>What (Feature):</strong> The feature sensed by sensory organs.</li>
                        <li><strong>Where (Location):</strong> Where that feature is located within the object's reference frame (Relative Location).</li>
                    </ul>

                    <p class="themeable-text leading-relaxed">
                        Layer 4 (L4) of the cortical column receives sensory input, while Layer 6 (L6) processes the current sensor's location information. The combination of these two forms the knowledge that "this object has this feature at this location" <a href="#ref-8" class="ref-link">[8]</a>. Unlike conventional deep learning, which primarily learns pixel patterns (texture), this suggests that the brain fundamentally learns <strong>structure and geometry</strong>.
                    </p>

                    <h4 class="text-xl font-semibold themeable-heading mt-6 mb-3">2.2.2 Extension to Abstract Thought</h4>

                    <p class="themeable-text leading-relaxed">
                        Hawkins argues that this reference frame mechanism applies equally to understanding <strong>abstract concepts</strong> such as mathematics, language, and politics -- not just physical objects. Even the concept of democracy is stored in the brain as a structure where related sub-concepts occupy specific logical 'positions' and 'relationships' <a href="#ref-3" class="ref-link">[3]</a>.
                    </p>

                    <h3 class="text-2xl font-semibold themeable-heading mt-8 mb-4">2.3 Sensorimotor Integration and Learning</h3>

                    <p class="themeable-text leading-relaxed">
                        Hawkins states that "<em>the act of thinking itself is a form of movement</em>." We do not passively observe objects; instead, we move our eyes (saccade), touch with our hands, and actively gather information <a href="#ref-4" class="ref-link">[4]</a>.
                    </p>

                    <ul class="list-disc pl-6 themeable-text-secondary mb-6">
                        <li><strong>Prediction:</strong> The brain constantly predicts the next sensory input. If the prediction "when I move my finger to the right, I will feel the handle of the cup" is correct, the model is reinforced; if wrong, it is corrected (Predictive Coding) <a href="#ref-6" class="ref-link">[6]</a>.</li>
                        <li><strong>Action:</strong> Therefore, action cannot be separated from learning. In Hawkins' model, intelligence is fundamentally <strong>embodied</strong>, and the sensory and motor systems are integrated into a single loop <a href="#ref-5" class="ref-link">[5]</a>.</li>
                    </ul>

                    <h3 class="text-2xl font-semibold themeable-heading mt-8 mb-4">2.4 Sparse Distributed Representations (SDR)</h3>

                    <p class="themeable-text leading-relaxed">
                        At the foundation of Numenta's technology lies a data structure called <strong>SDR</strong>. It mimics the way neurons in the brain activate very sparsely. Out of 2,048 bits, only about 2% (40 bits) are set to 1 while the rest remain 0 <a href="#ref-11" class="ref-link">[11]</a>.
                    </p>

                    <ul class="list-disc pl-6 themeable-text-secondary mb-6">
                        <li><strong>Semantic Embedding:</strong> Each bit in an SDR carries meaning. The more overlapping bits two SDRs share, the more semantically similar they are.</li>
                        <li><strong>Robustness:</strong> Even if half of the 40 active bits are lost due to noise, the remaining 20 can still reconstruct the full pattern. This explains the remarkable noise tolerance of the biological brain <a href="#ref-12" class="ref-link">[12]</a>.</li>
                    </ul>
                </section>

                <!-- Section 3: Yann LeCun -->
                <section id="lecun" class="mb-12">
                    <h2 class="text-3xl font-bold themeable-heading mb-6 pb-3 border-b-2 themeable-border">
                        3. Yann LeCun and Meta: A Cognitive Architecture for Autonomous Machine Intelligence
                    </h2>

                    <!-- Key Concept Box -->
                    <div class="key-concept-box">
                        <p class="font-semibold themeable-heading mb-1">Key Concept: JEPA (Joint Embedding Predictive Architecture)</p>
                        <p class="text-sm themeable-text-secondary">Directly generating pixels or text is inefficient. JEPA performs predictions in an <strong>abstract representation space</strong>, discarding unnecessary details and learning only core causal relationships. Through Energy-Based Models (EBM), it can clearly predict multiple possible futures.</p>
                    </div>

                    <p class="themeable-text leading-relaxed">
                        Turing Award laureate and Meta's Chief AI Scientist, Yann LeCun is one of the most vocal critics of the auto-regressive approach used by LLMs that currently dominate the AI industry. He argues that to achieve true AGI, what is needed is not a text generation model but a '<strong>world model</strong>' capable of reasoning and planning, and he has proposed <strong>JEPA (Joint Embedding Predictive Architecture)</strong> as his solution.
                    </p>

                    <h3 class="text-2xl font-semibold themeable-heading mt-8 mb-4">3.1 Critique of Generative AI: "LLMs Are Not World Models"</h3>

                    <p class="themeable-text leading-relaxed">
                        LeCun argues that the approach of sequentially predicting text or pixels has reached a fundamental limit <a href="#ref-1" class="ref-link">[1]</a>.
                    </p>

                    <ul class="list-disc pl-6 themeable-text-secondary mb-6">
                        <li><strong>Inefficiency:</strong> Video generation models (e.g., Sora) may appear to simulate a 3D world, but this is merely statistical mimicry in pixel space. LeCun criticizes that "generating pixels to build a world model is as inefficient as computing every particle's position at the quantum level to learn physics" <a href="#ref-15" class="ref-link">[15]</a>.</li>
                        <li><strong>Error Accumulation:</strong> Auto-regressive models feed their generated output back as input, causing initial errors to amplify exponentially and producing hallucinations (drift) that diverge from reality <a href="#ref-16" class="ref-link">[16]</a>.</li>
                        <li><strong>Data Bandwidth Gap:</strong> Humans learn about the world by absorbing far more information through visual input (tens of megabytes per second) than through text (a few bytes per second). An AI trained solely on text cannot possess physical common sense <a href="#ref-17" class="ref-link">[17]</a>.</li>
                    </ul>

                    <h3 class="text-2xl font-semibold themeable-heading mt-8 mb-4">3.2 JEPA (Joint Embedding Predictive Architecture): Prediction in Abstract Space</h3>

                    <p class="themeable-text leading-relaxed">
                        LeCun's solution is to <strong>perform predictions in the representation space rather than the input space</strong> <a href="#ref-18" class="ref-link">[18]</a>.
                    </p>

                    <h4 class="text-xl font-semibold themeable-heading mt-6 mb-3">3.2.1 How JEPA Works</h4>

                    <p class="themeable-text leading-relaxed">
                        JEPA does not directly connect input <code>x</code> (current state) and <code>y</code> (future state).
                    </p>

                    <!-- JEPA Process Flow -->
                    <div class="process-flow mt-6 mb-6">
                        <div class="process-step">
                            <span class="step-number">1</span>
                            <p class="step-title mt-2">Encoder</p>
                            <p class="step-desc">Transforms inputs x, y into<br>abstract vectors <code class="text-xs">s_x</code>, <code class="text-xs">s_y</code></p>
                        </div>
                        <div class="process-step">
                            <span class="step-number">2</span>
                            <p class="step-title mt-2">Predictor</p>
                            <p class="step-desc"><code class="text-xs">s_x</code> + latent variable <code class="text-xs">z</code><br>→ predicts <code class="text-xs">s_y'</code></p>
                        </div>
                        <div class="process-step">
                            <span class="step-number">3</span>
                            <p class="step-title mt-2">Non-generative</p>
                            <p class="step-desc">Predicts only in abstract<br>space without pixel reconstruction</p>
                        </div>
                    </div>

                    <p class="themeable-text leading-relaxed text-sm">
                        The encoder removes unnecessary details (background noise, etc.) and preserves only important information (object position, momentum) <a href="#ref-19" class="ref-link">[19]</a>. The predictor forecasts future states based on the current state and latent variables, and crucially, JEPA operates solely in abstract space without reconstructing pixels.
                    </p>

                    <h4 class="text-xl font-semibold themeable-heading mt-6 mb-3">3.2.2 Energy-Based Models (EBM)</h4>

                    <p class="themeable-text leading-relaxed">
                        The theoretical foundation of LeCun's approach is EBM. Learning proceeds by minimizing the 'energy (incompatibility/cost)' between two states (input and prediction).
                    </p>

                    <ul class="list-disc pl-6 themeable-text-secondary mb-6">
                        <li><strong>Compatibility:</strong> If observed data <code>x</code> and <code>y</code> are physically compatible, a low energy is assigned; impossible situations receive high energy <a href="#ref-20" class="ref-link">[20]</a>.</li>
                        <li><strong>Uncertainty Management:</strong> The future is not deterministic (stochastic). JEPA uses latent variable <code>z</code> to model multiple possible futures. Instead of predicting the average of all pixels to produce a blurry image like generative models, it can predict multiple plausible future states as clear vectors depending on the value of <code>z</code> <a href="#ref-18" class="ref-link">[18]</a>.</li>
                    </ul>

                    <h3 class="text-2xl font-semibold themeable-heading mt-8 mb-4">3.3 Hierarchical Planning and Autonomous Agents</h3>

                    <p class="themeable-text leading-relaxed">
                        The autonomous intelligent system envisioned by LeCun consists of six modules <a href="#ref-18" class="ref-link">[18]</a>.
                    </p>

                    <ol class="list-decimal pl-6 themeable-text-secondary mb-6">
                        <li><strong>Configurator:</strong> Sets the task objective.</li>
                        <li><strong>Perception:</strong> Receives sensor input and estimates the current state.</li>
                        <li><strong>World Model:</strong> Predicts future states based on actions.</li>
                        <li><strong>Cost:</strong> Evaluates the value of states through intrinsic cost (e.g., collision avoidance) and a learned critic.</li>
                        <li><strong>Short-term Memory:</strong> Stores the current situation and plans.</li>
                        <li><strong>Actor:</strong> Generates action sequences that minimize cost.</li>
                    </ol>

                    <p class="themeable-text leading-relaxed">
                        Here, <strong>Hierarchical JEPA (H-JEPA)</strong> enables long-term planning by operating at different levels of abstraction. At the higher level, it handles macroscopic plans like "go to the airport," while at the lower level, it processes microscopic controls like "turn the steering wheel 10 degrees to the left" <a href="#ref-18" class="ref-link">[18]</a>. This is an attempt to implement human <strong>System 2</strong> thinking -- slow, deliberate planning capability -- in AI <a href="#ref-23" class="ref-link">[23]</a>.
                    </p>

                    <h3 class="text-2xl font-semibold themeable-heading mt-8 mb-4">3.4 I-JEPA and V-JEPA</h3>

                    <p class="themeable-text leading-relaxed">
                        Meta released <strong>I-JEPA</strong> for images and <strong>V-JEPA</strong> for video to validate this theory. I-JEPA masks portions of an image and predicts the 'semantic representation' of those parts using surrounding context, while V-JEPA predicts spatiotemporal features of unseen portions within the temporal flow of video. Both demonstrated exceptional sample efficiency in object recognition and understanding physical interactions without generating pixels <a href="#ref-20" class="ref-link">[20]</a>.
                    </p>
                </section>

                <!-- Section 4: Fei-Fei Li -->
                <section id="feifei" class="mb-12">
                    <h2 class="text-3xl font-bold themeable-heading mb-6 pb-3 border-b-2 themeable-border">
                        4. Fei-Fei Li and World Labs: The Fusion of Spatial Intelligence and 3D Generation
                    </h2>

                    <!-- Key Concept Box -->
                    <div class="key-concept-box purple">
                        <p class="font-semibold themeable-heading mb-1">Key Concept: Spatial Intelligence</p>
                        <p class="text-sm themeable-text-secondary">AI is proficient with 2D images and text but lacks <strong>3D spatial understanding and manipulation</strong>. World Labs' <strong>Marble</strong> generates physically plausible 3D environments from text/images, providing a <strong>Sim2Real</strong> environment where robots can learn safely.</p>
                    </div>

                    <p class="themeable-text leading-relaxed">
                        Fei-Fei Li is the figure who catalyzed the revolution in computer vision through ImageNet. She has now founded World Labs and championed the concept of <strong>'Spatial Intelligence'</strong> to expand the AI paradigm beyond "seeing" to "doing."
                    </p>

                    <h3 class="text-2xl font-semibold themeable-heading mt-8 mb-4">4.1 Spatial Intelligence: The Missing Piece of AI</h3>

                    <p class="themeable-text leading-relaxed">
                        Li points out that while current AI excels at language and 2D image generation, it is markedly deficient in the ability to understand and manipulate three-dimensional space. She characterizes this as "wordsmiths in the dark," highlighting the limitations of intelligence that lacks a sense of physical reality <a href="#ref-24" class="ref-link">[24]</a>.
                    </p>

                    <ul class="list-disc pl-6 themeable-text-secondary mb-6">
                        <li><strong>Definition:</strong> Spatial intelligence is the ability to connect "imagination, perception, and action." It encompasses understanding depth, distance, physical interactions, and temporal changes in 3D space <a href="#ref-26" class="ref-link">[26]</a>.</li>
                        <li><strong>Evolutionary Perspective:</strong> The intelligence of living organisms evolved through visual perception and environmental interaction. Just as the Cambrian Explosion began with the emergence of 'eyes,' the next evolution of AI will stem from the ability to understand 3D space <a href="#ref-24" class="ref-link">[24]</a>.</li>
                    </ul>

                    <h3 class="text-2xl font-semibold themeable-heading mt-8 mb-4">4.2 Marble: A Large World Model (LWM)</h3>

                    <p class="themeable-text leading-relaxed">
                        World Labs' first product, <strong>Marble</strong>, is a generative world model that takes various inputs such as text, images, and video to generate complete 3D interactive environments <a href="#ref-27" class="ref-link">[27]</a>.
                    </p>

                    <h4 class="text-xl font-semibold themeable-heading mt-6 mb-3">4.2.1 Explicit 3D Representation</h4>

                    <p class="themeable-text leading-relaxed">
                        Unlike LeCun's JEPA, which aims for internal abstract representations, Marble generates <strong>explicit 3D assets</strong> that users can explore and interact with.
                    </p>

                    <ul class="list-disc pl-6 themeable-text-secondary mb-6">
                        <li><strong>Gaussian Splats:</strong> Marble represents 3D scenes as collections of numerous semi-transparent particles (Gaussians), enabling real-time photorealistic rendering <a href="#ref-27" class="ref-link">[27]</a>.</li>
                        <li><strong>Meshes & Colliders:</strong> Beyond visual representation, Marble also generates collider meshes that physics engines can recognize. This means the generated world is not merely a background image, but a physical space where robots or game characters can walk around and collide with objects <a href="#ref-29" class="ref-link">[29]</a>.</li>
                    </ul>

                    <h4 class="text-xl font-semibold themeable-heading mt-6 mb-3">4.2.2 Innovation in Generation Technology: Separating Structure from Style</h4>

                    <p class="themeable-text leading-relaxed">
                        Marble takes an approach of separating structure from style to address the chronic problem of 'uncontrollability' in generative AI.
                    </p>

                    <ul class="list-disc pl-6 themeable-text-secondary mb-6">
                        <li><strong>Chisel (Sculpting Mode):</strong> Users place basic 3D structures (boxes, planes, etc.), and the AI overlays textures and details matching the text prompt <a href="#ref-27" class="ref-link">[27]</a>.</li>
                        <li><strong>Multimodal Input Integration:</strong> When multiple photos or videos are provided as input, they are stitched together and reconstructed into a single coherent 3D space. This has immediate utility in real estate, architecture, game level design, and more <a href="#ref-31" class="ref-link">[31]</a>.</li>
                    </ul>

                    <h3 class="text-2xl font-semibold themeable-heading mt-8 mb-4">4.3 RTFM (Real-Time Frame Model): A Learned Renderer</h3>

                    <p class="themeable-text leading-relaxed">
                        The recently released <strong>RTFM</strong> demonstrates yet another approach different from Marble. Rather than generating explicit 3D geometry (meshes, etc.), it is a system where the <strong>neural network itself implicitly remembers and renders the 3D world</strong> <a href="#ref-32" class="ref-link">[32]</a>.
                    </p>

                    <ul class="list-disc pl-6 themeable-text-secondary mb-6">
                        <li><strong>How It Works:</strong> RTFM learns from video data and predicts new viewpoints of input images. It is a transformer-based auto-regressive model, but possesses prior knowledge of 3D Euclidean space, maintaining 'spatial consistency.'</li>
                        <li><strong>Context Juggling:</strong> To prevent memory explosion when exploring large-scale worlds, it dynamically retrieves only frames relevant to the current viewpoint, maintaining persistence. This technology is regarded as a bridge between LeCun's 'abstract simulation' and Li's 'visual generation.'</li>
                    </ul>

                    <h3 class="text-2xl font-semibold themeable-heading mt-8 mb-4">4.4 The Robotics and Sim2Real Revolution</h3>

                    <p class="themeable-text leading-relaxed">
                        Li's vision ultimately points toward robotics. Training robots in the real world is slow and dangerous. The <strong>Sim2Real</strong> strategy -- training robots in infinite, diverse 3D simulation environments (Sim) generated by Marble and then transferring them to reality (Real) -- is considered the key to solving the data bottleneck in robotics <a href="#ref-29" class="ref-link">[29]</a>.
                    </p>
                </section>

                <!-- Section 5: Comparison -->
                <section id="comparison" class="mb-12">
                    <h2 class="text-3xl font-bold themeable-heading mb-6 pb-3 border-b-2 themeable-border">
                        5. Comparative Analysis: Convergence and Divergence of Three Theories
                    </h2>

                    <p class="themeable-text leading-relaxed">
                        While all three scholars' theories share the goals of 'overcoming current AI limitations' and 'understanding the physical world,' they exhibit clear differences in their approaches and ontological perspectives.
                    </p>

                    <h3 class="text-2xl font-semibold themeable-heading mt-8 mb-4">5.1 Comparison Summary Table</h3>

                    <p class="themeable-text leading-relaxed mb-4">
                        The table below compares the world model theories of the three scholars across seven key criteria. It provides an at-a-glance view of how each approach differs in its core architecture, data representation, stance on generation, physics implementation, and ultimate goals.
                    </p>

                    <div class="overflow-x-auto mb-8 rounded-lg border themeable-border">
                        <table class="w-full border-collapse text-sm">
                            <thead>
                                <tr class="bg-gradient-to-r from-orange-500/10 via-teal-500/10 to-purple-500/10">
                                    <th class="px-4 py-3 text-left font-bold">Comparison Criteria</th>
                                    <th class="px-4 py-3 text-left font-bold text-orange-600">Jeff Hawkins</th>
                                    <th class="px-4 py-3 text-left font-bold text-teal-600">Yann LeCun</th>
                                    <th class="px-4 py-3 text-left font-bold text-purple-600">Fei-Fei Li</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td class="px-3 py-2 font-semibold">Core Theory</td>
                                    <td class="px-3 py-2"><strong>Thousand Brains Theory</strong></td>
                                    <td class="px-3 py-2"><strong>Autonomous Machine Intelligence</strong> (JEPA)</td>
                                    <td class="px-3 py-2"><strong>Spatial Intelligence</strong></td>
                                </tr>
                                <tr>
                                    <td class="px-3 py-2 font-semibold">Primary Architecture</td>
                                    <td class="px-3 py-2"><strong>Reference Frames</strong> & Cortical Columns (SDR)</td>
                                    <td class="px-3 py-2"><strong>Joint Embedding</strong> & Predictor</td>
                                    <td class="px-3 py-2"><strong>Generative 3D Models</strong> (Marble, RTFM)</td>
                                </tr>
                                <tr>
                                    <td class="px-3 py-2 font-semibold">Definition of World Model</td>
                                    <td class="px-3 py-2">Thousands of <strong>sensor-location models</strong> distributed across cortical columns</td>
                                    <td class="px-3 py-2">A state prediction simulator in abstract <strong>representation space</strong></td>
                                    <td class="px-3 py-2"><strong>Explicit 3D environments</strong> capable of physical interaction</td>
                                </tr>
                                <tr>
                                    <td class="px-3 py-2 font-semibold">Data Representation</td>
                                    <td class="px-3 py-2">Sparse Distributed Representations (SDR), Grid Cells</td>
                                    <td class="px-3 py-2">High-dimensional Embedding Vectors (Latent Representations)</td>
                                    <td class="px-3 py-2">Gaussian Splats, Meshes, Neural Rendering</td>
                                </tr>
                                <tr>
                                    <td class="px-3 py-2 font-semibold">Generation</td>
                                    <td class="px-3 py-2">Generation for <strong>internal prediction</strong> (Predictive Coding)</td>
                                    <td class="px-3 py-2"><strong>Against</strong> pixel generation (inefficient, hallucination risk)</td>
                                    <td class="px-3 py-2"><strong>Active generation</strong> (Generative AI) for spatial construction</td>
                                </tr>
                                <tr>
                                    <td class="px-3 py-2 font-semibold">Physics Implementation</td>
                                    <td class="px-3 py-2"><strong>Structurally</strong> acquired through sensorimotor learning</td>
                                    <td class="px-3 py-2"><strong>Intuitively</strong> learned through energy function minimization</td>
                                    <td class="px-3 py-2">Integration with physics engines or 3D <strong>simulation</strong> provision</td>
                                </tr>
                                <tr>
                                    <td class="px-3 py-2 font-semibold">Goal</td>
                                    <td class="px-3 py-2">AGI through reverse-engineering the <strong>biological brain</strong></td>
                                    <td class="px-3 py-2">AI with <strong>human-level reasoning/planning</strong> capabilities</td>
                                    <td class="px-3 py-2"><strong>Digital-physical fusion</strong> and robotics ecosystem</td>
                                </tr>
                            </tbody>
                        </table>
                    </div>

                    <h3 class="text-2xl font-semibold themeable-heading mt-8 mb-4">5.2 Debate 1: Representation -- Concreteness vs. Abstraction</h3>

                    <p class="themeable-text leading-relaxed">
                        <strong>Hawkins vs. LeCun:</strong> Hawkins believes the brain actually possesses geometric structures (reference frames) akin to 'maps.' Information is stored in conjunction with 'location.' LeCun, in contrast, argues that information should be compressed into a mathematically efficient 'abstract space.' However, the way latent variables (<code>z</code>) in LeCun's JEPA handle spatial uncertainty is functionally similar to how Hawkins' voting mechanism resolves uncertainty.
                    </p>

                    <p class="themeable-text leading-relaxed">
                        <strong>Li vs. LeCun:</strong> LeCun views directly generating pixels or 3D coordinates as "wasteful." Intelligence, he argues, does not require drawing an accurate 3D mesh of a cup -- only a 'conceptual understanding' sufficient to grasp it <a href="#ref-15" class="ref-link">[15]</a>. Li, on the other hand, believes that "concrete 3D environments" are essential for robots to actually act. Li's Marble paradoxically provides robots with a 'safe training ground' through the very 'detailed pixel/voxel generation' that LeCun rejects.
                    </p>

                    <h3 class="text-2xl font-semibold themeable-heading mt-8 mb-4">5.3 Debate 2: The Role of Generation</h3>

                    <p class="themeable-text leading-relaxed">
                        <strong>LeCun's Critique:</strong> Video generation models like Sora do not understand physical laws -- they merely mimic them. They may serve as excellent engineering tools, but as models of intelligence, they are destined to fail.
                    </p>

                    <p class="themeable-text leading-relaxed">
                        <strong>Li's Counter-Acceptance:</strong> Li's model is generative, but it enforces 3D geometric consistency rather than being a statistical arrangement of 2D pixels. That is, Marble aims to technically overcome LeCun's critique (lack of physical understanding) by generating 'physically plausible' 3D structures rather than merely 'visually plausible' imagery <a href="#ref-31" class="ref-link">[31]</a>.
                    </p>

                    <p class="themeable-text leading-relaxed">
                        <strong>Hawkins' Perspective:</strong> The brain constantly generates (predicts) inputs. However, this is not for external output but for verifying internal models. For Hawkins, generation is a means of learning, not an end in itself.
                    </p>

                    <h3 class="text-2xl font-semibold themeable-heading mt-8 mb-4">5.4 Complementarity: Convergence Toward AGI</h3>

                    <p class="themeable-text leading-relaxed">
                        These three theories are complementary rather than mutually exclusive.
                    </p>

                    <ol class="list-decimal pl-6 themeable-text-secondary mb-6">
                        <li><strong>Structure:</strong> Hawkins' <strong>reference frame</strong> theory provides a blueprint for the 'data structures' in which AI stores information. The attention mechanism of transformers can also be viewed as a form of implicit reference frame.</li>
                        <li><strong>Reasoning:</strong> LeCun's <strong>JEPA</strong> is a 'reasoning engine' that operates on top of this structure. It discards unnecessary information and learns only core causal relationships to formulate plans efficiently.</li>
                        <li><strong>Environment:</strong> Li's <strong>Marble</strong> provides the 'body and environment' for this AI to learn and interact with. We can envision a future where a robot equipped with JEPA (Brain) learns in a Marble-generated world (World) using Hawkins' sensorimotor approach.</li>
                    </ol>
                </section>

                <!-- Section 6: Conclusion -->
                <section id="conclusion" class="mb-12">
                    <h2 class="text-3xl font-bold themeable-heading mb-6 pb-3 border-b-2 themeable-border">
                        6. Conclusion: The Future Opened by World Models
                    </h2>

                    <p class="themeable-text leading-relaxed">
                        Jeff Hawkins, Yann LeCun, and Fei-Fei Li started from different points of departure, but their conclusions converge on a single insight: <strong>"Intelligence is not about learning static data, but about internalizing the structure and causal relationships of a dynamic world."</strong>
                    </p>

                    <ul class="list-disc pl-6 themeable-text-secondary mb-6">
                        <li><strong>Jeff Hawkins</strong> showed us how the brain <strong>'maps'</strong> the world. His theory paves the way for AI to go beyond simple pattern matching and structurally manipulate concepts the way humans do.</li>
                        <li><strong>Yann LeCun</strong> articulated how AI should think <strong>'efficiently.'</strong> His non-generative predictive model guides AI toward acquiring human-level common sense and planning capabilities without getting mired in unnecessary computation.</li>
                        <li><strong>Fei-Fei Li</strong> created a <strong>'space to inhabit'</strong> for AI. Her spatial intelligence technology extends digital intelligence into physical reality, blurring the boundaries between robotics and the metaverse.</li>
                    </ul>

                    <!-- Fusion Diagram: AGI로의 융합 -->
                    <h4 class="text-lg font-semibold themeable-heading mt-8 mb-4 text-center">Convergence of Three Theories Toward AGI</h4>
                    <div class="fusion-diagram">
                        <div class="fusion-item" style="border-color: #f97316;">
                            <div class="fusion-icon">🧠</div>
                            <p class="fusion-label">Layer 1</p>
                            <p class="fusion-title">Structure</p>
                            <p class="fusion-author">Jeff Hawkins</p>
                            <p class="text-xs themeable-text-secondary mt-2">Mapping information via reference frames</p>
                        </div>
                        <div class="fusion-item" style="border-color: #0d9488;">
                            <div class="fusion-icon">⚡</div>
                            <p class="fusion-label">Layer 2</p>
                            <p class="fusion-title">Reasoning</p>
                            <p class="fusion-author">Yann LeCun</p>
                            <p class="text-xs themeable-text-secondary mt-2">Efficient prediction in abstract space</p>
                        </div>
                        <div class="fusion-item" style="border-color: #8b5cf6;">
                            <div class="fusion-icon">🌍</div>
                            <p class="fusion-label">Layer 3</p>
                            <p class="fusion-title">Environment</p>
                            <p class="fusion-author">Fei-Fei Li</p>
                            <p class="text-xs themeable-text-secondary mt-2">Learning and acting in 3D worlds</p>
                        </div>
                    </div>

                    <p class="themeable-text leading-relaxed">
                        We are currently in a transitional period, moving from the era of text-based 'chatbots' to the era of 'agents' that understand and act upon physical reality. The world models being built by these three scholars will serve as the theoretical and technological pillars supporting this new era. The future of AI depends on how these three streams converge to give birth to <strong>embodied, reasoning, structured intelligence</strong>.
                    </p>
                </section>

                <!-- FAQ Section -->
                <section id="faq" class="mt-12">
                    <h2 class="text-3xl font-bold themeable-heading mb-6 pb-3 border-b-2 themeable-border">
                        Frequently Asked Questions (FAQ)
                    </h2>

                    <div class="space-y-4">
                        <div itemscope itemtype="https://schema.org/Question" class="border themeable-border rounded-lg p-6">
                            <h3 itemprop="name" class="text-xl font-semibold themeable-heading mb-3">
                                What is a World Model?
                            </h3>
                            <div itemscope itemprop="acceptedAnswer" itemtype="https://schema.org/Answer">
                                <div itemprop="text" class="themeable-text-secondary">
                                    <p>A world model is a cognitive framework that allows an agent (human or machine) to internally simulate the structure and operating principles of the external environment, enabling it to predict the outcomes of actions and formulate plans. It goes beyond simply classifying or generating data, serving as the core mechanism that makes possible the essence of intelligence: 'understanding' and 'survival.'</p>
                                </div>
                            </div>
                        </div>

                        <div itemscope itemtype="https://schema.org/Question" class="border themeable-border rounded-lg p-6">
                            <h3 itemprop="name" class="text-xl font-semibold themeable-heading mb-3">
                                What is the core of the Thousand Brains Theory?
                            </h3>
                            <div itemscope itemprop="acceptedAnswer" itemtype="https://schema.org/Answer">
                                <div itemprop="text" class="themeable-text-secondary">
                                    <p>Jeff Hawkins' Thousand Brains Theory proposes that the neocortex's approximately 150,000 cortical columns each function as independent sensorimotor modeling systems. These columns exchange information and reach consensus through a 'voting' mechanism. The key insight is that the brain uses coordinate systems called 'reference frames' to store information, and this applies not only to physical objects but also to abstract concepts.</p>
                                </div>
                            </div>
                        </div>

                        <div itemscope itemtype="https://schema.org/Question" class="border themeable-border rounded-lg p-6">
                            <h3 itemprop="name" class="text-xl font-semibold themeable-heading mb-3">
                                What is the difference between JEPA and conventional generative AI?
                            </h3>
                            <div itemscope itemprop="acceptedAnswer" itemtype="https://schema.org/Answer">
                                <div itemprop="text" class="themeable-text-secondary">
                                    <p>Yann LeCun's JEPA (Joint Embedding Predictive Architecture) performs predictions in an abstract representation space rather than in the input space (pixels). While conventional generative AI sequentially predicts pixels, leading to inefficiency and error accumulation, JEPA removes unnecessary details and learns only core causal relationships. This enables more efficient reasoning and planning.</p>
                                </div>
                            </div>
                        </div>

                        <div itemscope itemtype="https://schema.org/Question" class="border themeable-border rounded-lg p-6">
                            <h3 itemprop="name" class="text-xl font-semibold themeable-heading mb-3">
                                Why is Spatial Intelligence important for robotics?
                            </h3>
                            <div itemscope itemprop="acceptedAnswer" itemtype="https://schema.org/Answer">
                                <div itemprop="text" class="themeable-text-secondary">
                                    <p>Fei-Fei Li's spatial intelligence is AI's ability to understand and manipulate 3D space. For robots to act safely in the real world, they must understand depth, distance, and physical interactions. World Labs' Marble generates infinite 3D simulation environments, allowing robots to learn safely before transferring to reality (Sim2Real), which is a key strategy for solving the robotics data bottleneck.</p>
                                </div>
                            </div>
                        </div>

                        <div itemscope itemtype="https://schema.org/Question" class="border themeable-border rounded-lg p-6">
                            <h3 itemprop="name" class="text-xl font-semibold themeable-heading mb-3">
                                Are the three world model theories mutually exclusive?
                            </h3>
                            <div itemscope itemprop="acceptedAnswer" itemtype="https://schema.org/Answer">
                                <div itemprop="text" class="themeable-text-secondary">
                                    <p>No, the three theories are complementary. Hawkins' reference frames provide the data 'structure,' LeCun's JEPA provides the 'reasoning engine,' and Li's Marble provides the 'environment.' Future AGI could be realized as a robot equipped with JEPA performing sensorimotor learning in a Marble-generated world using Hawkins' approach.</p>
                                </div>
                            </div>
                        </div>

                        <div itemscope itemtype="https://schema.org/Question" class="border themeable-border rounded-lg p-6">
                            <h3 itemprop="name" class="text-xl font-semibold themeable-heading mb-3">
                                What is the relationship between LLM hallucination and world models?
                            </h3>
                            <div itemscope itemprop="acceptedAnswer" itemtype="https://schema.org/Answer">
                                <div itemprop="text" class="themeable-text-secondary">
                                    <p>LLM hallucinations occur because AI relies on probabilistic plausibility rather than grounding in real-world laws. World models aim to fundamentally solve this problem by internalizing the causal relationships and structure of the physical world. Text is merely a symbol describing the world; for true understanding, world models are essential.</p>
                                </div>
                            </div>
                        </div>

                        <div itemscope itemtype="https://schema.org/Question" class="border themeable-border rounded-lg p-6">
                            <h3 itemprop="name" class="text-xl font-semibold themeable-heading mb-3">
                                What are Sparse Distributed Representations (SDR) and why are they important?
                            </h3>
                            <div itemscope itemprop="acceptedAnswer" itemtype="https://schema.org/Answer">
                                <div itemprop="text" class="themeable-text-secondary">
                                    <p>SDR is a data structure at the foundation of Numenta's technology that mimics how neurons in the brain activate very sparsely. Only about 2% (40 out of 2,048 bits) are activated. Each bit in an SDR carries meaning, and the more overlapping bits two SDRs share, the more semantically similar they are. Even if half of the active bits are lost, the remaining bits can reconstruct the pattern, making SDRs robust to noise.</p>
                                </div>
                            </div>
                        </div>
                    </div>
                </section>

                <!-- References Section -->
                <section id="references" class="mt-12">
                    <h2 class="text-3xl font-bold themeable-heading mb-6 pb-3 border-b-2 themeable-border">
                        References
                    </h2>

                    <ol class="space-y-3 text-sm themeable-text-secondary">
                        <li id="ref-1">
                            <span class="font-semibold text-teal-600">[1]</span>
                            OpenAI's Video-Generating AI Is "Doomed to Failure," Says Meta's Top AI Scientist. Futurism.
                            <a href="https://futurism.com/the-byte/openai-video-ai-doomed-meta-scientist" class="text-orange-500 hover:underline" target="_blank">Link</a>
                        </li>
                        <li id="ref-2">
                            <span class="font-semibold text-teal-600">[2]</span>
                            Yan Nuriyev. LLMs Were Just the Warm-Up. AI's Next Revolution is World Models.
                            <a href="https://whoisyan.com/llms-were-just-the-warm-up-ais-next-revolution-is-world-models/" class="text-orange-500 hover:underline" target="_blank">Link</a>
                        </li>
                        <li id="ref-3">
                            <span class="font-semibold text-teal-600">[3]</span>
                            Jeff Hawkins. A Thousand Brains: A New Theory Of Intelligence. Numenta.
                            <a href="https://www.numenta.com/resources/books/a-thousand-brains-by-jeff-hawkins/" class="text-orange-500 hover:underline" target="_blank">Link</a>
                        </li>
                        <li id="ref-4">
                            <span class="font-semibold text-teal-600">[4]</span>
                            A Thousand Brains. Internet Archive.
                            <a href="https://dn790007.ca.archive.org/0/items/artificial-intelligence/2021_a_thousand_brains-a_new_theory_of_intelligence-Jeff%20Hawkins%20%282021%29.pdf" class="text-orange-500 hover:underline" target="_blank">Link</a>
                        </li>
                        <li id="ref-5">
                            <span class="font-semibold text-teal-600">[5]</span>
                            Christophe Pere. A Thousand Brains Theory: A Review. Medium.
                            <a href="https://medium.com/data-science/a-thousand-brains-theory-a-review-3ea6bbeeced0" class="text-orange-500 hover:underline" target="_blank">Link</a>
                        </li>
                        <li id="ref-6">
                            <span class="font-semibold text-teal-600">[6]</span>
                            The Thousand Brains Theory of Intelligence. Numenta.
                            <a href="https://www.numenta.com/blog/2019/01/16/the-thousand-brains-theory-of-intelligence/" class="text-orange-500 hover:underline" target="_blank">Link</a>
                        </li>
                        <li id="ref-7">
                            <span class="font-semibold text-teal-600">[7]</span>
                            A Framework for Intelligence and Cortical Function Based on Grid Cells in the Neocortex. Numenta.
                            <a href="https://www.numenta.com/assets/pdf/research-publications/papers/Companion-paper-to-Thousand-Brains-Theory-of-Intelligence.pdf" class="text-orange-500 hover:underline" target="_blank">Link</a>
                        </li>
                        <li id="ref-8">
                            <span class="font-semibold text-teal-600">[8]</span>
                            The Thousand Brains Theory. Microsoft Research.
                            <a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2019/03/42804_The_Thousand_Brains_Theory.pdf" class="text-orange-500 hover:underline" target="_blank">Link</a>
                        </li>
                        <li id="ref-9">
                            <span class="font-semibold text-teal-600">[9]</span>
                            The Thousand Brains Project. Numenta.
                            <a href="https://www.numenta.com/wp-content/uploads/2024/06/Short_TBP_Overview.pdf" class="text-orange-500 hover:underline" target="_blank">Link</a>
                        </li>
                        <li id="ref-10">
                            <span class="font-semibold text-teal-600">[10]</span>
                            Numenta Publishes a New Theory on Sensorimotor Inference. Numenta.
                            <a href="https://www.numenta.com/press/2017/11/15/numenta-publishes-new-sensorimotor-theory-in-frontiers/" class="text-orange-500 hover:underline" target="_blank">Link</a>
                        </li>
                        <li id="ref-11">
                            <span class="font-semibold text-teal-600">[11]</span>
                            Sparse Distributed Representations. Numenta.
                            <a href="https://www.numenta.com/assets/pdf/biological-and-machine-intelligence/BaMI-SDR.pdf" class="text-orange-500 hover:underline" target="_blank">Link</a>
                        </li>
                        <li id="ref-12">
                            <span class="font-semibold text-teal-600">[12]</span>
                            The HTM Spatial Pooler—A Neocortical Algorithm for Online Sparse Distributed Coding. Frontiers in Computational Neuroscience.
                            <a href="https://www.frontiersin.org/journals/computational-neuroscience/articles/10.3389/fncom.2017.00111/full" class="text-orange-500 hover:underline" target="_blank">Link</a>
                        </li>
                        <li id="ref-13">
                            <span class="font-semibold text-teal-600">[13]</span>
                            The HTM Spatial Pooler – a neocortical algorithm for online sparse distributed coding. bioRxiv.
                            <a href="https://www.biorxiv.org/content/10.1101/085035v1.full.pdf" class="text-orange-500 hover:underline" target="_blank">Link</a>
                        </li>
                        <li id="ref-14">
                            <span class="font-semibold text-teal-600">[14]</span>
                            World Models vs. Word Models: Why Yann LeCun Believes LLMs Will Be Obsolete. Medium.
                            <a href="https://medium.com/state-of-the-art-technology/world-models-vs-word-models-why-lecun-believes-llms-will-be-obsolete-23795e729cfa" class="text-orange-500 hover:underline" target="_blank">Link</a>
                        </li>
                        <li id="ref-15">
                            <span class="font-semibold text-teal-600">[15]</span>
                            Yann LeCun doubles down, claims Sora doesn't count. Reddit r/singularity.
                            <a href="https://www.reddit.com/r/singularity/comments/1atk19r/yann_lecun_doubles_down_claims_sora_doesnt_count/" class="text-orange-500 hover:underline" target="_blank">Link</a>
                        </li>
                        <li id="ref-16">
                            <span class="font-semibold text-teal-600">[16]</span>
                            Yann LeCun: Intense Complaints Before Leaving the Company. 36氪.
                            <a href="https://eu.36kr.com/en/p/3605931513267201" class="text-orange-500 hover:underline" target="_blank">Link</a>
                        </li>
                        <li id="ref-17">
                            <span class="font-semibold text-teal-600">[17]</span>
                            Highlights from Lex Fridman's interview of Yann LeCun. LessWrong.
                            <a href="https://www.lesswrong.com/posts/bce63kvsAMcwxPipX/highlights-from-lex-fridman-s-interview-of-yann-lecun" class="text-orange-500 hover:underline" target="_blank">Link</a>
                        </li>
                        <li id="ref-18">
                            <span class="font-semibold text-teal-600">[18]</span>
                            A Path Towards Autonomous Machine Intelligence. Temple CIS.
                            <a href="https://cis.temple.edu/tagit/presentations/A%20Path%20Towards%20Autonomous%20Machine%20Intelligence.pdf" class="text-orange-500 hover:underline" target="_blank">Link</a>
                        </li>
                        <li id="ref-19">
                            <span class="font-semibold text-teal-600">[19]</span>
                            What is Joint Embedding Predictive Architecture (JEPA)? Turing Post.
                            <a href="https://www.turingpost.com/p/jepa" class="text-orange-500 hover:underline" target="_blank">Link</a>
                        </li>
                        <li id="ref-20">
                            <span class="font-semibold text-teal-600">[20]</span>
                            Deep Dive into Yann LeCun's JEPA. Rohit Bandaru.
                            <a href="https://rohitbandaru.github.io/blog/JEPA-Deep-Dive/" class="text-orange-500 hover:underline" target="_blank">Link</a>
                        </li>
                        <li id="ref-21">
                            <span class="font-semibold text-teal-600">[21]</span>
                            LeCun's 2022 paper on autonomous machine intelligence rehashes but does not cite essential work of 1990-2015. Jürgen Schmidhuber.
                            <a href="https://people.idsia.ch/~juergen/lecun-rehash-1990-2022.html" class="text-orange-500 hover:underline" target="_blank">Link</a>
                        </li>
                        <li id="ref-22">
                            <span class="font-semibold text-teal-600">[22]</span>
                            Anil Jain. JEPA: LeCun's Path Towards More Human-Like AI. Medium.
                            <a href="https://medium.com/@anil.jain.baba/jepa-lecuns-path-towards-more-human-like-ai-9535e48b3c65" class="text-orange-500 hover:underline" target="_blank">Link</a>
                        </li>
                        <li id="ref-23">
                            <span class="font-semibold text-teal-600">[23]</span>
                            Malcolm Lett. Critical review of LeCun's Introductory JEPA paper. Medium.
                            <a href="https://malcolmlett.medium.com/critical-review-of-lecuns-introductory-jepa-paper-fabe5783134e" class="text-orange-500 hover:underline" target="_blank">Link</a>
                        </li>
                        <li id="ref-24">
                            <span class="font-semibold text-teal-600">[24]</span>
                            Viral! Fei-Fei Li's 10,000-Word Article Defines Next Decade of AI. 36氪.
                            <a href="https://eu.36kr.com/en/p/3548078081093508" class="text-orange-500 hover:underline" target="_blank">Link</a>
                        </li>
                        <li id="ref-25">
                            <span class="font-semibold text-teal-600">[25]</span>
                            Are World Models the Future of AI? Blockchain Council.
                            <a href="https://www.blockchain-council.org/ai/world-models-future-of-ai/" class="text-orange-500 hover:underline" target="_blank">Link</a>
                        </li>
                        <li id="ref-26">
                            <span class="font-semibold text-teal-600">[26]</span>
                            Building Spatial Intelligence: How World Labs is Creating the Next Frontier in AI. Radical VC.
                            <a href="https://radical.vc/building-spatial-intelligence-how-world-labs-is-creating-the-next-frontier-in-ai/" class="text-orange-500 hover:underline" target="_blank">Link</a>
                        </li>
                        <li id="ref-27">
                            <span class="font-semibold text-teal-600">[27]</span>
                            Marble: A Multimodal World Model. World Labs.
                            <a href="https://www.worldlabs.ai/blog/marble-world-model" class="text-orange-500 hover:underline" target="_blank">Link</a>
                        </li>
                        <li id="ref-28">
                            <span class="font-semibold text-teal-600">[28]</span>
                            New Marble AI Creates Entire 3D Worlds from Simple Text Prompts. Analytics Vidhya.
                            <a href="https://www.analyticsvidhya.com/blog/2025/11/marble-world-ai-creates-3d-worlds-from-text/" class="text-orange-500 hover:underline" target="_blank">Link</a>
                        </li>
                        <li id="ref-29">
                            <span class="font-semibold text-teal-600">[29]</span>
                            Scaling Robotic Simulation with Marble. World Labs.
                            <a href="https://www.worldlabs.ai/case-studies/1-robotics" class="text-orange-500 hover:underline" target="_blank">Link</a>
                        </li>
                        <li id="ref-30">
                            <span class="font-semibold text-teal-600">[30]</span>
                            Simulate Robotic Environments Faster with NVIDIA Isaac Sim and World Labs Marble. NVIDIA Developer Blog.
                            <a href="https://developer.nvidia.com/blog/simulate-robotic-environments-faster-with-nvidia-isaac-sim-and-world-labs-marble/" class="text-orange-500 hover:underline" target="_blank">Link</a>
                        </li>
                        <li id="ref-31">
                            <span class="font-semibold text-teal-600">[31]</span>
                            evoailabs. World Models — Not Next, Current Frontier. Medium.
                            <a href="https://evoailabs.medium.com/world-models-not-next-current-frontier-547c5eeb1307" class="text-orange-500 hover:underline" target="_blank">Link</a>
                        </li>
                        <li id="ref-32">
                            <span class="font-semibold text-teal-600">[32]</span>
                            RTFM: A Real-Time Frame Model. World Labs.
                            <a href="https://www.worldlabs.ai/blog/rtfm" class="text-orange-500 hover:underline" target="_blank">Link</a>
                        </li>
                        <li id="ref-33">
                            <span class="font-semibold text-teal-600">[33]</span>
                            Pebblous. (2026). What Is a World Model? The AI Requirement That Prevents $2M in Losses. Data Clinic Blog.
                            <a href="https://blog.dataclinic.ai/world-model/" class="text-orange-500 hover:underline" target="_blank">Link</a>
                        </li>
                    </ol>
                </section>

                <!-- PDF Download Section -->
                <section id="pdf-download" class="mt-12">
                    <div class="themeable-card rounded-xl p-8 md:p-10 border-2" style="border-color: var(--accent-color);">
                        <div class="flex flex-col md:flex-row items-center gap-8">
                            <div class="flex-shrink-0">
                                <div class="w-24 h-24 rounded-full flex items-center justify-center" style="background: linear-gradient(135deg, #F86825, #FF8C42);">
                                    <svg class="w-12 h-12 text-white" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                                        <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M12 10v6m0 0l-3-3m3 3l3-3m2 8H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"></path>
                                    </svg>
                                </div>
                            </div>
                            <div class="flex-1">
                                <h3 class="text-2xl font-bold themeable-heading mb-3">Original PDF Report</h3>
                                <p class="themeable-text-muted mb-6">
                                    You can view the full content of this report in PDF format directly or download it for offline reading.
                                </p>
                                <div class="flex flex-col sm:flex-row gap-4">
                                    <button id="open-pdf-modal"
                                       class="inline-flex items-center justify-center gap-2 px-6 py-3 rounded-lg font-bold text-white transition-all hover:scale-105"
                                       style="background: linear-gradient(135deg, #F86825, #FF8C42);">
                                        <svg class="w-5 h-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M15 12a3 3 0 11-6 0 3 3 0 016 0z"></path>
                                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M2.458 12C3.732 7.943 7.523 5 12 5c4.478 0 8.268 2.943 9.542 7-1.274 4.057-5.064 7-9.542 7-4.477 0-8.268-2.943-9.542-7z"></path>
                                        </svg>
                                        View Now
                                    </button>
                                    <a href="/project/World%20Model/source/%EC%B0%A8%EC%84%B8%EB%8C%80%20AI%EB%A5%BC%20%EC%9C%84%ED%95%9C%20%EC%84%B8%20%EA%B0%80%EC%A7%80%20%EC%9B%94%EB%93%9C%EB%AA%A8%EB%8D%B8%20%EB%B9%84%EA%B5%90_%20Jeff%20Hawkins%2C%20Yann%20LeCun%2C%20Fei-Fei%20Li.pdf"
                                       download
                                       class="inline-flex items-center justify-center gap-2 px-6 py-3 themeable-card rounded-lg font-bold themeable-heading transition-all hover:scale-105 border-2"
                                       style="border-color: var(--accent-color);">
                                        <svg class="w-5 h-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 16a4 4 0 01-.88-7.903A5 5 0 1115.9 6L16 6a5 5 0 011 9.9M9 19l3 3m0 0l3-3m-3 3V10"></path>
                                        </svg>
                                        Download PDF
                                    </a>
                                </div>
                            </div>
                        </div>
                    </div>
                </section>

                <!-- Related Posts -->
                <section id="related-posts" class="mt-12">
                    <h3 class="text-2xl font-semibold themeable-heading mb-6">Related Posts</h3>
                    <div id="related-posts-container"></div>
                </section>
            </main>

        </div><!-- End Flex Layout -->
    </div>

    <!-- Footer Placeholder -->
    <div id="footer-placeholder"></div>

    <!-- PDF Viewer Modal -->
    <div id="pdf-modal" class="fixed inset-0 z-50 hidden">
        <!-- Backdrop -->
        <div class="absolute inset-0 bg-black bg-opacity-75 transition-opacity" id="pdf-modal-backdrop"></div>

        <!-- Modal Content -->
        <div class="relative w-full h-full flex items-center justify-center p-4">
            <div class="relative w-full h-full max-w-7xl max-h-[95vh] themeable-card rounded-xl overflow-hidden shadow-2xl">
                <!-- Header -->
                <div class="flex items-center justify-between p-4 border-b themeable-border">
                    <h3 class="text-xl font-bold themeable-heading">PDF Report Preview</h3>
                    <button id="close-pdf-modal" class="p-2 hover:bg-opacity-10 rounded-lg transition-colors accent-text">
                        <svg class="w-6 h-6" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M6 18L18 6M6 6l12 12"></path>
                        </svg>
                    </button>
                </div>

                <!-- PDF Iframe -->
                <iframe
                    id="pdf-iframe"
                    class="w-full h-[calc(100%-60px)]"
                    src=""
                    frameborder="0"
                    allowfullscreen>
                </iframe>
            </div>
        </div>
    </div>

    <!-- Scripts -->
    <script src="/scripts/common-utils.js"></script>

    <!-- Page Initialization Script -->
    <script>
        document.addEventListener('DOMContentLoaded', async function() {
            const config = {
                mainTitle: "Comparing Three World Models for Next-Generation AI",
                subtitle: "Jeff Hawkins, Yann LeCun, Fei-Fei Li",
                pageTitle: "Comparing Three World Models for Next-Generation AI: Jeff Hawkins, Yann LeCun, Fei-Fei Li | Pebblous",

                publishDate: "January 2, 2026",
                publisher: "Pebblous Data Communication Team",

                defaultTheme: "light",
                category: "tech",
                articlePath: "project/World Model/world-model-comparison/ko/",
                tags: [
                    "World Model",
                    "Jeff Hawkins", "Yann LeCun", "Fei-Fei Li",
                    "Thousand Brains Theory",
                    "JEPA", "Joint Embedding Predictive Architecture",
                    "Spatial Intelligence",
                    "World Labs", "Numenta", "Meta AI",
                    "Cortical Column",
                    "Reference Frame",
                    "SDR", "Sparse Distributed Representations",
                    "Marble", "RTFM",
                    "Robotics",
                    "AGI", "Artificial General Intelligence",
                    "LLM Limitations", "Generative AI",
                    "Physical AI",
                    "Pebblous"
                ],

                faqs: [
                    {
                        question: "What is a World Model?",
                        answer: "A world model is a cognitive framework that allows an agent (human or machine) to internally simulate the structure and operating principles of the external environment, enabling it to predict the outcomes of actions and formulate plans. It goes beyond simply classifying or generating data, serving as the core mechanism that makes possible the essence of intelligence: 'understanding' and 'survival.'"
                    },
                    {
                        question: "What is the core of the Thousand Brains Theory?",
                        answer: "Jeff Hawkins' Thousand Brains Theory proposes that the neocortex's approximately 150,000 cortical columns each function as independent sensorimotor modeling systems. These columns exchange information and reach consensus through a 'voting' mechanism. The key insight is that the brain uses coordinate systems called 'reference frames' to store information, and this applies not only to physical objects but also to abstract concepts."
                    },
                    {
                        question: "What is the difference between JEPA and conventional generative AI?",
                        answer: "Yann LeCun's JEPA (Joint Embedding Predictive Architecture) performs predictions in an abstract representation space rather than in the input space (pixels). While conventional generative AI sequentially predicts pixels, leading to inefficiency and error accumulation, JEPA removes unnecessary details and learns only core causal relationships."
                    },
                    {
                        question: "Why is Spatial Intelligence important for robotics?",
                        answer: "Fei-Fei Li's spatial intelligence is AI's ability to understand and manipulate 3D space. For robots to act safely in the real world, they must understand depth, distance, and physical interactions. World Labs' Marble generates infinite 3D simulation environments, allowing robots to learn safely before transferring to reality (Sim2Real)."
                    },
                    {
                        question: "Are the three world model theories mutually exclusive?",
                        answer: "No, the three theories are complementary. Hawkins' reference frames provide the data 'structure,' LeCun's JEPA provides the 'reasoning engine,' and Li's Marble provides the 'environment.' Future AGI could be realized as a robot equipped with JEPA performing sensorimotor learning in a Marble-generated world using Hawkins' approach."
                    },
                    {
                        question: "What is the relationship between LLM hallucination and world models?",
                        answer: "LLM hallucinations occur because AI relies on probabilistic plausibility rather than grounding in real-world laws. World models aim to fundamentally solve this problem by internalizing the causal relationships and structure of the physical world."
                    },
                    {
                        question: "What are Sparse Distributed Representations (SDR) and why are they important?",
                        answer: "SDR is a data structure at the foundation of Numenta's technology that mimics how neurons in the brain activate very sparsely. Only about 2% (40 out of 2,048 bits) are activated. Each bit in an SDR carries meaning, and the more overlapping bits two SDRs share, the more semantically similar they are. Even if half of the active bits are lost, the remaining bits can reconstruct the pattern, making SDRs robust to noise."
                    }
                ]
            };

            await PebblousPage.init(config);

            // PDF Modal functionality
            const pdfModal = document.getElementById('pdf-modal');
            const pdfIframe = document.getElementById('pdf-iframe');
            const openPdfBtn = document.getElementById('open-pdf-modal');
            const closePdfBtn = document.getElementById('close-pdf-modal');
            const pdfBackdrop = document.getElementById('pdf-modal-backdrop');
            const pdfPath = '/project/World%20Model/source/%EC%B0%A8%EC%84%B8%EB%8C%80%20AI%EB%A5%BC%20%EC%9C%84%ED%95%9C%20%EC%84%B8%20%EA%B0%80%EC%A7%80%20%EC%9B%94%EB%93%9C%EB%AA%A8%EB%8D%B8%20%EB%B9%84%EA%B5%90_%20Jeff%20Hawkins%2C%20Yann%20LeCun%2C%20Fei-Fei%20Li.pdf';

            // Open PDF modal
            if (openPdfBtn) {
                openPdfBtn.addEventListener('click', () => {
                    pdfIframe.src = pdfPath;
                    pdfModal.classList.remove('hidden');
                    document.body.style.overflow = 'hidden';
                });
            }

            // Close PDF modal function
            const closePdfModal = () => {
                pdfModal.classList.add('hidden');
                pdfIframe.src = '';
                document.body.style.overflow = '';
            };

            // Close button click
            if (closePdfBtn) {
                closePdfBtn.addEventListener('click', closePdfModal);
            }

            // Backdrop click
            if (pdfBackdrop) {
                pdfBackdrop.addEventListener('click', closePdfModal);
            }

            // ESC key to close
            document.addEventListener('keydown', (e) => {
                if (e.key === 'Escape' && pdfModal && !pdfModal.classList.contains('hidden')) {
                    closePdfModal();
                }
            });
        });
    </script>
</body>
</html>
