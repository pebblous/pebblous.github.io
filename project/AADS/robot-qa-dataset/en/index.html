<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="author" content="Pebblous Data Communication Team">
    <meta name="language" content="English">
    <meta name="copyright" content="© 2025 Pebblous. All rights reserved.">
    <meta name="rating" content="general">
    <meta name="revisit-after" content="7 days">
    <meta name="distribution" content="global">
    <meta name="audience" content="AI Researchers, Data Scientists, Robotics Engineers, LLM Developers, Physical AI Researchers">
    <meta name="topic" content="LLM Fine-tuning, QA Dataset, Robotics AI, Physical AI, Data Quality">
    <meta http-equiv="content-language" content="en">

    <!-- Google Tag Manager -->
    <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
    new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
    j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
    'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','GTM-57L9F58B');</script>
    <!-- End Google Tag Manager -->

    <!-- Favicon -->
    <link rel="icon" href="/image/favicon.ico" sizes="any">
    <link rel="icon" href="/image/Pebblous_BM_Orange_RGB.png" type="image/png">
    <link rel="apple-touch-icon" href="/image/Pebblous_BM_Orange_RGB.png">

    <!-- SEO Meta Tags -->
    <title id="page-title">Building QA Datasets for LLM Fine-Tuning in Robotics: AADS Physical AI Approach | Pebblous</title>
    <meta id="meta-description" name="description" content="Pebblous AADS built 52 QA pairs for LLM fine-tuning across 13 robotics intelligence domains including occluded object reasoning, delivery robots, driving video, indoor maintenance, and object property recognition. A data-centric Physical AI approach spanning robot data collection, AI model training, and quality management.">
    <meta id="meta-keywords" name="keywords" content="LLM Fine-tuning, QA Dataset, Question-Answer Dataset, Robotics AI, Robot Data, AADS, Agentic AI Data Scientist, Physical AI, Data Quality, Data-Centric AI, Multimodal Data, Domain Knowledge, Occluded Object Detection, Delivery Robot, Off-Road Navigation, Driving Video, Indoor Maintenance, Service Robot, Object Property Recognition, Robot Hand, Grasp-Manipulation, Hand-Arm Coordination, Human Activity Recognition, Robot Autonomous Behavior, Few-Shot Learning, Prompt Engineering, Labeling, Data Validation, mAP, F1-score, mIoU, Pebblous, DataClinic">
    <meta name="robots" content="index, follow">

    <link id="hreflang-ko" rel="alternate" hreflang="ko" href="https://blog.pebblous.ai/project/AADS/robot-qa-dataset/ko/">
    <link id="hreflang-en" rel="alternate" hreflang="en" href="https://blog.pebblous.ai/project/AADS/robot-qa-dataset/en/">
    <link id="hreflang-default" rel="alternate" hreflang="x-default" href="https://blog.pebblous.ai/project/AADS/robot-qa-dataset/ko/">

    <link id="canonical-url" rel="canonical" href="https://blog.pebblous.ai/project/AADS/robot-qa-dataset/en/">

    <meta id="og-url" property="og:url" content="https://blog.pebblous.ai/project/AADS/robot-qa-dataset/en/">
    <meta id="og-title" property="og:title" content="Building QA Datasets for LLM Fine-Tuning in Robotics: AADS Physical AI Approach | Pebblous">
    <meta id="og-description" property="og:description" content="A practical case study of AADS building 52 high-quality QA pairs across 13 robotics intelligence domains. A data-centric Physical AI strategy balancing domain definition, data structure, AI models, and quality management.">
    <meta id="og-image" property="og:image" content="https://blog.pebblous.ai/image/Pebblous_BM_Orange_RGB.png">
    <meta property="og:image:width" content="1200">
    <meta property="og:image:height" content="630">
    <meta property="og:image:alt" content="AADS Robotics LLM Fine-Tuning QA Dataset - Pebblous">
    <meta property="og:type" content="article">
    <meta property="og:site_name" content="Pebblous Blog">
    <meta property="og:locale" content="en_US">
    <meta property="article:published_time" content="2025-11-30T09:00:00+09:00">
    <meta property="article:modified_time" content="2025-11-30T09:00:00+09:00">
    <meta property="article:author" content="Pebblous Data Communication Team">
    <meta property="article:section" content="Technology">
    <meta property="article:tag" content="LLM Fine-tuning">
    <meta property="article:tag" content="Robotics AI">
    <meta property="article:tag" content="Physical AI">
    <meta property="article:tag" content="AADS">

    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:site" content="@Pebblous">
    <meta name="twitter:creator" content="@pebblous">
    <meta name="twitter:title" content="Building QA Datasets for LLM Fine-Tuning in Robotics: AADS Physical AI Approach">
    <meta name="twitter:description" content="A practical case study of AADS building 52 high-quality QA pairs across 13 robotics intelligence domains. A data-centric Physical AI strategy balancing domain definition, data structure, AI models, and quality management.">
    <meta name="twitter:image" content="https://blog.pebblous.ai/image/Pebblous_BM_Orange_RGB.png">
    <meta name="twitter:image:alt" content="AADS Robotics LLM Fine-Tuning QA Dataset">
    <meta name="twitter:label1" content="Reading time">
    <meta name="twitter:data1" content="15 min">
    <meta name="twitter:label2" content="Level">
    <meta name="twitter:data2" content="Intermediate">

    <!-- JSON-LD Structured Data for SEO -->
    <script type="application/ld+json">
    {
        "@context": "https://schema.org",
        "@type": "TechArticle",
        "headline": "Building QA Datasets for LLM Fine-Tuning in Robotics: AADS Physical AI Approach",
        "alternativeHeadline": "A data-centric Physical AI strategy building 52 high-quality QA pairs across 13 robotics intelligence domains",
        "description": "Pebblous AADS built 52 QA pairs for LLM fine-tuning across 13 robotics intelligence domains including occluded object reasoning, delivery robots, driving video, indoor maintenance, and object property recognition. A data-centric Physical AI approach spanning robot data collection, AI model training, and quality management.",
        "image": {
            "@type": "ImageObject",
            "url": "https://blog.pebblous.ai/image/Pebblous_BM_Orange_RGB.png",
            "width": 1200,
            "height": 630
        },
        "author": {
            "@type": "Organization",
            "name": "Pebblous",
            "url": "https://www.pebblous.ai",
            "logo": {
                "@type": "ImageObject",
                "url": "https://www.pebblous.ai/image/Pebblous_BM_Orange_RGB.png"
            },
            "description": "Pebblous is a deep-tech company providing AI-Ready Data solutions."
        },
        "publisher": {
            "@type": "Organization",
            "name": "Pebblous",
            "url": "https://www.pebblous.ai",
            "logo": {
                "@type": "ImageObject",
                "url": "https://www.pebblous.ai/image/Pebblous_BM_Orange_RGB.png",
                "width": 600,
                "height": 60
            },
            "sameAs": [
                "https://www.linkedin.com/company/pebblous",
                "https://github.com/pebblous"
            ]
        },
        "datePublished": "2025-11-30T09:00:00+09:00",
        "dateModified": "2025-11-30T09:00:00+09:00",
        "mainEntityOfPage": {
            "@type": "WebPage",
            "@id": "https://blog.pebblous.ai/project/AADS/robot-qa-dataset/en/"
        },
        "keywords": "LLM Fine-tuning, QA Dataset, Robotics AI, Robot Data, AADS, Physical AI, Data Quality, Data-Centric AI, Multimodal Data, Domain Knowledge",
        "articleSection": "Technology",
        "inLanguage": "en-US",
        "isAccessibleForFree": true,
        "proficiencyLevel": "Intermediate"
    }
    </script>

    <!-- FAQ Schema is dynamically generated by common-utils.js from config.faqs -->

    <!-- Stylesheets -->
    <link rel="stylesheet" href="/styles/common-styles.css?v=20260107">
    <link rel="stylesheet" href="/styles/tailwind-build.css">

    <!-- Fonts -->
    <link rel="stylesheet" as="style" crossorigin
          href="https://cdn.jsdelivr.net/gh/orioncactus/pretendard@v1.3.9/dist/web/variable/pretendardvariable.min.css">

    <style>
        /* Theme Variables - Light as default */
        :root {
            --bg-primary: #F9FAFB;
            --bg-secondary: #F3F4F6;
            --bg-card: rgba(255, 255, 255, 0.95);
            --text-primary: #111827;
            --text-secondary: #4B5563;
            --text-muted: #6B7280;
            --heading-color: #111827;
            --border-color: #E5E7EB;
            --accent-color: #F86825;
            --teal-color: #0d9488;
        }

        [data-theme="dark"] {
            --bg-primary: #020617;
            --bg-secondary: #0f172a;
            --bg-card: rgba(30, 41, 59, 0.95);
            --text-primary: #F1F5F9;
            --text-secondary: #CBD5E1;
            --text-muted: #94A3B8;
            --heading-color: #F1F5F9;
            --border-color: #334155;
            --accent-color: #F86825;
            --teal-color: #14b8a6;
        }

        body {
            background-color: var(--bg-primary);
            color: var(--text-primary);
            font-family: 'Pretendard Variable', -apple-system, BlinkMacSystemFont, system-ui, Roboto, sans-serif;
            transition: background-color 0.3s ease, color 0.3s ease;
            line-height: 1.7;
        }

        .themeable-bg {
            background-color: var(--bg-card);
            transition: background-color 0.3s ease;
        }

        .themeable-text {
            color: var(--text-primary);
        }

        .themeable-text-secondary {
            color: var(--text-secondary);
        }

        .themeable-text-muted {
            color: var(--text-muted);
        }

        .themeable-heading {
            color: var(--heading-color);
        }

        .themeable-border {
            border-color: var(--border-color);
        }

        .orange-text {
            color: var(--accent-color);
        }

        .themeable-toc-border {
            border-color: var(--teal-color);
        }

        .teal-text {
            color: var(--teal-color);
        }

        .card-hover {
            transition: all 0.3s ease;
            border: 1px solid var(--border-color);
        }

        .card-hover:hover {
            border-color: var(--teal-color);
            box-shadow: 0 4px 12px rgba(20, 184, 166, 0.15);
        }

        /* Share buttons styling */
        .share-container {
            display: flex;
            align-items: center;
            justify-content: center;
            gap: 0.75rem;
            flex-wrap: wrap;
        }

        .share-label {
            font-size: 0.875rem;
            color: var(--text-secondary);
            font-weight: 500;
        }

        .share-btn {
            display: inline-flex;
            align-items: center;
            gap: 0.375rem;
            padding: 0.5rem 0.875rem;
            border-radius: 0.5rem;
            font-size: 0.875rem;
            font-weight: 500;
            transition: all 0.2s ease;
            border: 1px solid var(--border-color);
            background-color: var(--bg-card);
            color: var(--text-secondary);
            cursor: pointer;
        }

        .share-btn:hover {
            background-color: var(--accent-color);
            color: white;
            border-color: var(--accent-color);
            transform: translateY(-2px);
            box-shadow: 0 4px 8px rgba(248, 104, 37, 0.2);
        }

        .share-btn svg {
            width: 1rem;
            height: 1rem;
        }

        .share-btn.copied {
            background-color: var(--teal-color);
            color: white;
            border-color: var(--teal-color);
        }

        /* Table styling */
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 1.5rem 0;
            font-size: 0.9rem;
        }

        thead {
            background-color: rgba(248, 104, 37, 0.1);
        }

        th {
            padding: 1rem;
            text-align: left;
            font-weight: 600;
            color: var(--heading-color);
            border-bottom: 2px solid var(--border-color);
        }

        td {
            padding: 1rem;
            border-bottom: 1px solid var(--border-color);
            color: var(--text-secondary);
        }

        tbody tr:hover {
            background-color: rgba(248, 104, 37, 0.05);
        }

        /* Details/Summary styling */
        details {
            margin: 1rem 0;
            padding: 1rem;
            border: 1px solid var(--border-color);
            border-radius: 0.5rem;
            background-color: var(--bg-card);
        }

        details summary {
            cursor: pointer;
            font-weight: 600;
            color: var(--teal-color);
            padding: 0.5rem;
            user-select: none;
        }

        details summary:hover {
            color: var(--accent-color);
        }

        details[open] summary {
            margin-bottom: 1rem;
            padding-bottom: 0.5rem;
            border-bottom: 1px solid var(--border-color);
        }

        /* Code styling */
        code {
            background-color: rgba(248, 104, 37, 0.1);
            padding: 0.2rem 0.4rem;
            border-radius: 0.25rem;
            font-family: 'Courier New', monospace;
            font-size: 0.9em;
            color: var(--accent-color);
        }

        /* List styling */
        ul, ol {
            margin-left: 1.5rem;
            margin-bottom: 1.5rem;
        }

        li {
            margin-bottom: 0.75rem;
            line-height: 1.7;
        }

        /* Strong text */
        strong {
            font-weight: 600;
            color: var(--heading-color);
        }
    </style>
</head>
<body>
    <!-- Google Tag Manager (noscript) -->
    <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-57L9F58B"
    height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
    <!-- End Google Tag Manager (noscript) -->

    <!-- Header will be loaded by common-utils.js -->
    <div id="header-placeholder"></div>

    <!-- Main Content -->
    <div class="container mx-auto px-4 sm:px-6 lg:px-8 py-8 md:py-12 max-w-[1400px]">
        <div class="lg:flex lg:gap-8 lg:justify-center lg:items-start">

            <!-- TOC Sidebar -->
            <nav class="hidden lg:block lg:w-[240px] lg:shrink-0 sticky top-20 self-start">
                <h3 class="font-bold themeable-heading mb-4 text-lg">Contents</h3>
                <ul id="toc-links" class="space-y-3 text-sm border-l-2 themeable-toc-border pl-4">
                    <li><a href="#intro" class="toc-link themeable-text-secondary hover:text-teal-500 transition-colors">Introduction & Objectives</a></li>
                    <li><a href="#overview" class="toc-link themeable-text-secondary hover:text-teal-500 transition-colors">QA Dataset Overview</a></li>
                    <li><a href="#robot-datasets" class="toc-link themeable-text-secondary hover:text-teal-500 transition-colors">Robot Intelligence Datasets</a>
                        <ul class="ml-4 mt-2 space-y-2">
                            <li><a href="#dataset-1" class="toc-link themeable-text-secondary hover:text-teal-500 transition-colors text-xs">1. Occluded Object Reasoning</a></li>
                            <li><a href="#dataset-2" class="toc-link themeable-text-secondary hover:text-teal-500 transition-colors text-xs">2. Delivery Robot Off-Road</a></li>
                            <li><a href="#dataset-3" class="toc-link themeable-text-secondary hover:text-teal-500 transition-colors text-xs">3. Robot-View Driving Video</a></li>
                            <li><a href="#dataset-4" class="toc-link themeable-text-secondary hover:text-teal-500 transition-colors text-xs">4. Indoor Maintenance</a></li>
                            <li><a href="#dataset-5" class="toc-link themeable-text-secondary hover:text-teal-500 transition-colors text-xs">5. Robot Hand Object Properties</a></li>
                            <li><a href="#dataset-6" class="toc-link themeable-text-secondary hover:text-teal-500 transition-colors text-xs">6. Hand-Arm Grasp-Manipulation</a></li>
                            <li><a href="#dataset-7" class="toc-link themeable-text-secondary hover:text-teal-500 transition-colors text-xs">7. Human Activity Recognition</a></li>
                        </ul>
                    </li>
                    <li><a href="#statistics" class="toc-link themeable-text-secondary hover:text-teal-500 transition-colors">QA Type Statistics</a></li>
                    <li><a href="#prompt-template" class="toc-link themeable-text-secondary hover:text-teal-500 transition-colors">Prompt Template</a></li>
                    <li><a href="#pebblous-perspective" class="toc-link themeable-text-secondary hover:text-teal-500 transition-colors">Pebblous Perspective</a></li>
                    <li><a href="#faq" class="toc-link themeable-text-secondary hover:text-teal-500 transition-colors">FAQ</a></li>
                    <li><a href="#datasets-sources" class="toc-link themeable-text-secondary hover:text-teal-500 transition-colors">Dataset Sources</a></li>
                    <li><a href="#conclusion" class="toc-link themeable-text-secondary hover:text-teal-500 transition-colors">Conclusion</a></li>
                    <li><a href="#pdf-download" class="toc-link themeable-text-secondary hover:text-teal-500 transition-colors">PDF Download</a></li>
                </ul>
            </nav>

            <!-- Main Article -->
            <main class="max-w-[800px] px-4 sm:px-6">

                <header class="text-left mb-16">
                    <h1 id="page-h1-title" class="text-4xl md:text-5xl font-bold themeable-heading mb-6 leading-tight" style="line-height: 1.4;"></h1>

                    <p class="text-sm themeable-muted">2025.11 · Pebblous Data Communication Team</p>
                    <p class="text-sm themeable-muted mt-1">Reading time: ~15 min · <a href="../ko/" class="text-orange-400 hover:text-orange-300 transition-colors">한국어</a></p>
                </header>

                <!-- Section 1: Introduction & Objectives -->
                <section id="intro" class="mb-12">
                    <h2 class="text-3xl font-bold themeable-heading mb-6 pb-3 border-b-2 themeable-border">
                        I. Introduction & Objectives
                    </h2>

                    <div class="themeable-text-secondary space-y-4 mb-6">
                        <p>
                            This report introduces <strong>52 QA (Question-Answer) pairs</strong> built from actual AI Hub dataset documentation
                            under the <strong class="teal-text">Agentic AI Data Scientist (AADS)</strong> project,
                            with the goal of <strong class="orange-text">enhancing LLM domain expertise in robotics intelligence</strong>.
                        </p>
                        <p>
                            Pebblous reorganized <strong class="orange-text">13 robotics datasets</strong> into "logical data groups" and
                            systematically generated QA pairs in <strong>4 types (Simple Information Extraction, Summary & Explanation, Comparison & Analysis, Reasoning & Application)</strong>
                            for each group. This design enables LLMs to understand
                            <strong class="teal-text">the entire lifecycle of Physical AI</strong>, from robot data collection to AI model training and quality management.
                        </p>
                        <p>
                            The dataset covers core areas of robotics intelligence including <strong>occluded object reasoning, delivery robot off-road navigation, driving video, indoor space maintenance, object property identification,
                            grasp-manipulation actions, and human activity recognition</strong>,
                            and is structured for immediate practical use through Few-Shot Learning and prompt engineering.
                        </p>
                    </div>

                    <!-- PDF guide -->
                    <div class="interactive-card border border-orange-500/40 rounded-lg p-6 mb-6">
                        <h4 class="font-semibold orange-text mb-3">Original Report Guide</h4>
                        <p class="themeable-text-secondary">
                            This web page contains all <strong class="teal-text">52 core QA pairs across 13 robot datasets</strong>.
                            For more detailed analysis and original text, please <a href="#pdf-download" class="orange-text hover:underline">download the PDF report</a> at the bottom.
                        </p>
                    </div>
                </section>

                <!-- Section 1.5: QA Dataset Overview -->
                <section id="overview" class="mb-12">
                    <h2 class="text-3xl font-bold themeable-heading mb-6 pb-3 border-b-2 themeable-border">
                        II. QA Dataset Overview
                    </h2>

                    <div class="grid grid-cols-1 md:grid-cols-3 gap-6 mb-8">
                        <div class="stat-card border themeable-border rounded-lg p-6 text-center">
                            <h4 class="text-4xl font-bold orange-text mb-2">13</h4>
                            <p class="text-sm themeable-text-muted">Robot Data Groups</p>
                        </div>
                        <div class="stat-card border themeable-border rounded-lg p-6 text-center">
                            <h4 class="text-4xl font-bold orange-text mb-2">52</h4>
                            <p class="text-sm themeable-text-muted">QA Pairs (4 per group)</p>
                        </div>
                        <div class="stat-card border themeable-border rounded-lg p-6 text-center">
                            <h4 class="text-4xl font-bold orange-text mb-2">25%</h4>
                            <p class="text-sm themeable-text-muted">Equally Distributed per Type</p>
                        </div>
                    </div>

                    <div class="themeable-text-secondary space-y-4">
                        <p>
                            A total of <strong class="orange-text">52 QA pairs</strong> were constructed for
                            <strong class="teal-text">13 robot data groups</strong>.
                            Each group covers core Physical AI functions including robot perception, navigation, manipulation, and maintenance.
                        </p>
                        <p>
                            The QA pairs are <strong class="teal-text">equally distributed at 25% each</strong> across
                            the four types: <strong>Simple Information Extraction, Summary & Explanation, Comparison & Analysis, and Reasoning & Application</strong>,
                            designed to enable LLMs to learn comprehensive knowledge across all areas of robot data science.
                        </p>
                    </div>
                </section>

                <!-- Section 3: QA Construction Based on Robot Intelligence Datasets -->
                <section id="robot-datasets" class="mb-12">
                    <h2 class="text-3xl font-bold themeable-heading mb-6 pb-3 border-b-2 themeable-border">
                        III. QA Construction Based on Robot Intelligence Datasets
                    </h2>

                    <div class="themeable-text-secondary space-y-4 mb-8">
                        <p>
                            The robotics intelligence field demands high data quality due to
                            <strong class="orange-text">sensor data diversity</strong> and
                            <strong class="teal-text">real-time decision-making requirements</strong>.
                            AADS analyzed 13 robotics datasets from AI Hub and generated 4 types of QA pairs tailored to each dataset's characteristics.
                        </p>
                        <p>
                            Each dataset was systematically documented across four dimensions:
                            <strong>Domain Definition</strong> (data collection purpose and structure),
                            <strong>Data Structure</strong> (labeling methods and environments),
                            <strong>AI Models</strong> (learning algorithms and applications), and
                            <strong>Quality Management</strong> (validation criteria and performance metrics).
                        </p>
                    </div>

                    <!-- Dataset 1: Occluded Object Reasoning Dataset -->
                    <div id="dataset-1" class="mb-8">
                        <h3 class="text-2xl font-semibold themeable-heading mb-4 flex items-center">
                            <span class="teal-text mr-2">1.</span>
                            Occluded Object Reasoning Dataset
                        </h3>

                        <!-- 1-1: Overview & Structure -->
                        <div class="themeable-bg card-hover rounded-lg p-6 mb-6">
                            <h4 class="text-xl font-semibold mb-4 teal-text">1-1. Overview & Structure</h4>

                            <div class="themeable-text-secondary space-y-3 mb-4">
                                <p>
                                    For robots to perform missions in real industrial and living spaces beyond controlled laboratory environments, the ability to cope with unpredictable complexity is essential. In particular, occlusion -- where a target object is partially or fully hidden by other objects -- is one of the greatest challenges in robot vision technology. The 'Occluded Object Reasoning Dataset' is key data for solving this problem, with strategic importance in training robots to accurately infer the complete shape and position of objects from incomplete visual information alone. This dataset serves as the cornerstone of foundational technology that directly contributes to advancing sophisticated object recognition, grasping, and manipulation capabilities, from picking robots in logistics facilities to assembly robots in smart factories.
                                </p>
                            </div>

                            <details class="mt-4">
                                <summary class="cursor-pointer font-semibold teal-text hover:text-orange-500 transition-colors">
                                    View QA Samples (4 pairs)
                                </summary>
                                <div class="mt-4 overflow-x-auto">
                                    <table class="w-full border-collapse text-sm">
                                        <thead>
                                            <tr>
                                                <th class="border themeable-border px-3 py-2 text-left w-32">Question Type</th>
                                                <th class="border themeable-border px-3 py-2 text-left">Question</th>
                                                <th class="border themeable-border px-3 py-2 text-left">Answer</th>
                                            </tr>
                                        </thead>
                                        <tbody class="themeable-text-secondary">
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">Simple Extraction</strong></td>
                                                <td class="border themeable-border px-3 py-2">Which lead organization and participating organizations were responsible for data collection in the 'Occluded Object Reasoning Data' construction project?</td>
                                                <td class="border themeable-border px-3 py-2">The lead organization is 'Saram-gwa-Sup,' and the participating organizations are 'Gwangju Institute of Science and Technology (GIST)' and 'Hanaleum Information.'</td>
                                            </tr>
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">Summary &amp; Explanation</strong></td>
                                                <td class="border themeable-border px-3 py-2">Please describe the four major data types that constitute the folders of this dataset.</td>
                                                <td class="border themeable-border px-3 py-2">The dataset consists of four major folders: 'Object 3D Scan Data,' 'Multi-Object Occlusion Data,' 'Robot-Object Grasping Data,' and 'Human-Object Grasping Data.'</td>
                                            </tr>
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">Comparison &amp; Analysis</strong></td>
                                                <td class="border themeable-border px-3 py-2">Compare the objects in the 'Industrial' category with those in the 'Logistics' category included in the dataset, and analyze the characteristics of each category.</td>
                                                <td class="border themeable-border px-3 py-2">The 'Industrial' category consists of individual functional objects such as 'parts,' 'tools,' and '3D printed parts' (e.g., brackets, spanners, gearboxes). In contrast, the 'Logistics' category centers on commercialized objects designed for packaging and transport, such as 'Amazon logistics,' 'cardboard-type packages,' and 'box-type packages' (e.g., crayon boxes, CPU boxes). This reflects the distinct characteristics of different robot application fields: assembly/maintenance in industrial settings vs. picking/sorting in logistics settings.</td>
                                            </tr>
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">Reasoning &amp; Application</strong></td>
                                                <td class="border themeable-border px-3 py-2">To achieve the stated impact of 'revitalizing related industrial fields such as logistics through robot arm object grasping, manipulation, transport, and placement,' please infer what specific problems this data can contribute to solving.</td>
                                                <td class="border themeable-border px-3 py-2">In logistics settings, products of various sizes and shapes are often randomly stacked in boxes or overlapping on shelves. This dataset trains robots to accurately infer the complete shape and position (6D Pose) of occluded products, enabling them to plan grasping sequences by determining which products must be removed first to reach the target. This directly contributes to revitalizing the logistics industry by reducing picking task failure rates and maximizing automation efficiency.</td>
                                            </tr>
                                        </tbody>
                                    </table>
                                </div>
                            </details>

                            <p class="themeable-text-secondary mt-4 text-sm italic">
                                The next section examines the data collection environment and tools that ensure the quality of this critical data in greater detail.
                            </p>
                        </div>

                        <!-- 1-2: Collection Environment & Tools -->
                        <div class="themeable-bg card-hover rounded-lg p-6 mb-6">
                            <h4 class="text-xl font-semibold mb-4 teal-text">1-2. Collection Environment & Tools</h4>

                            <div class="themeable-text-secondary space-y-3 mb-4">
                                <p>
                                    The value of AI training datasets is determined not only by the volume of data but also by its quality and reproducibility. In particular, to ensure generalization performance of robot vision models, the specific specifications of the collection environment, location, and tools used are critically important. This prevents AI models from overfitting to specific environments and enables robust performance across diverse real-world situations. Therefore, information about the standardized collection environment and precision tools used in building the 'Occluded Object Reasoning Dataset' is a key factor in enhancing dataset reliability.
                                </p>
                            </div>

                            <details class="mt-4">
                                <summary class="cursor-pointer font-semibold teal-text hover:text-orange-500 transition-colors">
                                    View QA Samples (4 pairs)
                                </summary>
                                <div class="mt-4 overflow-x-auto">
                                    <table class="w-full border-collapse text-sm">
                                        <thead>
                                            <tr>
                                                <th class="border themeable-border px-3 py-2 text-left w-32">Question Type</th>
                                                <th class="border themeable-border px-3 py-2 text-left">Question</th>
                                                <th class="border themeable-border px-3 py-2 text-left">Answer</th>
                                            </tr>
                                        </thead>
                                        <tbody class="themeable-text-secondary">
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">Simple Extraction</strong></td>
                                                <td class="border themeable-border px-3 py-2">What is the model name of the 3D scanner used for data collection?</td>
                                                <td class="border themeable-border px-3 py-2">The Artec 3D Leo tool was used to generate RGB-D object 3D scan raw data.</td>
                                            </tr>
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">Summary &amp; Explanation</strong></td>
                                                <td class="border themeable-border px-3 py-2">Please summarize three key features of the 'Multi-Object Occlusion Data' collection environment.</td>
                                                <td class="border themeable-border px-3 py-2">The key features are as follows. First, three representative robot environments were configured: desk, shelf, and box. Second, data diversity was ensured by equally utilizing 5 furniture pieces and 5 backgrounds per environment. Third, IKEA furniture was used so that researchers worldwide could replicate the same environment.</td>
                                            </tr>
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">Comparison &amp; Analysis</strong></td>
                                                <td class="border themeable-border px-3 py-2">Analyze the diversity of grippers used in robot-object grasping data collection, and explain the positive impact of this selection on the dataset's usefulness.</td>
                                                <td class="border themeable-border px-3 py-2">The dataset achieved morphological diversity by including 1-finger (1 type), 2-finger (3 types), 3-finger (2 types), 4-finger (1 type), and 5-finger (1 type) grippers. In particular, 2-finger and 3-finger grippers, which are most commonly used in research, were used in 3 and 2 variants respectively to increase data density. This configuration enables the development of universal grasping algorithms applicable to various robot hands without overfitting to specific gripper types, significantly enhancing the dataset's practical usefulness.</td>
                                            </tr>
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">Reasoning &amp; Application</strong></td>
                                                <td class="border themeable-border px-3 py-2">It was mentioned that various lighting conditions, camera viewpoints, and types were used during data collection. If a robot trained with this dataset were deployed in an actual logistics warehouse with dim lighting or heavy shadows, what performance could be expected?</td>
                                                <td class="border themeable-border px-3 py-2">As stated in the source context, the data was acquired under various lighting conditions. Therefore, robot vision algorithms trained on this data are expected to exhibit robust performance against illumination changes. Even in actual logistics warehouse environments with dim or shadow-heavy lighting, there is a high likelihood of more stable extraction and recognition of object feature points, which can directly lead to improved grasping success rates.</td>
                                            </tr>
                                        </tbody>
                                    </table>
                                </div>
                            </details>

                            <p class="themeable-text-secondary mt-4 text-sm italic">
                                Next, we examine what learning models can be developed and applied based on this systematically collected data.
                            </p>
                        </div>

                        <!-- 1-3: Learning Models & Applications -->
                        <div class="themeable-bg card-hover rounded-lg p-6 mb-6">
                            <h4 class="text-xl font-semibold mb-4 teal-text">1-3. Learning Models & Applications</h4>

                            <div class="themeable-text-secondary space-y-3 mb-4">
                                <p>
                                    The ultimate value of a high-quality dataset is realized when it contributes to solving real industrial problems through superior AI models. A dataset alone is merely potential; its value can only be realized through learning models that interpret and utilize it. This section analyzes representative AI models used to validate the effectiveness and maximize the utilization potential of the 'Occluded Object Reasoning Dataset.' By illuminating the concrete application services that can be realized, we explore how data is transformed into practical technology.
                                </p>
                            </div>

                            <details class="mt-4">
                                <summary class="cursor-pointer font-semibold teal-text hover:text-orange-500 transition-colors">
                                    View QA Samples (4 pairs)
                                </summary>
                                <div class="mt-4 overflow-x-auto">
                                    <table class="w-full border-collapse text-sm">
                                        <thead>
                                            <tr>
                                                <th class="border themeable-border px-3 py-2 text-left w-32">Question Type</th>
                                                <th class="border themeable-border px-3 py-2 text-left">Question</th>
                                                <th class="border themeable-border px-3 py-2 text-left">Answer</th>
                                            </tr>
                                        </thead>
                                        <tbody class="themeable-text-secondary">
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">Simple Extraction</strong></td>
                                                <td class="border themeable-border px-3 py-2">Among the models used for 'amodal instance segmentation,' what is the name of the GIST-developed model selected as the best-performing model as of 2022?</td>
                                                <td class="border themeable-border px-3 py-2">It is UOAIS-Net. This model was proposed to detect occluded regions of unseen objects as well.</td>
                                            </tr>
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">Summary &amp; Explanation</strong></td>
                                                <td class="border themeable-border px-3 py-2">Please explain the key principle by which the 6D Object Pose Estimation model 'PVNet' improves performance in occluded environments.</td>
                                                <td class="border themeable-border px-3 py-2">PVNet improves performance through per-object Keypoint Vector Field prediction. That is, by having each pixel in the image predict a direction vector pointing toward a specific keypoint of the object, the positions of occluded keypoints can be estimated through pixel voting from visible parts, even when portions of the object are hidden, thus improving occluded object pose estimation performance.</td>
                                            </tr>
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">Comparison &amp; Analysis</strong></td>
                                                <td class="border themeable-border px-3 py-2">What is the most significant structural difference between instance segmentation models Mask R-CNN and SOLOv2, and how does this difference affect efficiency?</td>
                                                <td class="border themeable-border px-3 py-2">The biggest difference is that Mask R-CNN has a 'two-stage' architecture while SOLOv2 has a 'single-stage' architecture. The two-stage Mask R-CNN first extracts region proposal candidates and then predicts masks for each candidate region. In contrast, the single-stage SOLOv2 processes both steps simultaneously. Due to this structural difference, SOLOv2 achieves faster prediction speed by eliminating the inefficiencies of the two-stage architecture.</td>
                                            </tr>
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">Reasoning &amp; Application</strong></td>
                                                <td class="border themeable-border px-3 py-2">Based on GIST AILAB's 'Robot Occluded Object Grasping Sequence Planning' application service example, infer how applying this technology to a smart factory parts assembly line could improve productivity.</td>
                                                <td class="border themeable-border px-3 py-2">In parts assembly lines, various components are often stacked and overlapping in parts bins. Applying the technology presented in the source context, when a target part is occluded by other parts, the robot arm can accurately recognize occluded and visible regions and autonomously determine which parts to remove first to reach the target, planning the sequence accordingly. This eliminates the need for human intervention to rearrange parts and reduces robot idle time, significantly improving productivity by shortening the overall assembly line cycle time.</td>
                                            </tr>
                                        </tbody>
                                    </table>
                                </div>
                            </details>

                            <p class="themeable-text-secondary mt-4 text-sm italic">
                                Having covered object recognition in logistics and industrial environments, the next chapter continues the discussion by examining the characteristics of another key dataset: 'Delivery Robot Off-Road Navigation Data.'
                            </p>
                        </div>
                    </div>

                    <!-- Dataset 2: Delivery Robot Off-Road Navigation Dataset -->
                    <div id="dataset-2" class="mb-8">
                        <h3 class="text-2xl font-semibold themeable-heading mb-4 flex items-center">
                            <span class="teal-text mr-2">2.</span>
                            Delivery Robot Off-Road Navigation Dataset
                        </h3>

                        <!-- 2-1: Overview & Labeling -->
                        <div class="themeable-bg card-hover rounded-lg p-6 mb-6">
                            <h4 class="text-xl font-semibold mb-4 teal-text">2-1. Overview & Labeling</h4>

                            <div class="themeable-text-secondary space-y-3 mb-4">
                                <p>
                                    As the autonomous navigation domain for robots expands beyond well-maintained 'roads' to 'off-road' environments with many unpredictable elements such as sidewalks, parks, and indoor spaces, new types of datasets are becoming essential. Off-road environments contain complexities that cannot be addressed by conventional road driving data, including unspecified pedestrians, various types of obstacles, and unclear boundaries. This section analyzes the overview of the 'Delivery Robot Off-Road Navigation Dataset' and its core components: the labeling class definitions for 2D and 3D data. Through this, we describe the importance of how this dataset enables AI to structurally understand the complexity of off-road environments.
                                </p>
                            </div>

                            <details class="mt-4">
                                <summary class="cursor-pointer font-semibold teal-text hover:text-orange-500 transition-colors">
                                    View QA Samples (4 pairs)
                                </summary>
                                <div class="mt-4 overflow-x-auto">
                                    <table class="w-full border-collapse text-sm">
                                        <thead>
                                            <tr>
                                                <th class="border themeable-border px-3 py-2 text-left w-32">Question Type</th>
                                                <th class="border themeable-border px-3 py-2 text-left">Question</th>
                                                <th class="border themeable-border px-3 py-2 text-left">Answer</th>
                                            </tr>
                                        </thead>
                                        <tbody class="themeable-text-secondary">
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">Simple Extraction</strong></td>
                                                <td class="border themeable-border px-3 py-2">What is the labeling type of the 3D LiDAR data, and how many processing target classes are defined in total?</td>
                                                <td class="border themeable-border px-3 py-2">The labeling type is 'Cuboid,' and there are 9 processing target classes in total: 'passenger car,' 'motorcycle,' 'bicycle,' 'kickboard,' 'pedestrian,' 'bus,' 'truck,' 'other dynamic objects,' and 'other static objects.'</td>
                                            </tr>
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">Summary &amp; Explanation</strong></td>
                                                <td class="border themeable-border px-3 py-2">Among the Semantic Segmentation processing principles for 2D image data, how are cases handled where the same object is occluded by another object (Area A) or where pedestrians with expected sudden behavior are occluded (Area B)?</td>
                                                <td class="border themeable-border px-3 py-2">For Area A, the same object is divided into two regions and annotated separately. For Area B, pedestrians who pose a safety risk are processed with the 'is_crowd' (occluded) attribute, enabling the robot to recognize and prevent potential dangers.</td>
                                            </tr>
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">Comparison &amp; Analysis</strong></td>
                                                <td class="border themeable-border px-3 py-2">Compare the 2D processing target classes (22 types) with the 3D processing target classes (9 types), and analyze the reasons for the difference in relation to sensor characteristics.</td>
                                                <td class="border themeable-border px-3 py-2">2D classes are rich in information about the surrounding environment and background such as 'road,' 'sidewalk,' 'signs,' 'traffic lights,' and 'buildings,' while 3D classes focus primarily on dynamic or static 'objects' such as 'passenger cars' and 'pedestrians.' This is due to the information collection characteristics of each sensor. 2D images are advantageous for recognizing planar environmental information such as drivable areas and signs through color and texture information, while 3D LiDAR has greater strength in accurately detecting the position and size of three-dimensional objects through distance and shape information. In other words, the labeling targets were optimized according to each sensor's information collection method.</td>
                                            </tr>
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">Reasoning &amp; Application</strong></td>
                                                <td class="border themeable-border px-3 py-2">The JSON structure of the labeling data includes a 'sidewalk_flatness' (road surface flatness) attribute. Infer how delivery robots could utilize this attribute data for safer and more efficient navigation.</td>
                                                <td class="border themeable-border px-3 py-2">Delivery robots can learn from 'sidewalk_flatness' data to proactively identify sections with 'low' flatness. This allows them to autonomously decelerate before entering such sections or switch to a driving mode that minimizes vibration, reducing the risk of damage to items being delivered. Additionally, as this information accumulates, it can enhance path planning algorithms that prioritize the smoothest routes, contributing to long-term reduction of mechanical wear on robots and maintenance cost savings.</td>
                                            </tr>
                                        </tbody>
                                    </table>
                                </div>
                            </details>

                            <p class="themeable-text-secondary mt-4 text-sm italic">
                                Since data quality largely depends on the collection location and environment, the next section provides a detailed analysis of where and how this dataset was collected.
                            </p>
                        </div>

                        <!-- 2-2: Collection Locations & Environment -->
                        <div class="themeable-bg card-hover rounded-lg p-6 mb-6">
                            <h4 class="text-xl font-semibold mb-4 teal-text">2-2. Collection Locations & Environment</h4>

                            <div class="themeable-text-secondary space-y-3 mb-4">
                                <p>
                                    For AI models to be universally applicable across diverse real-world environments rather than becoming 'greenhouse flowers' that only work in specific settings, securing comprehensiveness and realism from the data collection stage is paramount. The choice of data collection locations directly determines dataset diversity, which is ultimately the key variable that determines an AI model's generalization performance. This section analyzes the environmental characteristics of the specific locations where the 'Delivery Robot Off-Road Navigation Dataset' was collected, evaluating the strategic significance of how these location selections guarantee the dataset's realism and how well they reflect the actual challenges robots will face.
                                </p>
                            </div>

                            <details class="mt-4">
                                <summary class="cursor-pointer font-semibold teal-text hover:text-orange-500 transition-colors">
                                    View QA Samples (4 pairs)
                                </summary>
                                <div class="mt-4 overflow-x-auto">
                                    <table class="w-full border-collapse text-sm">
                                        <thead>
                                            <tr>
                                                <th class="border themeable-border px-3 py-2 text-left w-32">Question Type</th>
                                                <th class="border themeable-border px-3 py-2 text-left">Question</th>
                                                <th class="border themeable-border px-3 py-2 text-left">Answer</th>
                                            </tr>
                                        </thead>
                                        <tbody class="themeable-text-secondary">
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">Simple Extraction</strong></td>
                                                <td class="border themeable-border px-3 py-2">Among the data collection locations, which location enabled free autonomous driving operation through a regulatory sandbox?</td>
                                                <td class="border themeable-border px-3 py-2">It is Sejong Central Park.</td>
                                            </tr>
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">Summary &amp; Explanation</strong></td>
                                                <td class="border themeable-border px-3 py-2">Please explain from two perspectives why 'Bukchon Hanok Village' is considered an optimal location for verifying off-road environment driving robots.</td>
                                                <td class="border themeable-border px-3 py-2">First, it includes both narrow roads where vehicles can pass and alley environments where vehicle entry is restricted, enabling testing of various off-road conditions. Second, as a complex cultural heritage area where residential and commercial zones coexist, it provides a realistic environment with diverse types of pedestrians and static/dynamic obstacles.</td>
                                            </tr>
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">Comparison &amp; Analysis</strong></td>
                                                <td class="border themeable-border px-3 py-2">Compare the environmental characteristics of the outdoor collection location 'Insadong' and the indoor collection location 'Naver New Headquarters (1784),' and analyze what different challenges each location presents for robot autonomous driving technology verification.</td>
                                                <td class="border themeable-border px-3 py-2">'Insadong' is alley-centric with very high pedestrian density, focusing on verification of dynamic obstacle avoidance technology for safe navigation while avoiding unpredictable movements among numerous unspecified pedestrians. In contrast, 'Naver New Headquarters' is a refined indoor space where actual robot services operate, suitable for verification of more precise SLAM and localization technologies, including accurate recognition of narrow corridors, glass walls, and indoor structures, and building continuous indoor-outdoor driving datasets.</td>
                                            </tr>
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">Reasoning &amp; Application</strong></td>
                                                <td class="border themeable-border px-3 py-2">This dataset was collected during 'summer' and 'autumn.' If a robot trained with this data were operated in off-road environments during a snowy 'winter,' what potential problems could arise, and what additional data would be needed to resolve them?</td>
                                                <td class="border themeable-border px-3 py-2">According to the given information, data was only collected during summer and autumn. Therefore, in winter, snow would blur the distinction between lanes, sidewalks, and curbs, and LiDAR sensors could misidentify snow as obstacles or suffer performance degradation due to diffuse reflection. Additionally, pedestrian walking patterns (slipping, etc.) would differ. Consequently, driving performance in winter could be significantly degraded with the current dataset alone. To resolve this, winter-specific driving data (images and LiDAR) from snow-covered environments, snowy conditions, and icy roads should be additionally collected along with corresponding labeling data (e.g., adding 'snow-covered sidewalk' and 'icy road' classes) for model retraining.</td>
                                            </tr>
                                        </tbody>
                                    </table>
                                </div>
                            </details>

                            <p class="themeable-text-secondary mt-4 text-sm italic">
                                Next, we explore the learning models developed and verified using data collected from these diverse environments, and their application potential.
                            </p>
                        </div>

                        <!-- 2-3: Learning Models & Applications -->
                        <div class="themeable-bg card-hover rounded-lg p-6 mb-6">
                            <h4 class="text-xl font-semibold mb-4 teal-text">2-3. Learning Models & Applications</h4>

                            <div class="themeable-text-secondary space-y-3 mb-4">
                                <p>
                                    The effectiveness of a dataset is ultimately proven through AI model performance and real-world application potential. No matter how vast and diverse the data, it merely occupies storage space without models that can effectively learn from and utilize it. This section analyzes the 2D and 3D learning models selected to validate the 'Delivery Robot Off-Road Navigation Dataset.' We evaluate the technical value of the data through each model's selection rationale and development details, and assess the industrial value by examining the specific application services that can be realized through these models.
                                </p>
                            </div>

                            <details class="mt-4">
                                <summary class="cursor-pointer font-semibold teal-text hover:text-orange-500 transition-colors">
                                    View QA Samples (4 pairs)
                                </summary>
                                <div class="mt-4 overflow-x-auto">
                                    <table class="w-full border-collapse text-sm">
                                        <thead>
                                            <tr>
                                                <th class="border themeable-border px-3 py-2 text-left w-32">Question Type</th>
                                                <th class="border themeable-border px-3 py-2 text-left">Question</th>
                                                <th class="border themeable-border px-3 py-2 text-left">Answer</th>
                                            </tr>
                                        </thead>
                                        <tbody class="themeable-text-secondary">
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">Simple Extraction</strong></td>
                                                <td class="border themeable-border px-3 py-2">Among the 3D dynamic object detection model candidates, which model was not finally selected due to poor performance reproducibility of its open-source code?</td>
                                                <td class="border themeable-border px-3 py-2">It is the BtcDet model.</td>
                                            </tr>
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">Summary &amp; Explanation</strong></td>
                                                <td class="border themeable-border px-3 py-2">Please describe the development details of 'ERF-PSPNet,' which was selected as the 2D driving environment recognition model.</td>
                                                <td class="border themeable-border px-3 py-2">This model takes 2D RGB images as input and outputs pixel-wise recognition results for road, sidewalk, curb, crosswalk, and indoor floor surfaces relevant to robot autonomous driving, identifying drivable areas.</td>
                                            </tr>
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">Comparison &amp; Analysis</strong></td>
                                                <td class="border themeable-border px-3 py-2">While 'PointPillars' was selected as the 3D dynamic object detection model, 'VoxelRCNN' was not selected. Compare and analyze the selection and non-selection rationale that can estimate the performance difference between the two models.</td>
                                                <td class="border themeable-border px-3 py-2">PointPillars was selected because it showed good performance on the KITTI dataset with high performance reproducibility, suggesting stable detection performance even in environments with various objects. In contrast, VoxelRCNN was not selected because it exhibited degraded performance when detecting multiple objects simultaneously, which was deemed unsuitable for delivery robot missions that require simultaneous processing of multiple dynamic objects (pedestrians, bicycles, etc.) in complex off-road environments.</td>
                                            </tr>
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">Reasoning &amp; Application</strong></td>
                                                <td class="border themeable-border px-3 py-2">Assume we combine the drivable area recognition model and dynamic object detection model developed with this dataset and apply them to a 'patrol robot in a park environment.' Infer how this robot could perform its patrol mission more safely and intelligently.</td>
                                                <td class="border themeable-border px-3 py-2">The drivable area recognition model helps the robot clearly distinguish sidewalks, grass, and bicycle paths within the park to patrol stably without deviating from designated routes. Simultaneously, the dynamic object detection model detects and predicts dynamic objects in real time, such as suddenly running children, fast-passing bicycles, and people playing ball. Through the combination of these two models, the patrol robot can go beyond simply following a path to make intelligent decisions such as proactively recognizing dangerous situations and autonomously stopping or detour to safe routes, ensuring the safety of park visitors while performing patrol missions.</td>
                                            </tr>
                                        </tbody>
                                    </table>
                                </div>
                            </details>

                            <p class="themeable-text-secondary mt-4 text-sm italic">
                                In the next chapter, we deepen the discussion by analyzing the characteristics of 'Robot-View Driving Video Data,' which focuses more on complex indoor environments as well as outdoor ones.
                            </p>
                        </div>
                    </div>

                    <!-- Dataset 3: Robot-View Driving Video Dataset -->
                    <div id="dataset-3" class="mb-8">
                        <h3 class="text-2xl font-semibold themeable-heading mb-4 flex items-center">
                            <span class="teal-text mr-2">3.</span>
                            Robot-View Driving Video Dataset
                        </h3>

                        <!-- 3-1: Overview & Class Distribution -->
                        <div class="themeable-bg card-hover rounded-lg p-6 mb-6">
                            <h4 class="text-xl font-semibold mb-4 teal-text">3-1. Overview & Class Distribution</h4>

                            <div class="themeable-text-secondary space-y-3 mb-4">
                                <p>
                                    As the operating environment of autonomous robots expands beyond outdoors to complex indoor multi-use facilities such as restaurants, shopping malls, and terminals, the importance of data for understanding and interpreting the world from a robot's perspective is growing. Indoor environments pose significant challenges for robot navigation due to frequent lighting changes, dense dynamic obstacles like people, and numerous unpredictable static obstacles (chairs, tables, etc.). This section analyzes the core data composition of the 'Robot-View Driving Video Dataset' designed to address these challenges, evaluating the dataset's realism and balance by examining class statistics that reflect the object distribution in real environments.
                                </p>
                            </div>

                            <details class="mt-4">
                                <summary class="cursor-pointer font-semibold teal-text hover:text-orange-500 transition-colors">
                                    View QA Samples (4 pairs)
                                </summary>
                                <div class="mt-4 overflow-x-auto">
                                    <table class="w-full border-collapse text-sm">
                                        <thead>
                                            <tr>
                                                <th class="border themeable-border px-3 py-2 text-left w-32">Question Type</th>
                                                <th class="border themeable-border px-3 py-2 text-left">Question</th>
                                                <th class="border themeable-border px-3 py-2 text-left">Answer</th>
                                            </tr>
                                        </thead>
                                        <tbody class="themeable-text-secondary">
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">Simple Extraction</strong></td>
                                                <td class="border themeable-border px-3 py-2">What are the three sensor data formats included in the source data of the 'Robot-View Driving Video Data'?</td>
                                                <td class="border themeable-border px-3 py-2">They are PCD (LIDAR data), CSV (6D IMU Sensor data), and PNG (RGB-D Depth data).</td>
                                            </tr>
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">Summary &amp; Explanation</strong></td>
                                                <td class="border themeable-border px-3 py-2">Describe the two types of robots used to collect data for this dataset, and provide the data construction ratio for each.</td>
                                                <td class="border themeable-border px-3 py-2">Data was collected using a quadruped walking robot (RB1) and a wheeled driving robot (RB2). The data construction ratio is 45.92% (68,980 items) for the quadruped robot and 54.08% (81,249 items) for the wheeled robot.</td>
                                            </tr>
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">Comparison &amp; Analysis</strong></td>
                                                <td class="border themeable-border px-3 py-2">What are the top 3 locations with the highest composition ratios and the bottom 3 locations with the lowest composition ratios in the collection location distribution, and how can the dataset's collection environment characteristics be analyzed through this?</td>
                                                <td class="border themeable-border px-3 py-2">The top 3 locations are medium-sized restaurants (9.53%), banks (9.50%), and terminals (9.38%), while the bottom 3 are large marts (6.60%), exhibition halls (7.09%), and indoor parking lots (7.52%). This distribution shows a relatively even spread between 6.6% and 9.53% without bias toward any specific environment, indicating the intent to ensure data diversity and balance so that robots can be universally deployed across various indoor multi-use facility environments.</td>
                                            </tr>
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">Reasoning &amp; Application</strong></td>
                                                <td class="border themeable-border px-3 py-2">Among static object classes, 'chairs' account for 22.29% and 'pillars' for 9.76%, while the dynamic object 'people' holds an overwhelming 51.41%. What strengths can be inferred for a robot trained with this data during indoor autonomous driving?</td>
                                                <td class="border themeable-border px-3 py-2">According to the source context, 'people,' 'chairs,' and 'pillars' are the most commonly encountered dynamic and static obstacles in indoor environments. The abundance of data for these classes means the robot will specialize in avoiding unpredictable human movements, navigating through randomly arranged chairs, and reliably recognizing fixed structures like pillars. Therefore, this robot can be expected to demonstrate excellent avoidance maneuvers and path planning capabilities even in complex indoor environments with many people and obstacles, such as crowded restaurants or lobbies.</td>
                                            </tr>
                                        </tbody>
                                    </table>
                                </div>
                            </details>

                            <p class="themeable-text-secondary mt-4 text-sm italic">
                                Next, we examine the verification environment and models used to ensure the quality and validity of this dataset.
                            </p>
                        </div>

                        <!-- 3-2: Verification Environment & Models -->
                        <div class="themeable-bg card-hover rounded-lg p-6 mb-6">
                            <h4 class="text-xl font-semibold mb-4 teal-text">3-2. Verification Environment & Models</h4>

                            <div class="themeable-text-secondary space-y-3 mb-4">
                                <p>
                                    Dataset reliability cannot be achieved simply by collecting large volumes of data. Its technical completeness is only proven through rigorous verification procedures conducted with objective performance metrics and standardized hardware and software environments. This section details the hardware and software environments configured to validate the 'Robot-View Driving Video Dataset.' Additionally, we analyze the state-of-the-art learning algorithms used to evaluate performance in 2D bounding box, 3D cuboid, and SLAM (Simultaneous Localization and Mapping), assessing the technical completeness of the data.
                                </p>
                            </div>

                            <details class="mt-4">
                                <summary class="cursor-pointer font-semibold teal-text hover:text-orange-500 transition-colors">
                                    View QA Samples (4 pairs)
                                </summary>
                                <div class="mt-4 overflow-x-auto">
                                    <table class="w-full border-collapse text-sm">
                                        <thead>
                                            <tr>
                                                <th class="border themeable-border px-3 py-2 text-left w-32">Question Type</th>
                                                <th class="border themeable-border px-3 py-2 text-left">Question</th>
                                                <th class="border themeable-border px-3 py-2 text-left">Answer</th>
                                            </tr>
                                        </thead>
                                        <tbody class="themeable-text-secondary">
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">Simple Extraction</strong></td>
                                                <td class="border themeable-border px-3 py-2">What are the name and performance metric of the learning algorithm used for SLAM performance validation?</td>
                                                <td class="border themeable-border px-3 py-2">The learning algorithm is Fast-LIO2, and the performance metric is End to End RMSE (within 0.2m).</td>
                                            </tr>
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">Summary &amp; Explanation</strong></td>
                                                <td class="border themeable-border px-3 py-2">When validating 2D bounding box object detection, please describe the ratio for splitting total data into Training, Validation, and Test sets, and the number of data items in each set.</td>
                                                <td class="border themeable-border px-3 py-2">The total data is split in a Train:Val:Test = 80:10:10 ratio. The Training Set comprises 80% with 120,220 items, the Validation Set comprises 10% with 14,993 items, and the Test Set comprises 10% with 15,016 items.</td>
                                            </tr>
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">Comparison &amp; Analysis</strong></td>
                                                <td class="border themeable-border px-3 py-2">Compare the GPU and OS environments used for 2D object detection (YOLOV7) and 3D object detection (FocalsConv) validation, and explain the differences from the SLAM performance verification environment.</td>
                                                <td class="border themeable-border px-3 py-2">Both 2D and 3D object detection are deep learning-based models, so they identically use 'NVIDIA RTX A6000 * 4' GPUs and 'Ubuntu 18.04.6 LTS' for large-scale parallel computation. In contrast, the Fast-LIO2 algorithm used for SLAM performance verification is not an AI model, so it uses only CPU (AMD EPYC 7742) without GPU, and the OS also differs with 'Ubuntu 20.04.' This shows that optimized environments were configured according to the computational characteristics of each verification target algorithm.</td>
                                            </tr>
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">Reasoning &amp; Application</strong></td>
                                                <td class="border themeable-border px-3 py-2">The 3D object detection model FocalsConv is said to combine RGB (3 channels) with Depth map (1 channel). Infer what advantages this approach could have over using only LiDAR data, particularly when recognizing objects like 'glass doors.'</td>
                                                <td class="border themeable-border px-3 py-2">LiDAR sensors may fail to properly recognize transparent 'glass doors,' passing through them or treating them as noise. However, RGB cameras can capture visual features such as the door frame, handle, and surface reflections. By combining RGB and Depth information like the FocalsConv model, the visual information from the camera can compensate for what LiDAR misses. That is, Depth information detects the approximate presence of a plane, and RGB information identifies the visual context that it is a 'door,' significantly increasing the probability that the robot accurately recognizes glass doors as obstacles rather than misidentifying them as passable spaces.</td>
                                            </tr>
                                        </tbody>
                                    </table>
                                </div>
                            </details>

                            <p class="themeable-text-secondary mt-4 text-sm italic">
                                While we have focused on the robot's interaction with the external environment through 'perception' and 'navigation,' the next chapter analyzes datasets for diagnosing the robot's own 'status' and 'maintenance.'
                            </p>
                        </div>
                    </div>

                    <!-- Dataset 4: Indoor Space Maintenance Service Robot Dataset -->
                    <div id="dataset-4" class="mb-8">
                        <h3 class="text-2xl font-semibold themeable-heading mb-4 flex items-center">
                            <span class="teal-text mr-2">4.</span>
                            Indoor Space Maintenance Service Robot Dataset
                        </h3>

                        <!-- 4-1: Overview & Format -->
                        <div class="themeable-bg card-hover rounded-lg p-6 mb-6">
                            <h4 class="text-xl font-semibold mb-4 teal-text">4-1. Overview & Format</h4>

                            <div class="themeable-text-secondary space-y-3 mb-4">
                                <p>
                                    For service robots to reliably proliferate across industrial sites and everyday spaces, maintenance technology that goes beyond simply performing assigned tasks to self-diagnose conditions and preemptively predict failures is essential. This requires data that records the robot's internal state in real-time. This section highlights the unique characteristics of the 'Indoor Space Maintenance Service Robot Dataset' built for this purpose -- namely, that it consists of text-based status data rather than images or video. By analyzing the detailed JSON data format, we evaluate what information this data provides for developing AI models for robot state reasoning and anomaly detection.
                                </p>
                            </div>

                            <details class="mt-4">
                                <summary class="cursor-pointer font-semibold teal-text hover:text-orange-500 transition-colors">
                                    View QA Samples (4 pairs)
                                </summary>
                                <div class="mt-4 overflow-x-auto">
                                    <table class="w-full border-collapse text-sm">
                                        <thead>
                                            <tr>
                                                <th class="border themeable-border px-3 py-2 text-left w-32">Question Type</th>
                                                <th class="border themeable-border px-3 py-2 text-left">Question</th>
                                                <th class="border themeable-border px-3 py-2 text-left">Answer</th>
                                            </tr>
                                        </thead>
                                        <tbody class="themeable-text-secondary">
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">Simple Extraction</strong></td>
                                                <td class="border themeable-border px-3 py-2">In the JSON format of the labeling data, what are the attribute names (keys) indicating whether a robot error has occurred and the error code, respectively?</td>
                                                <td class="border themeable-border px-3 py-2">Error occurrence is 'errorState' (boolean), and error code is 'errorCode' (String).</td>
                                            </tr>
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">Summary &amp; Explanation</strong></td>
                                                <td class="border themeable-border px-3 py-2">Please describe the five pieces of information whose combination constitutes the filename of this dataset, in order.</td>
                                                <td class="border themeable-border px-3 py-2">The filename consists of 'robot type,' 'unique ID assigned to the robot,' 'month the data was generated,' 'task ID the robot performs,' and 'data sequence number,' in that order. (e.g., GuideRobot_GuideRobot01_11_task01_05233.json)</td>
                                            </tr>
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">Comparison &amp; Analysis</strong></td>
                                                <td class="border themeable-border px-3 py-2">Compare the information types of the 'deviceData' object and the 'taskData' object included in the source data, and analyze how these two types of information can be used complementarily from a robot maintenance perspective.</td>
                                                <td class="border themeable-border px-3 py-2">'deviceData' contains the robot's physical and hardware state information such as battery level, collision count, and current speed. In contrast, 'taskData' contains logical and software state information of the mission the robot performs, such as task name, estimated duration, and current task status. By combining both, for example, one can discover that 'collision' counts spike only during specific 'tasks' to diagnose path planning issues, or identify that 'batteryLevel' depletes unusually fast during certain 'tasks' to analyze that the mission is overloading the robot -- enabling diagnosis of complex problem causes between task and state rather than simple hardware failures.</td>
                                            </tr>
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">Reasoning &amp; Application</strong></td>
                                                <td class="border themeable-border px-3 py-2">The 'errorStatementLong' attribute in the labeling data describes error situations in detailed natural language sentences. If this data were used for LLM fine-tuning, infer what practical help it could provide to robot control system operators.</td>
                                                <td class="border themeable-border px-3 py-2">Simple error codes like 'E-ENV-O' alone make it difficult for operators to immediately identify the cause of problems. However, an LLM fine-tuned with 'errorStatementLong' data ("The current robot... judged to be an immobility error due to obstacle detection...") can automatically convert the code's meaning into sentences including specific situations and causes, such as "The serving robot is currently immobile due to obstacle detection in a congested office." Furthermore, the LLM can suggest solutions such as "It is recommended to check surrounding obstacles and reset the route, or wait until the congestion level in the area decreases." This would shorten the operator's situation assessment time and enable rapid and accurate initial response, significantly improving robot operation efficiency.</td>
                                            </tr>
                                        </tbody>
                                    </table>
                                </div>
                            </details>

                            <p class="themeable-text-secondary mt-4 text-sm italic">
                                Having examined robot perception of external environments through internal state diagnosis, we now shift the analytical focus to 'Robot Hand Object Property Identification Data' related to the robot's core function of 'manipulation.'
                            </p>
                        </div>
                    </div>

                    <!-- Dataset 5: Robot Hand Object Property Identification Dataset -->
                    <div id="dataset-5" class="mb-8">
                        <h3 class="text-2xl font-semibold themeable-heading mb-4 flex items-center">
                            <span class="teal-text mr-2">5.</span>
                            Robot Hand Object Property Identification Dataset
                        </h3>

                        <!-- 5-1: Overview & Structure -->
                        <div class="themeable-bg card-hover rounded-lg p-6 mb-6">
                            <h4 class="text-xl font-semibold mb-4 teal-text">5-1. Overview & Structure</h4>

                            <div class="themeable-text-secondary space-y-3 mb-4">
                                <p>
                                    For robots to handle diverse objects as skillfully as humans, it is essential to go beyond simply seeing and recognizing shapes to understanding physical properties such as weight, material, and rigidity. Without understanding these physical properties, manipulation requiring delicate force control is impossible. This section analyzes the multi-modal composition of 'Robot Hand Object Property Identification Data' built to address these challenges. We evaluate the importance of how visual information (images, 3D meshes) and physical information (physical quantities, time-series sensor data) are integrated to establish the foundation for object manipulation intelligence.
                                </p>
                            </div>

                            <details class="mt-4">
                                <summary class="cursor-pointer font-semibold teal-text hover:text-orange-500 transition-colors">
                                    View QA Samples (4 pairs)
                                </summary>
                                <div class="mt-4 overflow-x-auto">
                                    <table class="w-full border-collapse text-sm">
                                        <thead>
                                            <tr>
                                                <th class="border themeable-border px-3 py-2 text-left w-32">Question Type</th>
                                                <th class="border themeable-border px-3 py-2 text-left">Question</th>
                                                <th class="border themeable-border px-3 py-2 text-left">Answer</th>
                                            </tr>
                                        </thead>
                                        <tbody class="themeable-text-secondary">
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">Simple Extraction</strong></td>
                                                <td class="border themeable-border px-3 py-2">How many types of household items are targeted for dataset construction, and what is the target image data quantity per item?</td>
                                                <td class="border themeable-border px-3 py-2">There are 200 types of target items in total, with the goal of collecting at least 600 images each of Low-RGB, Hi-RGB, and RGB-D per item.</td>
                                            </tr>
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">Summary &amp; Explanation</strong></td>
                                                <td class="border themeable-border px-3 py-2">Please list the five robot hand manipulation tasks that constitute the 'mission data' of this dataset.</td>
                                                <td class="border themeable-border px-3 py-2">It consists of five manipulation tasks: 'grasping,' 'squeezing,' 'rotating,' 'shaking,' and 'scratching.'</td>
                                            </tr>
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">Comparison &amp; Analysis</strong></td>
                                                <td class="border themeable-border px-3 py-2">Describe the differences between 'Hi-RGB' and 'Low-RGB' images in the 'item data,' and analyze how collecting both types benefits the robustness of robot vision models.</td>
                                                <td class="border themeable-border px-3 py-2">'Hi-RGB' is a high-resolution, uncompressed image in 12bit raw data format, while 'Low-RGB' is a lossy compressed image in 8bit jpg format. High-quality 'Hi-RGB' data is advantageous for learning fine object features, while low-quality 'Low-RGB' data simulates realistic situations where low-spec cameras on actual robots or quality degradation during communication occurs. By training on both types, robot vision models can achieve robustness with stable recognition performance across various quality image inputs.</td>
                                            </tr>
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">Reasoning &amp; Application</strong></td>
                                                <td class="border themeable-border px-3 py-2">This dataset includes physical quantity information such as object 'weight,' 'size,' and 'material' as well as time-series data like 'tactile' and 'force' feedback. When a robot attempts to lift an object it sees for the first time, infer how it could comprehensively utilize this data to grasp the object safely and efficiently.</td>
                                                <td class="border themeable-border px-3 py-2">The robot first uses visual information (images, 3D meshes) to determine which objects in the dataset are similar to the unfamiliar object. Based on the 'material' and 'size' information of similar objects, it predicts the surface slipperiness and center of gravity, and estimates the minimum force needed to lift the object from 'weight' information. Then, as grasping begins, it receives real-time feedback from 'tactile' and 'force' sensor data, enabling fine adjustment of grasping force to prevent the object from slipping or being crushed. This enables sophisticated and stable grasping based on physical properties, which is impossible with visual information alone.</td>
                                            </tr>
                                        </tbody>
                                    </table>
                                </div>
                            </details>

                            <p class="themeable-text-secondary mt-4 text-sm italic">
                                Next, we examine in detail the specific tools and standardized procedures used to collect this rich multi-modal data.
                            </p>
                        </div>

                        <!-- 5-2: Collection Tools & Procedures -->
                        <div class="themeable-bg card-hover rounded-lg p-6 mb-6">
                            <h4 class="text-xl font-semibold mb-4 teal-text">5-2. Collection Tools & Procedures</h4>

                            <div class="themeable-text-secondary space-y-3 mb-4">
                                <p>
                                    Reliable datasets are built through standardized collection tools and systematic procedures, not intuition or subjectivity. The precision of sensors used for data collection and consistent collection procedures guarantee data quality, forming the foundation that ensures the reproducibility and reliability of AI models trained on this data. This section analyzes the specifications of key collection tools, including high-precision sensors and robot hands, used to build the 'Robot Hand Object Property Identification Dataset.' Additionally, we illuminate the quality assurance process by explaining the step-by-step collection procedures through which data is consistently acquired.
                                </p>
                            </div>

                            <details class="mt-4">
                                <summary class="cursor-pointer font-semibold teal-text hover:text-orange-500 transition-colors">
                                    View QA Samples (4 pairs)
                                </summary>
                                <div class="mt-4 overflow-x-auto">
                                    <table class="w-full border-collapse text-sm">
                                        <thead>
                                            <tr>
                                                <th class="border themeable-border px-3 py-2 text-left w-32">Question Type</th>
                                                <th class="border themeable-border px-3 py-2 text-left">Question</th>
                                                <th class="border themeable-border px-3 py-2 text-left">Answer</th>
                                            </tr>
                                        </thead>
                                        <tbody class="themeable-text-secondary">
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">Simple Extraction</strong></td>
                                                <td class="border themeable-border px-3 py-2">What are the measurement range and measurement error of the 'digital vernier caliper' used to measure object size?</td>
                                                <td class="border themeable-border px-3 py-2">The measurement range is 0-150mm, and the measurement error is 0.2mm.</td>
                                            </tr>
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">Summary &amp; Explanation</strong></td>
                                                <td class="border themeable-border px-3 py-2">Please describe the features of the 'photography jig' configured to photograph 200 items under consistent conditions.</td>
                                                <td class="border themeable-border px-3 py-2">The photography jig consists of brackets physically fixed to a surface plate, 5 sets of cameras and RGB-D sensors, and a rotating turntable for placing objects. This enables systematic acquisition of data from multiple viewpoints by rotating objects at consistent angles.</td>
                                            </tr>
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">Comparison &amp; Analysis</strong></td>
                                                <td class="border themeable-border px-3 py-2">Analyze the respective roles of the 'Allegro Hand' and 'finger-type tactile sensor' used in data collection, and explain how the combination of these two tools enables data collection similar to a human hand.</td>
                                                <td class="border themeable-border px-3 py-2">The 'Allegro Hand' controls 16 independent joints to perform 'motions' similar to a human hand, such as grasping and squeezing. The 'finger-type tactile sensors' attached to it collect pressure distribution data, or 'sensory' data, generated when contacting objects during those motions. The combination of both enables the robot to go beyond simply grasping objects to digitize subtle force changes and contact states during grasping, simulating human 'tactile-accompanied manipulation.'</td>
                                            </tr>
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">Reasoning &amp; Application</strong></td>
                                                <td class="border themeable-border px-3 py-2">There is a data collection procedure where 'force levels are divided into 5 stages with 10 mission executions per stage during robot hand task performance.' Infer what advantages this fine-grained force level data collection approach could provide when developing robots that handle objects like 'fragile eggs' or 'soft tofu.'</td>
                                                <td class="border themeable-border px-3 py-2">Collecting data with fine-grained force levels enables building precise models of how objects deform and how sensor (tactile, force) values change at each force level. Through this, when handling delicate objects like 'eggs' or 'tofu,' the robot can learn the critical force threshold just before the object breaks. Consequently, the robot acquires sophisticated force control capabilities such as stably grasping objects with minimal force or immediately reducing force upon detecting object deformation, significantly enhancing the practicality of domestic assistance robots.</td>
                                            </tr>
                                        </tbody>
                                    </table>
                                </div>
                            </details>

                            <p class="themeable-text-secondary mt-4 text-sm italic">
                                Having analyzed sophisticated robot manipulation, we next examine 'Hand-Arm Coordinated Grasp-Manipulation Motion Data' for understanding and imitating the motions of 'humans' who are the subjects of such manipulation.
                            </p>
                        </div>
                    </div>

                    <!-- Dataset 6: Hand-Arm Coordinated Grasp-Manipulation Motion Dataset -->
                    <div id="dataset-6" class="mb-8">
                        <h3 class="text-2xl font-semibold themeable-heading mb-4 flex items-center">
                            <span class="teal-text mr-2">6.</span>
                            Hand-Arm Coordinated Grasp-Manipulation Motion Dataset
                        </h3>

                        <!-- 6-1: Overview & Labeling Attributes -->
                        <div class="themeable-bg card-hover rounded-lg p-6 mb-6">
                            <h4 class="text-xl font-semibold mb-4 teal-text">6-1. Overview & Labeling Attributes</h4>

                            <div class="themeable-text-secondary space-y-3 mb-4">
                                <p>
                                    For robots to naturally integrate into human living spaces and effectively imitate human tasks, a deep understanding of complex and sophisticated human movements -- particularly 'hand-arm coordination' -- is a prerequisite. When grasping and manipulating objects, humans organically use their fingers, wrists, arms, and even upper body to create optimal movements. This section examines the composition of the 'Hand-Arm Coordinated Grasp-Manipulation Motion Dataset' built for in-depth analysis of such human grasp-manipulation motions. We evaluate the information density and utilization potential of the data by analyzing the multi-layered labeling attributes that encompass hand joints, upper body, force, and object information.
                                </p>
                            </div>

                            <details class="mt-4">
                                <summary class="cursor-pointer font-semibold teal-text hover:text-orange-500 transition-colors">
                                    View QA Samples (4 pairs)
                                </summary>
                                <div class="mt-4 overflow-x-auto">
                                    <table class="w-full border-collapse text-sm">
                                        <thead>
                                            <tr>
                                                <th class="border themeable-border px-3 py-2 text-left w-32">Question Type</th>
                                                <th class="border themeable-border px-3 py-2 text-left">Question</th>
                                                <th class="border themeable-border px-3 py-2 text-left">Answer</th>
                                            </tr>
                                        </thead>
                                        <tbody class="themeable-text-secondary">
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">Simple Extraction</strong></td>
                                                <td class="border themeable-border px-3 py-2">What does the 'gesture.hand_gesture_data.hand_keypoints_2D' attribute in the labeling data represent?</td>
                                                <td class="border themeable-border px-3 py-2">It represents 2D coordinate data for each joint of the human hand.</td>
                                            </tr>
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">Summary &amp; Explanation</strong></td>
                                                <td class="border themeable-border px-3 py-2">Please explain the three values (1, 0, -1) representing 'hand joint data visibility' in the annotation format and what state each indicates.</td>
                                                <td class="border themeable-border px-3 py-2">'1' means the joint is observable by the camera, '0' means it is occluded by another object or hand part, and '-1' means the joint is located outside the camera's field of view.</td>
                                            </tr>
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">Comparison &amp; Analysis</strong></td>
                                                <td class="border themeable-border px-3 py-2">This dataset's labeling attributes include both 'object.object_2D' (object bounding box) and 'object.grasp_2D' (object graspable points). What are the differences between these two pieces of information, and analyze the advantages of providing both for robot learning.</td>
                                                <td class="border themeable-border px-3 py-2">'object.object_2D' is information indicating the overall position and size of the object. In contrast, 'object.grasp_2D' is more detailed information specifying specific regions or points within the object where the robot hand can stably grasp. Unlike knowing only the overall object position, a robot that has learned graspable points can go beyond simply detecting objects to establish optimal strategies for 'how to grasp.' This provides a key advantage of enabling faster, higher-success-rate manipulation by reducing the search space for grasp planning.</td>
                                            </tr>
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">Reasoning &amp; Application</strong></td>
                                                <td class="border themeable-border px-3 py-2">The dataset includes a 'light_source.light_degree' (lighting level) attribute. Infer how this information could be used to make user experiences of interacting with virtual objects in AR (Augmented Reality) environments more realistic.</td>
                                                <td class="border themeable-border px-3 py-2">When placing virtual objects in real spaces in AR environments, they look very unnatural if they don't match the surrounding lighting. An AI model trained with 'light_degree' data can recognize the lighting brightness and color of the actual space where the user is located. Based on this information, the AR system can adjust virtual object shadow direction, brightness, surface reflections, and more in real time to render them in perfect harmony with the actual lighting environment. This maximizes the immersion and realism of AR experiences by making users feel as if virtual objects actually exist.</td>
                                            </tr>
                                        </tbody>
                                    </table>
                                </div>
                            </details>

                            <p class="themeable-text-secondary mt-4 text-sm italic">
                                Finally, we conclude the report by analyzing 'Human Activity Recognition Robot Autonomous Behavior Data,' which addresses task-specific robot-human interaction.
                            </p>
                        </div>
                    </div>

                    <!-- Dataset 7: Human Activity Recognition Robot Autonomous Behavior Dataset -->
                    <div id="dataset-7" class="mb-8">
                        <h3 class="text-2xl font-semibold themeable-heading mb-4 flex items-center">
                            <span class="teal-text mr-2">7.</span>
                            Human Activity Recognition Robot Autonomous Behavior Dataset
                        </h3>

                        <!-- 7-1: Overview & Use Cases -->
                        <div class="themeable-bg card-hover rounded-lg p-6 mb-6">
                            <h4 class="text-xl font-semibold mb-4 teal-text">7-1. Overview & Use Cases</h4>

                            <div class="themeable-text-secondary space-y-3 mb-4">
                                <p>
                                    For service robots to provide truly useful experiences beyond simple functions in public places and commercial facilities, social intelligence that accurately identifies the behavior and intentions of surrounding people and autonomously responds accordingly is essential. Helping users who struggle with automated systems like kiosks is an important use case that enhances the social value of robots. This section examines the overview of 'Human Activity Recognition Robot Autonomous Behavior Data' built to analyze such interactions, and evaluates the social and technical value of the data by analyzing concrete use cases that address problems of informationally vulnerable populations.
                                </p>
                            </div>

                            <details class="mt-4">
                                <summary class="cursor-pointer font-semibold teal-text hover:text-orange-500 transition-colors">
                                    View QA Samples (4 pairs)
                                </summary>
                                <div class="mt-4 overflow-x-auto">
                                    <table class="w-full border-collapse text-sm">
                                        <thead>
                                            <tr>
                                                <th class="border themeable-border px-3 py-2 text-left w-32">Question Type</th>
                                                <th class="border themeable-border px-3 py-2 text-left">Question</th>
                                                <th class="border themeable-border px-3 py-2 text-left">Answer</th>
                                            </tr>
                                        </thead>
                                        <tbody class="themeable-text-secondary">
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">Simple Extraction</strong></td>
                                                <td class="border themeable-border px-3 py-2">How many hours of video data were used to construct the source data of this dataset, and from how many types of real service environments was it collected?</td>
                                                <td class="border themeable-border px-3 py-2">It was constructed from over 1,000 hours of automated service system operation video data, collected from a total of 10 types of service environments including transportation, medical, and educational facilities.</td>
                                            </tr>
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">Summary &amp; Explanation</strong></td>
                                                <td class="border themeable-border px-3 py-2">Please describe two solutions to the 'kiosk problems for wheelchair users and people of short stature' presented as use cases of the dataset.</td>
                                                <td class="border themeable-border px-3 py-2">The first is 'automatic height adjustment,' where sensors recognize the user's condition (e.g., wheelchair use) and automatically adjust the kiosk height. The second is 'enhanced accessibility design,' which optimizes the positions of screens and control buttons so users with various physical conditions can easily use them.</td>
                                            </tr>
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">Comparison &amp; Analysis</strong></td>
                                                <td class="border themeable-border px-3 py-2">Please analyze the 'age distribution' and 'environment distribution' in the data distribution. Evaluate whether the data has secured diversity without bias toward specific groups or environments.</td>
                                                <td class="border themeable-border px-3 py-2">The 'age distribution' is dominated by young and middle-aged adults at 84.57%, but also includes substantial representation of informationally vulnerable groups such as youth (4.7%) and elderly (10.73%), securing diversity. The 'environment distribution' shows higher proportions for educational facilities (21.97%) and convenience facilities (15.41%), but all 10 categories show relatively even distribution between 4.97% and 21.97%, indicating no bias toward specific environments. This means the data composition is suitable for developing robot activity recognition models across diverse user groups and environments.</td>
                                            </tr>
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">Reasoning &amp; Application</strong></td>
                                                <td class="border themeable-border px-3 py-2">The metadata of this dataset includes users' encrypted IDs, age, height, and disability status. Infer how this information could be used to implement commercial robot services that provide personalized advertisements or promotions.</td>
                                                <td class="border themeable-border px-3 py-2">The AI installed in the robot recognizes users standing before the kiosk and matches them with metadata. If the user is in the 'youth' age group, the robot can display or verbally announce advertisements related to toys or children's menus. When an 'elderly' user approaches, it can prioritize providing information about health supplements or senior discount benefits. In this way, by identifying individual characteristics in real time and immediately providing the most relevant information and services, highly personalized commercial services can be implemented that increase user satisfaction and maximize purchase conversion rates.</td>
                                            </tr>
                                        </tbody>
                                    </table>
                                </div>
                            </details>

                            <p class="themeable-text-secondary mt-4 text-sm italic">
                                So far, we have analyzed robot intelligence data from 7 datasets (13 groups) to establish the foundation for QA datasets for LLM fine-tuning. The following section provides a comprehensive summary of statistics and utilization methods.
                            </p>
                        </div>
                    </div>

                </section>

                <!-- Section 4: Statistics -->
                <section id="statistics" class="mb-12">
                    <h2 class="text-3xl font-bold themeable-heading mb-6 pb-3 border-b-2 themeable-border">
                        IV. Final QA Type Statistics
                    </h2>

                    <p class="themeable-text-secondary mb-6">
                        A total of <strong class="orange-text">52</strong> QA pairs were constructed for
                        <strong class="teal-text">13</strong> robot data groups for LLM training data generation.
                        The QA type statistics reflecting the Physical AI characteristics of robotics intelligence are as follows.
                    </p>

                    <div class="overflow-x-auto mb-8">
                        <table class="w-full border-collapse">
                            <thead>
                                <tr class="bg-slate-800/80">
                                    <th class="border themeable-border px-4 py-3 text-left themeable-text-primary">Question Type</th>
                                    <th class="border themeable-border px-4 py-3 text-left themeable-text-primary">Definition</th>
                                    <th class="border themeable-border px-4 py-3 text-center themeable-text-primary">Usage Count</th>
                                    <th class="border themeable-border px-4 py-3 text-center themeable-text-primary">Ratio</th>
                                </tr>
                            </thead>
                            <tbody class="themeable-text-secondary">
                                <tr class="hover:bg-orange-500/5 transition-colors">
                                    <td class="border themeable-border px-4 py-3"><strong class="teal-text">Simple Extraction</strong></td>
                                    <td class="border themeable-border px-4 py-3">Direct extraction of factual information stated in documents</td>
                                    <td class="border themeable-border px-4 py-3 text-center font-bold">13</td>
                                    <td class="border themeable-border px-4 py-3 text-center font-bold">25.0%</td>
                                </tr>
                                <tr class="hover:bg-orange-500/5 transition-colors">
                                    <td class="border themeable-border px-4 py-3"><strong class="teal-text">Summary &amp; Explanation</strong></td>
                                    <td class="border themeable-border px-4 py-3">Comprehensive summary of specific concepts or processes</td>
                                    <td class="border themeable-border px-4 py-3 text-center font-bold">13</td>
                                    <td class="border themeable-border px-4 py-3 text-center font-bold">25.0%</td>
                                </tr>
                                <tr class="hover:bg-orange-500/5 transition-colors">
                                    <td class="border themeable-border px-4 py-3"><strong class="teal-text">Comparison &amp; Analysis</strong></td>
                                    <td class="border themeable-border px-4 py-3">Analyzing differences by comparing two or more concepts</td>
                                    <td class="border themeable-border px-4 py-3 text-center font-bold">13</td>
                                    <td class="border themeable-border px-4 py-3 text-center font-bold">25.0%</td>
                                </tr>
                                <tr class="hover:bg-orange-500/5 transition-colors">
                                    <td class="border themeable-border px-4 py-3"><strong class="teal-text">Reasoning &amp; Application</strong></td>
                                    <td class="border themeable-border px-4 py-3">Inferring outcomes of specific situations based on given information</td>
                                    <td class="border themeable-border px-4 py-3 text-center font-bold">13</td>
                                    <td class="border themeable-border px-4 py-3 text-center font-bold">25.0%</td>
                                </tr>
                                <tr class="bg-orange-100/50">
                                    <td class="border themeable-border px-4 py-3 font-bold themeable-text-primary" colspan="2">Total</td>
                                    <td class="border themeable-border px-4 py-3 text-center font-bold orange-text">52</td>
                                    <td class="border themeable-border px-4 py-3 text-center font-bold orange-text">100.0%</td>
                                </tr>
                            </tbody>
                        </table>
                    </div>

                    <div class="interactive-card border border-teal-500/40 rounded-lg p-6">
                        <h4 class="font-semibold teal-text mb-3">Key Features</h4>
                        <p class="themeable-text-secondary">
                            Questions are <strong class="orange-text">equally distributed</strong> across the core elements of robotics intelligence data:
                            <strong>information extraction, concept explanation, comparative analysis, and applied reasoning</strong>,
                            designed to enable LLMs to learn <strong class="teal-text">comprehensive knowledge</strong> across all domains.
                        </p>
                    </div>
                </section>

                <!-- Section 5: Prompt Template -->
                <section id="prompt-template" class="mb-12">
                    <h2 class="text-3xl font-bold themeable-heading mb-6 pb-3 border-b-2 themeable-border">
                        V. Prompt Template for Domain LLM Report Generation
                    </h2>

                    <p class="themeable-text-secondary mb-6">
                        This prompt can be used to generate <strong class="teal-text">structured QA dataset reports</strong> that enable LLMs
                        to learn domain expertise when given training data documents from other domains
                        (e.g., manufacturing, healthcare, autonomous driving, etc.).
                    </p>

                    <div class="interactive-card border themeable-border rounded-lg p-6">
                        <h3 class="text-lg font-bold themeable-heading mb-4">Report Generation Prompt Template</h3>

                        <div class="bg-slate-900 text-slate-100 rounded-lg p-6 overflow-x-auto mb-4">
                            <pre class="text-sm leading-relaxed"><code>[지시사항]
당신은 Agentic AI Data Scientist (AADS) 과제에서 대규모 언어 모델(LLM) 파인튜닝을 위한
전문 QA 데이터셋을 구축하는 전문가입니다. 아래에 제시된 [INPUT: 분석 대상 문서]의 내용을 분석하여,
**'논리적 데이터 그룹'** 단위로 묶어 QA 보고서를 생성해야 합니다.

**[보고서 구성 요소]**
1. **보고서 제목:** 도메인 및 목적에 맞게 작성하십시오.
2. **논리적 데이터 그룹 식별:** 문서 내에서 동일한 프로젝트나 목표를 공유하는 문서들을
   하나의 '논리적 그룹'으로 묶습니다.
3. **QA 쌍 생성:** 각 논리적 그룹별로 **4개**의 질의응답(QA) 쌍을 생성해야 합니다.
4. **Question Type 분류:** 생성된 QA 쌍은 다음 4가지 핵심 유형 중 하나로 분류되어야 합니다.
   * **단순 정보 추출형:** Direct extraction of factual information stated in documents
   * **요약 및 설명형:** Comprehensive summary of specific concepts or processes
   * **비교 및 분석형:** 둘 이상의 개념을 비교하여 차이점과 공통점 분석
   * **추론 및 적용형:** 주어진 정보를 바탕으로 특정 상황의 결과나 응용 추론
5. **출처 표기:** 응답의 모든 문장은 원본 문서의 출처를 명확하게 표기해야 합니다.
6. **최종 통계:** 생성된 모든 QA 쌍을 대상으로, 사용된 **4가지 유형의 최종 횟수와 Ratio**을
   정리해야 합니다.</code></pre>
                        </div>

                        <p class="text-sm themeable-text-muted">
                            <strong>Usage:</strong> Use this template to automatically generate high-quality QA datasets
                            from training data documents across various domains (healthcare, autonomous driving, manufacturing, etc.).
                        </p>
                    </div>
                </section>

                <!-- Section 6: Pebblous Perspective -->
                <section id="pebblous-perspective" class="mb-12">
                    <h2 class="text-3xl font-bold themeable-heading mb-6 pb-3 border-b-2 themeable-border">
                        Pebblous Perspective: A Data-Centric Approach for the Physical AI Era
                    </h2>

                    <div class="themeable-text-secondary space-y-6">
                        <p>
                            The robotics QA dataset built in this AADS project clearly demonstrates
                            <strong class="orange-text">the intelligence difference that data quality creates</strong>
                            in the era of <strong class="teal-text">Physical AI</strong>.
                        </p>

                        <div class="interactive-card border border-teal-500/40 rounded-lg p-6 mb-6">
                            <h3 class="text-xl font-bold themeable-heading mb-4">AADS's Differentiated Approach</h3>
                            <ul class="space-y-3 themeable-text-secondary">
                                <li class="flex items-start">
                                    <span class="teal-text mr-2">•</span>
                                    <div>
                                        <strong class="themeable-text-primary">Balanced Knowledge Structure:</strong>
                                        Evenly distributing 4 Question Types at 25% each,
                                        designed for LLMs to acquire unbiased expertise
                                    </div>
                                </li>
                                <li class="flex items-start">
                                    <span class="teal-text mr-2">•</span>
                                    <div>
                                        <strong class="themeable-text-primary">Practice-Oriented QA:</strong>
                                        Minimizing hallucination through factual QA extracted from actual documents
                                        of 13 robotics datasets
                                    </div>
                                </li>
                                <li class="flex items-start">
                                    <span class="teal-text mr-2">•</span>
                                    <div>
                                        <strong class="themeable-text-primary">Scalable Template:</strong>
                                        Reusable structure that can be immediately extended to other domains
                                        (healthcare, autonomous driving, etc.) through prompt templates
                                    </div>
                                </li>
                            </ul>
                        </div>

                        <div class="interactive-card border border-orange-500/40 rounded-lg p-6">
                            <h3 class="text-xl font-bold themeable-heading mb-4">The Core of Physical AI: Understanding Sensor Data</h3>
                            <p class="themeable-text-secondary mb-3">
                                Understanding the physical characteristics of various sensors including
                                <strong>RGB-D cameras, LiDAR, tactile sensors, and IMU</strong> is essential for robotics intelligence datasets.
                            </p>
                            <p class="themeable-text-secondary">
                                AADS systematized each sensor's information collection methods, labeling class differences, and model selection criteria
                                through <strong class="teal-text">Comparison & Analysis QA</strong>,
                                designing LLMs to go beyond simply listing data to
                                reason about <strong class="orange-text">"why is this sensor suitable for this task?"</strong>
                            </p>
                        </div>
                    </div>
                </section>

                <!-- Section 7: FAQ -->
                <section id="faq" class="mb-12">
                    <h2 class="text-3xl font-bold themeable-heading mb-6 pb-3 border-b-2 themeable-border">
                        Frequently Asked Questions (FAQ)
                    </h2>

                    <div class="space-y-4">
                        <!-- FAQ items will be rendered from config -->
                        <div class="interactive-card border themeable-border rounded-lg p-6">
                            <h3 class="text-xl font-semibold themeable-heading mb-3">
                                How does AADS build robotics QA datasets?
                            </h3>
                            <p class="themeable-text-secondary">
                                AADS analyzes AI Hub's robot dataset documents and groups
                                documents sharing the same project objectives into 'logical data groups.'
                                For each group, it generates one QA pair from each of the 4 Question Types
                                (information extraction, summary & explanation, comparison & analysis, reasoning & application),
                                producing a total of 4 QA pairs per group, resulting in 52 high-quality question-answers.
                            </p>
                        </div>

                        <div class="interactive-card border themeable-border rounded-lg p-6">
                            <h3 class="text-xl font-semibold themeable-heading mb-3">
                                How do robotics datasets differ from manufacturing datasets?
                            </h3>
                            <p class="themeable-text-secondary">
                                Robotics datasets are characterized by <strong>sensor diversity</strong> (RGB-D, LiDAR, tactile, IMU, etc.) and
                                <strong>real-time decision-making</strong> requirements.
                                In contrast, manufacturing datasets focus on quality inspection and predictive maintenance.
                                AADS builds QA datasets reflecting the characteristics of each domain.
                            </p>
                        </div>

                        <div class="interactive-card border themeable-border rounded-lg p-6">
                            <h3 class="text-xl font-semibold themeable-heading mb-3">
                                What are the most important robotics datasets?
                            </h3>
                            <p class="themeable-text-secondary">
                                The <strong>Occluded Object Reasoning Dataset</strong> and the <strong>Delivery Robot Off-Road Navigation Dataset</strong> are
                                the most critical. The former is essential for validating robot perception capabilities (occlusion handling),
                                while the latter validates navigation capabilities (off-road environments). Both datasets require multimodal sensor fusion.
                            </p>
                        </div>

                        <div class="interactive-card border themeable-border rounded-lg p-6">
                            <h3 class="text-xl font-semibold themeable-heading mb-3">
                                What advantages does multimodal data provide for LLM fine-tuning?
                            </h3>
                            <p class="themeable-text-secondary">
                                Multimodal data (sensor values + images, LiDAR + RGB) helps LLMs
                                <strong>comprehensively understand physical phenomena</strong> of robots.
                                For example, delivery robot data combines 2D images (drivable areas) and 3D LiDAR (dynamic object detection),
                                enabling LLMs to learn multidimensional knowledge such as
                                "speed should be reduced on sidewalks with low flatness."
                            </p>
                        </div>

                        <div class="interactive-card border themeable-border rounded-lg p-6">
                            <h3 class="text-xl font-semibold themeable-heading mb-3">
                                How does the AADS QA dataset integrate with DataClinic?
                            </h3>
                            <p class="themeable-text-secondary">
                                The QA dataset generated by AADS is closely integrated with <strong>DataClinic's data quality diagnosis pipeline</strong>.
                                When DataClinic automatically detects outliers, labeling errors, and imbalanced distributions in sensor data,
                                AADS can learn QA pairs about those quality issues and suggest
                                <strong>practical solutions</strong> such as
                                "a specific filter should be applied to reduce noise in this LiDAR data."
                            </p>
                        </div>
                    </div>
                </section>

                <!-- Section 8: Dataset Sources -->
                <section id="datasets-sources" class="mb-12">
                    <h2 class="text-3xl font-bold themeable-heading mb-6 pb-3 border-b-2 themeable-border">
                        Related Dataset Sources
                    </h2>

                    <p class="themeable-text-secondary mb-6">
                        Below is the source information for the 13 robotics datasets analyzed in this report.
                    </p>

                    <ol class="space-y-3 text-sm themeable-text-secondary">
                        <li class="flex items-start">
                            <span class="font-bold teal-text mr-3 min-w-[2rem]">[1]</span>
                            <div>
                                <strong>Occluded Object Reasoning Data</strong><br>
                                <span class="text-xs themeable-text-muted">
                                    RGB-D Camera / Object 3D Scan, Multi-Object Occlusion, Grasping Data
                                </span>
                            </div>
                        </li>
                        <li class="flex items-start">
                            <span class="font-bold teal-text mr-3 min-w-[2rem]">[2]</span>
                            <div>
                                <strong>Delivery Robot Off-Road Navigation Data</strong><br>
                                <span class="text-xs themeable-text-muted">
                                    2D Image (Semantic Segmentation) + 3D LiDAR (Cuboid) / 9 Dynamic/Static Object Types
                                </span>
                            </div>
                        </li>
                        <li class="flex items-start">
                            <span class="font-bold teal-text mr-3 min-w-[2rem]">[3]</span>
                            <div>
                                <strong>Robot-View Driving Video Data</strong><br>
                                <span class="text-xs themeable-text-muted">
                                    Quadruped Robot (45.92%) + Wheeled Robot (54.08%) / RGB, PCD, CSV, Depth
                                </span>
                            </div>
                        </li>
                        <li class="flex items-start">
                            <span class="font-bold teal-text mr-3 min-w-[2rem]">[4]</span>
                            <div>
                                <strong>Indoor Space Maintenance Service Robot Data</strong><br>
                                <span class="text-xs themeable-text-muted">
                                    JSON-based State Data / errorState, batteryLevel, collision, etc.
                                </span>
                            </div>
                        </li>
                        <li class="flex items-start">
                            <span class="font-bold teal-text mr-3 min-w-[2rem]">[5]</span>
                            <div>
                                <strong>Robot Hand Object Property Identification Data</strong><br>
                                <span class="text-xs themeable-text-muted">
                                    200 Household Items / Hi-RGB, Low-RGB, RGB-D / Weight, Size, Material, Tactile
                                </span>
                            </div>
                        </li>
                        <li class="flex items-start">
                            <span class="font-bold teal-text mr-3 min-w-[2rem]">[6]</span>
                            <div>
                                <strong>Hand-Arm Coordinated Grasp-Manipulation Motion Data</strong><br>
                                <span class="text-xs themeable-text-muted">
                                    Hand Joint 2D/3D Coordinates, Upper Body Pose, Graspable Points, Force Sensor Data
                                </span>
                            </div>
                        </li>
                        <li class="flex items-start">
                            <span class="font-bold teal-text mr-3 min-w-[2rem]">[7]</span>
                            <div>
                                <strong>Human Activity Recognition Robot Autonomous Behavior Data</strong><br>
                                <span class="text-xs themeable-text-muted">
                                    1,000+ Hours Video / 10 Service Environments / Kiosk Accessibility Enhancement
                                </span>
                            </div>
                        </li>
                    </ol>
                </section>

                <!-- Section 9: 결론 -->
                <section id="conclusion" class="mb-12">
                    <h2 class="text-3xl font-bold themeable-heading mb-6 pb-3 border-b-2 themeable-border">
                        결론
                    </h2>

                    <div class="themeable-text-secondary space-y-4 mb-6">
                        <p>
                            This report introduced the <strong class="teal-text">52 QA pairs in the robotics intelligence domain</strong>
                            built through the <strong class="orange-text">AADS project</strong>.
                            The 13 datasets cover core areas of robotics intelligence including <strong>occluded object reasoning, delivery robot off-road navigation, driving video, indoor space maintenance,
                            object property identification, grasp-manipulation actions, and human activity recognition</strong>.
                        </p>
                        <p>
                            Each dataset was systematically analyzed across four dimensions: <strong>Domain Definition, Data Structure, AI Models, and Quality Management</strong>,
                            and composed of four QA types -- <strong>Simple Information Extraction, Summary & Explanation, Comparison & Analysis, and Reasoning & Application</strong> --
                            to strengthen the LLM's <strong class="teal-text">Physical AI domain expertise</strong>.
                        </p>
                        <p>
                            Through this dataset, Pebblous applied <strong class="orange-text">Few-Shot Learning</strong> and
                            <strong class="orange-text">Prompt Engineering</strong>,
                            designing LLMs to support decision-making in practical environments for robot data analysis, model selection, and quality management.
                        </p>
                        <p>
                            Going forward, AADS plans to integrate with the <strong>DataClinic</strong> platform
                            to provide automated quality diagnosis and improvement recommendations for robotics intelligence datasets,
                            and to strengthen collaboration between domain experts and AI models.
                        </p>
                    </div>
                </section>

                <!-- PDF Download Section -->
                <section id="pdf-download" class="mb-12">
                    <h2 class="text-3xl font-bold themeable-heading mb-6 pb-3 border-b-2 themeable-border">
                        Original Report Download
                    </h2>

                    <div class="themeable-bg card-hover rounded-lg p-8 text-center">
                        <div class="mb-4">
                            <svg class="w-16 h-16 mx-auto text-orange-500 mb-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                                <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z"></path>
                            </svg>
                            <h3 class="text-xl font-semibold themeable-heading mb-2">
                                AADS LLM Fine-Tuning QA Dataset Construction: Robotics
                            </h3>
                            <p class="themeable-text-secondary mb-4">
                                Download the original report containing detailed analysis of 13 datasets and 52 QA pairs in the robotics intelligence domain.
                            </p>
                            <p class="text-sm themeable-text-muted mb-6">
                                <strong class="teal-text">In addition to all QA on this web page</strong>, additional analysis materials and original text are included.
                            </p>
                        </div>

                        <a href="/project/AADS/source/AADS LLM 파인튜닝용 QA 데이터셋 구축_ 로봇 분야.pdf"
                           download="AADS_로봇분야_QA데이터셋_구축보고서.pdf"
                           class="inline-flex items-center gap-2 bg-orange-500 hover:bg-orange-600 text-white font-semibold px-6 py-3 rounded-lg transition-all transform hover:scale-105 shadow-lg hover:shadow-xl">
                            <svg class="w-5 h-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                                <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M12 10v6m0 0l-3-3m3 3l3-3m2 8H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"></path>
                            </svg>
                            <span>Download PDF</span>
                        </a>

                        <div class="mt-6 text-sm themeable-text-muted">
                            <p>File format: PDF | Written: November 30, 2025 | Pebblous Data Communication Team</p>
                        </div>
                    </div>
                </section>

            </main>
        </div>
    </div>

    <!-- Footer will be loaded by common-utils.js -->
    <div id="footer-placeholder"></div>

    <!-- Scripts -->
    <script src="/scripts/common-utils.js"></script>
    <script>
    document.addEventListener('DOMContentLoaded', async function() {
        const config = {
            mainTitle: "Building QA Datasets for LLM Fine-Tuning in Robotics: (1) Domain Knowledge",
            subtitle: "AADS Physical AI Approach",
            pageTitle: "Building QA Datasets for LLM Fine-Tuning in Robotics: (1) Domain Knowledge - AADS Physical AI Approach | Pebblous",
            publishDate: "November 30, 2025",
            publisher: "Pebblous Data Communication Team",
            defaultTheme: "beige",
            category: "tech",
            articlePath: "project/AADS/robot-qa-dataset/en/index.html",
            tags: [
                "LLM 파인튜닝", "LLM Fine-tuning", "QA 데이터셋", "Question-Answer Dataset",
                "로봇 분야", "Robotics AI", "로봇 데이터", "Robot Data", "AADS", "Agentic AI Data Scientist",
                "피지컬 AI", "Physical AI", "데이터 품질", "Data Quality",
                "데이터 중심 AI", "Data-Centric AI", "멀티모달 데이터", "Multimodal Data",
                "도메인 지식", "Domain Knowledge", "가려진 객체 추론", "Occluded Object Detection",
                "배송로봇", "Delivery Robot", "비도로 운행", "Off-Road Navigation",
                "주행영상", "Driving Video", "실내공간 유지관리", "Indoor Maintenance",
                "서비스 로봇", "Service Robot", "객체 특성 식별", "Object Property Recognition",
                "로봇 핸드", "Robot Hand", "파지-조작 동작", "Grasp-Manipulation",
                "손·팔 협조", "Hand-Arm Coordination", "사람 행동 인식", "Human Activity Recognition",
                "로봇 자율 행동", "Robot Autonomous Behavior", "Few-Shot Learning", "퓨샷 러닝",
                "프롬프트 엔지니어링", "Prompt Engineering", "라벨링", "Labeling",
                "데이터 검수", "Data Validation", "mAP", "F1-score", "mIoU",
                "페블러스", "Pebblous", "DataClinic", "데이터클리닉"
            ],
            faqs: [
                {
                    question: "How does AADS build robotics QA datasets?",
                    answer: "AADS analyzes AI Hub's robotics intelligence dataset documents, groups them into 13 'logical data groups,' and generates QA pairs from four types -- Domain Definition, Data Structure, AI Models, and Quality Management -- for each group, building a total of 52 high-quality question-answer pairs."
                },
                {
                    question: "Why is multimodal data important in robotics datasets?",
                    answer: "Robots integrate various sensors including RGB cameras, Depth sensors, LiDAR, IMU, and GPS to understand their environment. Multimodal data fuses the advantages of each sensor for robust recognition performance and safe decision-making. For example, RGB provides color, Depth provides distance, and LiDAR provides 3D shape."
                },
                {
                    question: "What are practical use cases of the occluded object reasoning dataset?",
                    answer: "In logistics warehouses, robots infer the position and shape of products hidden behind boxes to determine efficient picking sequences, or autonomous driving robots predict the complete shape of partially visible obstacles to plan safe routes."
                },
                {
                    question: "How does delivery robot off-road navigation data contribute to last-mile delivery?",
                    answer: "By learning from 50,000 images collected in various off-road environments such as sidewalks, parks, and parking lots, along with weather condition data (clear, rain, snow), delivery robots can operate safely without traffic congestion, reducing delivery times and costs."
                },
                {
                    question: "What is the core value of robot hand object property identification data?",
                    answer: "100,000 grasping data points measuring object texture, weight, temperature, and rigidity with tactile sensors enable robots to apply appropriate grasping force to prevent damage and demonstrate precise manipulation capabilities in food packaging, electronics assembly, medical surgery, and more."
                },
                {
                    question: "How is the human activity recognition dataset used in service robots?",
                    answer: "Through 15,000 activity videos (20 classes including walking, sitting, waving, etc.) and Skeleton Keypoint data, guide robots recognize human gestures to provide directions, detect falls of elderly or infirm persons to provide assistance, and implement other situational awareness services."
                },
                {
                    question: "How does the AADS QA dataset integrate with DataClinic?",
                    answer: "AADS's robotics QA dataset integrates with the DataClinic platform to provide automated quality diagnosis of robot data (label consistency, sensor calibration, scenario balance) and improvement recommendations, accelerating practical Physical AI adoption through collaboration between domain experts and AI models."
                }
            ]
        };

        await PebblousPage.init(config);
    });
    </script>
</body>
</html>
