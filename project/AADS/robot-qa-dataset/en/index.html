<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="author" content="Pebblous Data Communication Team">
    <meta name="language" content="English">
    <meta name="copyright" content="© 2025 Pebblous. All rights reserved.">
    <meta name="rating" content="general">
    <meta name="revisit-after" content="7 days">
    <meta name="distribution" content="global">
    <meta name="audience" content="AI Researchers, Data Scientists, Robotics Engineers, LLM Developers, Physical AI Researchers">
    <meta name="topic" content="LLM Fine-tuning, QA Dataset, Robotics AI, Physical AI, Data Quality">
    <meta http-equiv="content-language" content="en">

    <!-- Google Tag Manager -->
    <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
    new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
    j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
    'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','GTM-57L9F58B');</script>
    <!-- End Google Tag Manager -->

    <!-- Favicon -->
    <link rel="icon" href="/image/favicon.ico" sizes="any">
    <link rel="icon" href="/image/Pebblous_BM_Orange_RGB.png" type="image/png">
    <link rel="apple-touch-icon" href="/image/Pebblous_BM_Orange_RGB.png">

    <!-- SEO Meta Tags -->
    <title id="page-title">Building QA Datasets for LLM Fine-Tuning in Robotics: AADS Physical AI Approach | Pebblous</title>
    <meta id="meta-description" name="description" content="Pebblous AADS built 52 QA pairs for LLM fine-tuning across 13 robotics intelligence domains including occluded object reasoning, delivery robots, driving video, indoor maintenance, and object property recognition. A data-centric Physical AI approach spanning robot data collection, AI model training, and quality management.">
    <meta id="meta-keywords" name="keywords" content="LLM Fine-tuning, QA Dataset, Question-Answer Dataset, Robotics AI, Robot Data, AADS, Agentic AI Data Scientist, Physical AI, Data Quality, Data-Centric AI, Multimodal Data, Domain Knowledge, Occluded Object Detection, Delivery Robot, Off-Road Navigation, Driving Video, Indoor Maintenance, Service Robot, Object Property Recognition, Robot Hand, Grasp-Manipulation, Hand-Arm Coordination, Human Activity Recognition, Robot Autonomous Behavior, Few-Shot Learning, Prompt Engineering, Labeling, Data Validation, mAP, F1-score, mIoU, Pebblous, DataClinic">
    <meta name="robots" content="index, follow">

    <link id="hreflang-ko" rel="alternate" hreflang="ko" href="https://blog.pebblous.ai/project/AADS/robot-qa-dataset/ko/">
    <link id="hreflang-en" rel="alternate" hreflang="en" href="https://blog.pebblous.ai/project/AADS/robot-qa-dataset/en/">
    <link id="hreflang-default" rel="alternate" hreflang="x-default" href="https://blog.pebblous.ai/project/AADS/robot-qa-dataset/ko/">

    <link id="canonical-url" rel="canonical" href="https://blog.pebblous.ai/project/AADS/robot-qa-dataset/en/">

    <meta id="og-url" property="og:url" content="https://blog.pebblous.ai/project/AADS/robot-qa-dataset/en/">
    <meta id="og-title" property="og:title" content="Building QA Datasets for LLM Fine-Tuning in Robotics: AADS Physical AI Approach | Pebblous">
    <meta id="og-description" property="og:description" content="A practical case study of AADS building 52 high-quality QA pairs across 13 robotics intelligence domains. A data-centric Physical AI strategy balancing domain definition, data structure, AI models, and quality management.">
    <meta id="og-image" property="og:image" content="https://blog.pebblous.ai/image/Pebblous_BM_Orange_RGB.png">
    <meta property="og:image:width" content="1200">
    <meta property="og:image:height" content="630">
    <meta property="og:image:alt" content="AADS Robotics LLM Fine-Tuning QA Dataset - Pebblous">
    <meta property="og:type" content="article">
    <meta property="og:site_name" content="Pebblous Blog">
    <meta property="og:locale" content="en_US">
    <meta property="article:published_time" content="2025-11-30T09:00:00+09:00">
    <meta property="article:modified_time" content="2025-11-30T09:00:00+09:00">
    <meta property="article:author" content="Pebblous Data Communication Team">
    <meta property="article:section" content="Technology">
    <meta property="article:tag" content="LLM Fine-tuning">
    <meta property="article:tag" content="Robotics AI">
    <meta property="article:tag" content="Physical AI">
    <meta property="article:tag" content="AADS">

    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:site" content="@Pebblous">
    <meta name="twitter:creator" content="@pebblous">
    <meta name="twitter:title" content="Building QA Datasets for LLM Fine-Tuning in Robotics: AADS Physical AI Approach">
    <meta name="twitter:description" content="A practical case study of AADS building 52 high-quality QA pairs across 13 robotics intelligence domains. A data-centric Physical AI strategy balancing domain definition, data structure, AI models, and quality management.">
    <meta name="twitter:image" content="https://blog.pebblous.ai/image/Pebblous_BM_Orange_RGB.png">
    <meta name="twitter:image:alt" content="AADS Robotics LLM Fine-Tuning QA Dataset">
    <meta name="twitter:label1" content="Reading time">
    <meta name="twitter:data1" content="15 min">
    <meta name="twitter:label2" content="Level">
    <meta name="twitter:data2" content="Intermediate">

    <!-- JSON-LD Structured Data for SEO -->
    <script type="application/ld+json">
    {
        "@context": "https://schema.org",
        "@type": "TechArticle",
        "headline": "Building QA Datasets for LLM Fine-Tuning in Robotics: AADS Physical AI Approach",
        "alternativeHeadline": "A data-centric Physical AI strategy building 52 high-quality QA pairs across 13 robotics intelligence domains",
        "description": "Pebblous AADS built 52 QA pairs for LLM fine-tuning across 13 robotics intelligence domains including occluded object reasoning, delivery robots, driving video, indoor maintenance, and object property recognition. A data-centric Physical AI approach spanning robot data collection, AI model training, and quality management.",
        "image": {
            "@type": "ImageObject",
            "url": "https://blog.pebblous.ai/image/Pebblous_BM_Orange_RGB.png",
            "width": 1200,
            "height": 630
        },
        "author": {
            "@type": "Organization",
            "name": "Pebblous",
            "url": "https://www.pebblous.ai",
            "logo": {
                "@type": "ImageObject",
                "url": "https://www.pebblous.ai/image/Pebblous_BM_Orange_RGB.png"
            },
            "description": "Pebblous is a deep-tech company providing AI-Ready Data solutions."
        },
        "publisher": {
            "@type": "Organization",
            "name": "Pebblous",
            "url": "https://www.pebblous.ai",
            "logo": {
                "@type": "ImageObject",
                "url": "https://www.pebblous.ai/image/Pebblous_BM_Orange_RGB.png",
                "width": 600,
                "height": 60
            },
            "sameAs": [
                "https://www.linkedin.com/company/pebblous",
                "https://github.com/pebblous"
            ]
        },
        "datePublished": "2025-11-30T09:00:00+09:00",
        "dateModified": "2025-11-30T09:00:00+09:00",
        "mainEntityOfPage": {
            "@type": "WebPage",
            "@id": "https://blog.pebblous.ai/project/AADS/robot-qa-dataset/en/"
        },
        "keywords": "LLM Fine-tuning, QA Dataset, Robotics AI, Robot Data, AADS, Physical AI, Data Quality, Data-Centric AI, Multimodal Data, Domain Knowledge",
        "articleSection": "Technology",
        "inLanguage": "en-US",
        "isAccessibleForFree": true,
        "proficiencyLevel": "Intermediate"
    }
    </script>

    <!-- FAQ Schema is dynamically generated by common-utils.js from config.faqs -->

    <!-- Stylesheets -->
    <link rel="stylesheet" href="/styles/common-styles.css?v=20260107">
    <link rel="stylesheet" href="/styles/tailwind-build.css">

    <!-- Fonts -->
    <link rel="stylesheet" as="style" crossorigin
          href="https://cdn.jsdelivr.net/gh/orioncactus/pretendard@v1.3.9/dist/web/variable/pretendardvariable.min.css">

    <style>
        /* Theme Variables - Light as default */
        :root {
            --bg-primary: #F9FAFB;
            --bg-secondary: #F3F4F6;
            --bg-card: rgba(255, 255, 255, 0.95);
            --text-primary: #111827;
            --text-secondary: #4B5563;
            --text-muted: #6B7280;
            --heading-color: #111827;
            --border-color: #E5E7EB;
            --accent-color: #F86825;
            --teal-color: #0d9488;
        }

        [data-theme="dark"] {
            --bg-primary: #020617;
            --bg-secondary: #0f172a;
            --bg-card: rgba(30, 41, 59, 0.95);
            --text-primary: #F1F5F9;
            --text-secondary: #CBD5E1;
            --text-muted: #94A3B8;
            --heading-color: #F1F5F9;
            --border-color: #334155;
            --accent-color: #F86825;
            --teal-color: #14b8a6;
        }

        body {
            background-color: var(--bg-primary);
            color: var(--text-primary);
            font-family: 'Pretendard Variable', -apple-system, BlinkMacSystemFont, system-ui, Roboto, sans-serif;
            transition: background-color 0.3s ease, color 0.3s ease;
            line-height: 1.7;
        }

        .themeable-bg {
            background-color: var(--bg-card);
            transition: background-color 0.3s ease;
        }

        .themeable-text {
            color: var(--text-primary);
        }

        .themeable-text-secondary {
            color: var(--text-secondary);
        }

        .themeable-text-muted {
            color: var(--text-muted);
        }

        .themeable-heading {
            color: var(--heading-color);
        }

        .themeable-border {
            border-color: var(--border-color);
        }

        .orange-text {
            color: var(--accent-color);
        }

        .themeable-toc-border {
            border-color: var(--teal-color);
        }

        .teal-text {
            color: var(--teal-color);
        }

        .card-hover {
            transition: all 0.3s ease;
            border: 1px solid var(--border-color);
        }

        .card-hover:hover {
            border-color: var(--teal-color);
            box-shadow: 0 4px 12px rgba(20, 184, 166, 0.15);
        }

        /* Share buttons styling */
        .share-container {
            display: flex;
            align-items: center;
            justify-content: center;
            gap: 0.75rem;
            flex-wrap: wrap;
        }

        .share-label {
            font-size: 0.875rem;
            color: var(--text-secondary);
            font-weight: 500;
        }

        .share-btn {
            display: inline-flex;
            align-items: center;
            gap: 0.375rem;
            padding: 0.5rem 0.875rem;
            border-radius: 0.5rem;
            font-size: 0.875rem;
            font-weight: 500;
            transition: all 0.2s ease;
            border: 1px solid var(--border-color);
            background-color: var(--bg-card);
            color: var(--text-secondary);
            cursor: pointer;
        }

        .share-btn:hover {
            background-color: var(--accent-color);
            color: white;
            border-color: var(--accent-color);
            transform: translateY(-2px);
            box-shadow: 0 4px 8px rgba(248, 104, 37, 0.2);
        }

        .share-btn svg {
            width: 1rem;
            height: 1rem;
        }

        .share-btn.copied {
            background-color: var(--teal-color);
            color: white;
            border-color: var(--teal-color);
        }

        /* Table styling */
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 1.5rem 0;
            font-size: 0.9rem;
        }

        thead {
            background-color: rgba(248, 104, 37, 0.1);
        }

        th {
            padding: 1rem;
            text-align: left;
            font-weight: 600;
            color: var(--heading-color);
            border-bottom: 2px solid var(--border-color);
        }

        td {
            padding: 1rem;
            border-bottom: 1px solid var(--border-color);
            color: var(--text-secondary);
        }

        tbody tr:hover {
            background-color: rgba(248, 104, 37, 0.05);
        }

        /* Details/Summary styling */
        details {
            margin: 1rem 0;
            padding: 1rem;
            border: 1px solid var(--border-color);
            border-radius: 0.5rem;
            background-color: var(--bg-card);
        }

        details summary {
            cursor: pointer;
            font-weight: 600;
            color: var(--teal-color);
            padding: 0.5rem;
            user-select: none;
        }

        details summary:hover {
            color: var(--accent-color);
        }

        details[open] summary {
            margin-bottom: 1rem;
            padding-bottom: 0.5rem;
            border-bottom: 1px solid var(--border-color);
        }

        /* Code styling */
        code {
            background-color: rgba(248, 104, 37, 0.1);
            padding: 0.2rem 0.4rem;
            border-radius: 0.25rem;
            font-family: 'Courier New', monospace;
            font-size: 0.9em;
            color: var(--accent-color);
        }

        /* List styling */
        ul, ol {
            margin-left: 1.5rem;
            margin-bottom: 1.5rem;
        }

        li {
            margin-bottom: 0.75rem;
            line-height: 1.7;
        }

        /* Strong text */
        strong {
            font-weight: 600;
            color: var(--heading-color);
        }
    </style>
</head>
<body>
    <!-- Google Tag Manager (noscript) -->
    <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-57L9F58B"
    height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
    <!-- End Google Tag Manager (noscript) -->

    <!-- Header will be loaded by common-utils.js -->
    <div id="header-placeholder"></div>

    <!-- Main Content -->
    <div class="container mx-auto px-4 sm:px-6 lg:px-8 py-8 md:py-12 max-w-[1400px]">
        <div class="lg:flex lg:gap-8 lg:justify-center lg:items-start">

            <!-- TOC Sidebar -->
            <nav class="hidden lg:block lg:w-[240px] lg:shrink-0 sticky top-20 self-start">
                <h3 class="font-bold themeable-heading mb-4 text-lg">Contents</h3>
                <ul id="toc-links" class="space-y-3 text-sm border-l-2 themeable-toc-border pl-4">
                    <li><a href="#intro" class="toc-link themeable-text-secondary hover:text-teal-500 transition-colors">Introduction & Objectives</a></li>
                    <li><a href="#overview" class="toc-link themeable-text-secondary hover:text-teal-500 transition-colors">QA Dataset Overview</a></li>
                    <li><a href="#robot-datasets" class="toc-link themeable-text-secondary hover:text-teal-500 transition-colors">Robot Intelligence Datasets</a>
                        <ul class="ml-4 mt-2 space-y-2">
                            <li><a href="#dataset-1" class="toc-link themeable-text-secondary hover:text-teal-500 transition-colors text-xs">1. Occluded Object Reasoning</a></li>
                            <li><a href="#dataset-2" class="toc-link themeable-text-secondary hover:text-teal-500 transition-colors text-xs">2. Delivery Robot Off-Road</a></li>
                            <li><a href="#dataset-3" class="toc-link themeable-text-secondary hover:text-teal-500 transition-colors text-xs">3. Robot-View Driving Video</a></li>
                            <li><a href="#dataset-4" class="toc-link themeable-text-secondary hover:text-teal-500 transition-colors text-xs">4. Indoor Maintenance</a></li>
                            <li><a href="#dataset-5" class="toc-link themeable-text-secondary hover:text-teal-500 transition-colors text-xs">5. Robot Hand Object Properties</a></li>
                            <li><a href="#dataset-6" class="toc-link themeable-text-secondary hover:text-teal-500 transition-colors text-xs">6. Hand-Arm Grasp-Manipulation</a></li>
                            <li><a href="#dataset-7" class="toc-link themeable-text-secondary hover:text-teal-500 transition-colors text-xs">7. Human Activity Recognition</a></li>
                        </ul>
                    </li>
                    <li><a href="#statistics" class="toc-link themeable-text-secondary hover:text-teal-500 transition-colors">QA Type Statistics</a></li>
                    <li><a href="#prompt-template" class="toc-link themeable-text-secondary hover:text-teal-500 transition-colors">Prompt Template</a></li>
                    <li><a href="#pebblous-perspective" class="toc-link themeable-text-secondary hover:text-teal-500 transition-colors">Pebblous Perspective</a></li>
                    <li><a href="#faq" class="toc-link themeable-text-secondary hover:text-teal-500 transition-colors">FAQ</a></li>
                    <li><a href="#datasets-sources" class="toc-link themeable-text-secondary hover:text-teal-500 transition-colors">Dataset Sources</a></li>
                    <li><a href="#conclusion" class="toc-link themeable-text-secondary hover:text-teal-500 transition-colors">Conclusion</a></li>
                    <li><a href="#pdf-download" class="toc-link themeable-text-secondary hover:text-teal-500 transition-colors">PDF Download</a></li>
                </ul>
            </nav>

            <!-- Main Article -->
            <main class="max-w-[800px] px-4 sm:px-6">

                <header class="text-left mb-16">
                    <h1 id="page-h1-title" class="text-4xl md:text-5xl font-bold themeable-heading mb-6 leading-tight" style="line-height: 1.4;"></h1>

                    <p class="text-sm themeable-muted">2025.11 · Pebblous Data Communication Team</p>
                    <p class="text-sm themeable-muted mt-1">Reading time: ~15 min · <a href="../ko/" class="text-orange-400 hover:text-orange-300 transition-colors">한국어</a></p>
                </header>

                <!-- Section 1: Introduction & Objectives -->
                <section id="intro" class="mb-12">
                    <h2 class="text-3xl font-bold themeable-heading mb-6 pb-3 border-b-2 themeable-border">
                        I. Introduction & Objectives
                    </h2>

                    <div class="themeable-text-secondary space-y-4 mb-6">
                        <p>
                            This report introduces <strong>52 QA (Question-Answer) pairs</strong> built from actual AI Hub dataset documentation
                            under the <strong class="teal-text">Agentic AI Data Scientist (AADS)</strong> project,
                            with the goal of <strong class="orange-text">enhancing LLM domain expertise in robotics intelligence</strong>.
                        </p>
                        <p>
                            Pebblous reorganized <strong class="orange-text">13 robotics datasets</strong> into "logical data groups" and
                            systematically generated QA pairs in <strong>4 types (Simple Information Extraction, Summary & Explanation, Comparison & Analysis, Reasoning & Application)</strong>
                            for each group. This design enables LLMs to understand
                            <strong class="teal-text">the entire lifecycle of Physical AI</strong>, from robot data collection to AI model training and quality management.
                        </p>
                        <p>
                            The dataset covers core areas of robotics intelligence including <strong>occluded object reasoning, delivery robot off-road navigation, driving video, indoor space maintenance, object property identification,
                            grasp-manipulation actions, and human activity recognition</strong>,
                            and is structured for immediate practical use through Few-Shot Learning and prompt engineering.
                        </p>
                    </div>

                    <!-- PDF guide -->
                    <div class="interactive-card border border-orange-500/40 rounded-lg p-6 mb-6">
                        <h4 class="font-semibold orange-text mb-3">Original Report Guide</h4>
                        <p class="themeable-text-secondary">
                            This web page contains all <strong class="teal-text">52 core QA pairs across 13 robot datasets</strong>.
                            For more detailed analysis and original text, please <a href="#pdf-download" class="orange-text hover:underline">download the PDF report</a> at the bottom.
                        </p>
                    </div>
                </section>

                <!-- Section 1.5: QA Dataset Overview -->
                <section id="overview" class="mb-12">
                    <h2 class="text-3xl font-bold themeable-heading mb-6 pb-3 border-b-2 themeable-border">
                        II. QA Dataset Overview
                    </h2>

                    <div class="grid grid-cols-1 md:grid-cols-3 gap-6 mb-8">
                        <div class="stat-card border themeable-border rounded-lg p-6 text-center">
                            <h4 class="text-4xl font-bold orange-text mb-2">13</h4>
                            <p class="text-sm themeable-text-muted">Robot Data Groups</p>
                        </div>
                        <div class="stat-card border themeable-border rounded-lg p-6 text-center">
                            <h4 class="text-4xl font-bold orange-text mb-2">52</h4>
                            <p class="text-sm themeable-text-muted">QA Pairs (4 per group)</p>
                        </div>
                        <div class="stat-card border themeable-border rounded-lg p-6 text-center">
                            <h4 class="text-4xl font-bold orange-text mb-2">25%</h4>
                            <p class="text-sm themeable-text-muted">Equally Distributed per Type</p>
                        </div>
                    </div>

                    <div class="themeable-text-secondary space-y-4">
                        <p>
                            A total of <strong class="orange-text">52 QA pairs</strong> were constructed for
                            <strong class="teal-text">13 robot data groups</strong>.
                            Each group covers core Physical AI functions including robot perception, navigation, manipulation, and maintenance.
                        </p>
                        <p>
                            The QA pairs are <strong class="teal-text">equally distributed at 25% each</strong> across
                            the four types: <strong>Simple Information Extraction, Summary & Explanation, Comparison & Analysis, and Reasoning & Application</strong>,
                            designed to enable LLMs to learn comprehensive knowledge across all areas of robot data science.
                        </p>
                    </div>
                </section>

                <!-- Section 3: QA Construction Based on Robot Intelligence Datasets -->
                <section id="robot-datasets" class="mb-12">
                    <h2 class="text-3xl font-bold themeable-heading mb-6 pb-3 border-b-2 themeable-border">
                        III. QA Construction Based on Robot Intelligence Datasets
                    </h2>

                    <div class="themeable-text-secondary space-y-4 mb-8">
                        <p>
                            The robotics intelligence field demands high data quality due to
                            <strong class="orange-text">sensor data diversity</strong> and
                            <strong class="teal-text">real-time decision-making requirements</strong>.
                            AADS analyzed 13 robotics datasets from AI Hub and generated 4 types of QA pairs tailored to each dataset's characteristics.
                        </p>
                        <p>
                            Each dataset was systematically documented across four dimensions:
                            <strong>Domain Definition</strong> (data collection purpose and structure),
                            <strong>Data Structure</strong> (labeling methods and environments),
                            <strong>AI Models</strong> (learning algorithms and applications), and
                            <strong>Quality Management</strong> (validation criteria and performance metrics).
                        </p>
                    </div>

                    <!-- Dataset 1: Occluded Object Reasoning Dataset -->
                    <div id="dataset-1" class="mb-8">
                        <h3 class="text-2xl font-semibold themeable-heading mb-4 flex items-center">
                            <span class="teal-text mr-2">1.</span>
                            Occluded Object Reasoning Dataset
                        </h3>

                        <!-- 1-1: Overview & Structure -->
                        <div class="themeable-bg card-hover rounded-lg p-6 mb-6">
                            <h4 class="text-xl font-semibold mb-4 teal-text">1-1. Overview & Structure</h4>

                            <div class="themeable-text-secondary space-y-3 mb-4">
                                <p>
                                    For robots to perform missions in real industrial and living spaces beyond controlled laboratory environments, the ability to cope with unpredictable complexity is essential. In particular, occlusion -- where a target object is partially or fully hidden by other objects -- is one of the greatest challenges in robot vision technology. The 'Occluded Object Reasoning Dataset' is key data for solving this problem, with strategic importance in training robots to accurately infer the complete shape and position of objects from incomplete visual information alone. This dataset serves as the cornerstone of foundational technology that directly contributes to advancing sophisticated object recognition, grasping, and manipulation capabilities, from picking robots in logistics facilities to assembly robots in smart factories.
                                </p>
                            </div>

                            <details class="mt-4">
                                <summary class="cursor-pointer font-semibold teal-text hover:text-orange-500 transition-colors">
                                    View QA Samples (4 pairs)
                                </summary>
                                <div class="mt-4 overflow-x-auto">
                                    <table class="w-full border-collapse text-sm">
                                        <thead>
                                            <tr>
                                                <th class="border themeable-border px-3 py-2 text-left w-32">Question Type</th>
                                                <th class="border themeable-border px-3 py-2 text-left">Question</th>
                                                <th class="border themeable-border px-3 py-2 text-left">Answer</th>
                                            </tr>
                                        </thead>
                                        <tbody class="themeable-text-secondary">
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">Simple Extraction</strong></td>
                                                <td class="border themeable-border px-3 py-2">Which lead organization and participating organizations were responsible for data collection in the 'Occluded Object Reasoning Data' construction project?</td>
                                                <td class="border themeable-border px-3 py-2">The lead organization is 'Saram-gwa-Sup,' and the participating organizations are 'Gwangju Institute of Science and Technology (GIST)' and 'Hanaleum Information.'</td>
                                            </tr>
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">Summary &amp; Explanation</strong></td>
                                                <td class="border themeable-border px-3 py-2">Please describe the four major data types that constitute the folders of this dataset.</td>
                                                <td class="border themeable-border px-3 py-2">The dataset consists of four major folders: 'Object 3D Scan Data,' 'Multi-Object Occlusion Data,' 'Robot-Object Grasping Data,' and 'Human-Object Grasping Data.'</td>
                                            </tr>
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">Comparison &amp; Analysis</strong></td>
                                                <td class="border themeable-border px-3 py-2">Compare the objects in the 'Industrial' category with those in the 'Logistics' category included in the dataset, and analyze the characteristics of each category.</td>
                                                <td class="border themeable-border px-3 py-2">The 'Industrial' category consists of individual functional objects such as 'parts,' 'tools,' and '3D printed parts' (e.g., brackets, spanners, gearboxes). In contrast, the 'Logistics' category centers on commercialized objects designed for packaging and transport, such as 'Amazon logistics,' 'cardboard-type packages,' and 'box-type packages' (e.g., crayon boxes, CPU boxes). This reflects the distinct characteristics of different robot application fields: assembly/maintenance in industrial settings vs. picking/sorting in logistics settings.</td>
                                            </tr>
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">Reasoning &amp; Application</strong></td>
                                                <td class="border themeable-border px-3 py-2">To achieve the stated impact of 'revitalizing related industrial fields such as logistics through robot arm object grasping, manipulation, transport, and placement,' please infer what specific problems this data can contribute to solving.</td>
                                                <td class="border themeable-border px-3 py-2">In logistics settings, products of various sizes and shapes are often randomly stacked in boxes or overlapping on shelves. This dataset trains robots to accurately infer the complete shape and position (6D Pose) of occluded products, enabling them to plan grasping sequences by determining which products must be removed first to reach the target. This directly contributes to revitalizing the logistics industry by reducing picking task failure rates and maximizing automation efficiency.</td>
                                            </tr>
                                        </tbody>
                                    </table>
                                </div>
                            </details>

                            <p class="themeable-text-secondary mt-4 text-sm italic">
                                The next section examines the data collection environment and tools that ensure the quality of this critical data in greater detail.
                            </p>
                        </div>

                        <!-- 1-2: 수집 환경 및 도구 -->
                        <div class="themeable-bg card-hover rounded-lg p-6 mb-6">
                            <h4 class="text-xl font-semibold mb-4 teal-text">1-2. Collection Environment & Tools</h4>

                            <div class="themeable-text-secondary space-y-3 mb-4">
                                <p>
                                    The value of AI training datasets is determined not only by the volume of data but also by its quality and reproducibility. In particular, to ensure generalization performance of robot vision models, the specific specifications of the collection environment, location, and tools used are critically important. This prevents AI models from overfitting to specific environments and enables robust performance across diverse real-world situations. Therefore, information about the standardized collection environment and precision tools used in building the 'Occluded Object Reasoning Dataset' is a key factor in enhancing dataset reliability.
                                </p>
                            </div>

                            <details class="mt-4">
                                <summary class="cursor-pointer font-semibold teal-text hover:text-orange-500 transition-colors">
                                    View QA Samples (4 pairs)
                                </summary>
                                <div class="mt-4 overflow-x-auto">
                                    <table class="w-full border-collapse text-sm">
                                        <thead>
                                            <tr>
                                                <th class="border themeable-border px-3 py-2 text-left w-32">Question Type</th>
                                                <th class="border themeable-border px-3 py-2 text-left">Question</th>
                                                <th class="border themeable-border px-3 py-2 text-left">Answer</th>
                                            </tr>
                                        </thead>
                                        <tbody class="themeable-text-secondary">
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">Simple Extraction</strong></td>
                                                <td class="border themeable-border px-3 py-2">What is the model name of the 3D scanner used for data collection?</td>
                                                <td class="border themeable-border px-3 py-2">The Artec 3D Leo tool was used to generate RGB-D object 3D scan raw data.</td>
                                            </tr>
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">Summary &amp; Explanation</strong></td>
                                                <td class="border themeable-border px-3 py-2">Please summarize three key features of the 'Multi-Object Occlusion Data' collection environment.</td>
                                                <td class="border themeable-border px-3 py-2">The key features are as follows. First, three representative robot environments were configured: desk, shelf, and box. Second, data diversity was ensured by equally utilizing 5 furniture pieces and 5 backgrounds per environment. Third, IKEA furniture was used so that researchers worldwide could replicate the same environment.</td>
                                            </tr>
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">Comparison &amp; Analysis</strong></td>
                                                <td class="border themeable-border px-3 py-2">Analyze the diversity of grippers used in robot-object grasping data collection, and explain the positive impact of this selection on the dataset's usefulness.</td>
                                                <td class="border themeable-border px-3 py-2">The dataset achieved morphological diversity by including 1-finger (1 type), 2-finger (3 types), 3-finger (2 types), 4-finger (1 type), and 5-finger (1 type) grippers. In particular, 2-finger and 3-finger grippers, which are most commonly used in research, were used in 3 and 2 variants respectively to increase data density. This configuration enables the development of universal grasping algorithms applicable to various robot hands without overfitting to specific gripper types, significantly enhancing the dataset's practical usefulness.</td>
                                            </tr>
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">Reasoning &amp; Application</strong></td>
                                                <td class="border themeable-border px-3 py-2">It was mentioned that various lighting conditions, camera viewpoints, and types were used during data collection. If a robot trained with this dataset were deployed in an actual logistics warehouse with dim lighting or heavy shadows, what performance could be expected?</td>
                                                <td class="border themeable-border px-3 py-2">As stated in the source context, the data was acquired under various lighting conditions. Therefore, robot vision algorithms trained on this data are expected to exhibit robust performance against illumination changes. Even in actual logistics warehouse environments with dim or shadow-heavy lighting, there is a high likelihood of more stable extraction and recognition of object feature points, which can directly lead to improved grasping success rates.</td>
                                            </tr>
                                        </tbody>
                                    </table>
                                </div>
                            </details>

                            <p class="themeable-text-secondary mt-4 text-sm italic">
                                Next, we examine what learning models can be developed and applied based on this systematically collected data.
                            </p>
                        </div>

                        <!-- 1-3: 학습 모델 및 응용 -->
                        <div class="themeable-bg card-hover rounded-lg p-6 mb-6">
                            <h4 class="text-xl font-semibold mb-4 teal-text">1-3. Learning Models & Applications</h4>

                            <div class="themeable-text-secondary space-y-3 mb-4">
                                <p>
                                    The ultimate value of a high-quality dataset is realized when it contributes to solving real industrial problems through superior AI models. A dataset alone is merely potential; its value can only be realized through learning models that interpret and utilize it. This section analyzes representative AI models used to validate the effectiveness and maximize the utilization potential of the 'Occluded Object Reasoning Dataset.' By illuminating the concrete application services that can be realized, we explore how data is transformed into practical technology.
                                </p>
                            </div>

                            <details class="mt-4">
                                <summary class="cursor-pointer font-semibold teal-text hover:text-orange-500 transition-colors">
                                    View QA Samples (4 pairs)
                                </summary>
                                <div class="mt-4 overflow-x-auto">
                                    <table class="w-full border-collapse text-sm">
                                        <thead>
                                            <tr>
                                                <th class="border themeable-border px-3 py-2 text-left w-32">Question Type</th>
                                                <th class="border themeable-border px-3 py-2 text-left">Question</th>
                                                <th class="border themeable-border px-3 py-2 text-left">Answer</th>
                                            </tr>
                                        </thead>
                                        <tbody class="themeable-text-secondary">
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">Simple Extraction</strong></td>
                                                <td class="border themeable-border px-3 py-2">Among the models used for 'amodal instance segmentation,' what is the name of the GIST-developed model selected as the best-performing model as of 2022?</td>
                                                <td class="border themeable-border px-3 py-2">It is UOAIS-Net. This model was proposed to detect occluded regions of unseen objects as well.</td>
                                            </tr>
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">Summary &amp; Explanation</strong></td>
                                                <td class="border themeable-border px-3 py-2">Please explain the key principle by which the 6D Object Pose Estimation model 'PVNet' improves performance in occluded environments.</td>
                                                <td class="border themeable-border px-3 py-2">PVNet improves performance through per-object Keypoint Vector Field prediction. That is, by having each pixel in the image predict a direction vector pointing toward a specific keypoint of the object, the positions of occluded keypoints can be estimated through pixel voting from visible parts, even when portions of the object are hidden, thus improving occluded object pose estimation performance.</td>
                                            </tr>
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">Comparison &amp; Analysis</strong></td>
                                                <td class="border themeable-border px-3 py-2">What is the most significant structural difference between instance segmentation models Mask R-CNN and SOLOv2, and how does this difference affect efficiency?</td>
                                                <td class="border themeable-border px-3 py-2">The biggest difference is that Mask R-CNN has a 'two-stage' architecture while SOLOv2 has a 'single-stage' architecture. The two-stage Mask R-CNN first extracts region proposal candidates and then predicts masks for each candidate region. In contrast, the single-stage SOLOv2 processes both steps simultaneously. Due to this structural difference, SOLOv2 achieves faster prediction speed by eliminating the inefficiencies of the two-stage architecture.</td>
                                            </tr>
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">Reasoning &amp; Application</strong></td>
                                                <td class="border themeable-border px-3 py-2">Based on GIST AILAB's 'Robot Occluded Object Grasping Sequence Planning' application service example, infer how applying this technology to a smart factory parts assembly line could improve productivity.</td>
                                                <td class="border themeable-border px-3 py-2">In parts assembly lines, various components are often stacked and overlapping in parts bins. Applying the technology presented in the source context, when a target part is occluded by other parts, the robot arm can accurately recognize occluded and visible regions and autonomously determine which parts to remove first to reach the target, planning the sequence accordingly. This eliminates the need for human intervention to rearrange parts and reduces robot idle time, significantly improving productivity by shortening the overall assembly line cycle time.</td>
                                            </tr>
                                        </tbody>
                                    </table>
                                </div>
                            </details>

                            <p class="themeable-text-secondary mt-4 text-sm italic">
                                Having covered object recognition in logistics and industrial environments, the next chapter continues the discussion by examining the characteristics of another key dataset: 'Delivery Robot Off-Road Navigation Data.'
                            </p>
                        </div>
                    </div>

                    <!-- Dataset 2: Delivery Robot Off-Road Navigation Dataset -->
                    <div id="dataset-2" class="mb-8">
                        <h3 class="text-2xl font-semibold themeable-heading mb-4 flex items-center">
                            <span class="teal-text mr-2">2.</span>
                            Delivery Robot Off-Road Navigation Dataset
                        </h3>

                        <!-- 2-1: 개요 및 라벨링 -->
                        <div class="themeable-bg card-hover rounded-lg p-6 mb-6">
                            <h4 class="text-xl font-semibold mb-4 teal-text">2-1. Overview & Labeling</h4>

                            <div class="themeable-text-secondary space-y-3 mb-4">
                                <p>
                                    로봇의 자율주행 영역이 잘 정비된 '도로'를 넘어 보도, 공원, 실내 공간 등 예측 불가능한 요소가 많은 '비도로' 환경으로 확장됨에 따라, 새로운 유형의 데이터셋이 필수적으로 요구되고 있습니다. 비도로 환경은 불특정 다수의 보행자, 다양한 형태의 장애물, 불분명한 경계 등 기존 도로 주행 데이터로는 해결할 수 없는 복잡성을 내포하기 때문입니다. 이 섹션에서는 'Delivery Robot Off-Road Navigation Dataset'의 개요와 그 핵심 구성 요소인 2D 및 3D 데이터의 라벨링 클래스 정의를 분석합니다. 이를 통해 이 데이터셋이 비도로 환경의 복잡성을 AI가 구조적으로 이해하는 데 어떤 역할을 하는지 그 중요성을 기술합니다.
                                </p>
                            </div>

                            <details class="mt-4">
                                <summary class="cursor-pointer font-semibold teal-text hover:text-orange-500 transition-colors">
                                    View QA Samples (4 pairs)
                                </summary>
                                <div class="mt-4 overflow-x-auto">
                                    <table class="w-full border-collapse text-sm">
                                        <thead>
                                            <tr>
                                                <th class="border themeable-border px-3 py-2 text-left w-32">Question Type</th>
                                                <th class="border themeable-border px-3 py-2 text-left">Question</th>
                                                <th class="border themeable-border px-3 py-2 text-left">Answer</th>
                                            </tr>
                                        </thead>
                                        <tbody class="themeable-text-secondary">
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">Simple Extraction</strong></td>
                                                <td class="border themeable-border px-3 py-2">What is the labeling type of the 3D LiDAR data, and how many processing target classes are defined in total?</td>
                                                <td class="border themeable-border px-3 py-2">The labeling type is 'Cuboid,' and there are 9 processing target classes in total: 'passenger car,' 'motorcycle,' 'bicycle,' 'kickboard,' 'pedestrian,' 'bus,' 'truck,' 'other dynamic objects,' and 'other static objects.'</td>
                                            </tr>
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">Summary &amp; Explanation</strong></td>
                                                <td class="border themeable-border px-3 py-2">Among the Semantic Segmentation processing principles for 2D image data, how are cases handled where the same object is occluded by another object (Area A) or where pedestrians with expected sudden behavior are occluded (Area B)?</td>
                                                <td class="border themeable-border px-3 py-2">For Area A, the same object is divided into two regions and annotated separately. For Area B, pedestrians who pose a safety risk are processed with the 'is_crowd' (occluded) attribute, enabling the robot to recognize and prevent potential dangers.</td>
                                            </tr>
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">Comparison &amp; Analysis</strong></td>
                                                <td class="border themeable-border px-3 py-2">Compare the 2D processing target classes (22 types) with the 3D processing target classes (9 types), and analyze the reasons for the difference in relation to sensor characteristics.</td>
                                                <td class="border themeable-border px-3 py-2">2D classes are rich in information about the surrounding environment and background such as 'road,' 'sidewalk,' 'signs,' 'traffic lights,' and 'buildings,' while 3D classes focus primarily on dynamic or static 'objects' such as 'passenger cars' and 'pedestrians.' This is due to the information collection characteristics of each sensor. 2D images are advantageous for recognizing planar environmental information such as drivable areas and signs through color and texture information, while 3D LiDAR has greater strength in accurately detecting the position and size of three-dimensional objects through distance and shape information. In other words, the labeling targets were optimized according to each sensor's information collection method.</td>
                                            </tr>
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">Reasoning &amp; Application</strong></td>
                                                <td class="border themeable-border px-3 py-2">The JSON structure of the labeling data includes a 'sidewalk_flatness' (road surface flatness) attribute. Infer how delivery robots could utilize this attribute data for safer and more efficient navigation.</td>
                                                <td class="border themeable-border px-3 py-2">Delivery robots can learn from 'sidewalk_flatness' data to proactively identify sections with 'low' flatness. This allows them to autonomously decelerate before entering such sections or switch to a driving mode that minimizes vibration, reducing the risk of damage to items being delivered. Additionally, as this information accumulates, it can enhance path planning algorithms that prioritize the smoothest routes, contributing to long-term reduction of mechanical wear on robots and maintenance cost savings.</td>
                                            </tr>
                                        </tbody>
                                    </table>
                                </div>
                            </details>

                            <p class="themeable-text-secondary mt-4 text-sm italic">
                                Since data quality largely depends on the collection location and environment, the next section provides a detailed analysis of where and how this dataset was collected.
                            </p>
                        </div>

                        <!-- 2-2: 수집 장소 및 환경 -->
                        <div class="themeable-bg card-hover rounded-lg p-6 mb-6">
                            <h4 class="text-xl font-semibold mb-4 teal-text">2-2. Collection Locations & Environment</h4>

                            <div class="themeable-text-secondary space-y-3 mb-4">
                                <p>
                                    AI 모델이 특정 환경에서만 작동하는 '온실 속 화초'가 되지 않고, 다양한 실제 환경에서 범용적으로 활용되기 위해서는 데이터 수집 단계에서부터 포괄성과 현실성을 확보하는 것이 무엇보다 중요합니다. 데이터 수집 장소의 선택은 데이터셋의 다양성과 직결되며, 이는 최종적으로 AI 모델의 일반화 성능을 결정하는 핵심 변수입니다. 이 섹션에서는 'Delivery Robot Off-Road Navigation Dataset'이 수집된 구체적인 장소들의 환경적 특성을 분석하고, 이러한 장소 선정이 데이터셋의 현실성을 어떻게 담보하며 로봇이 마주할 실제 도전 과제들을 얼마나 잘 반영하고 있는지 그 전략적 의의를 평가합니다.
                                </p>
                            </div>

                            <details class="mt-4">
                                <summary class="cursor-pointer font-semibold teal-text hover:text-orange-500 transition-colors">
                                    View QA Samples (4 pairs)
                                </summary>
                                <div class="mt-4 overflow-x-auto">
                                    <table class="w-full border-collapse text-sm">
                                        <thead>
                                            <tr>
                                                <th class="border themeable-border px-3 py-2 text-left w-32">Question Type</th>
                                                <th class="border themeable-border px-3 py-2 text-left">Question</th>
                                                <th class="border themeable-border px-3 py-2 text-left">Answer</th>
                                            </tr>
                                        </thead>
                                        <tbody class="themeable-text-secondary">
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">Simple Extraction</strong></td>
                                                <td class="border themeable-border px-3 py-2">Among the data collection locations, which location enabled free autonomous driving operation through a regulatory sandbox?</td>
                                                <td class="border themeable-border px-3 py-2">It is Sejong Central Park.</td>
                                            </tr>
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">Summary &amp; Explanation</strong></td>
                                                <td class="border themeable-border px-3 py-2">Please explain from two perspectives why 'Bukchon Hanok Village' is considered an optimal location for verifying off-road environment driving robots.</td>
                                                <td class="border themeable-border px-3 py-2">First, it includes both narrow roads where vehicles can pass and alley environments where vehicle entry is restricted, enabling testing of various off-road conditions. Second, as a complex cultural heritage area where residential and commercial zones coexist, it provides a realistic environment with diverse types of pedestrians and static/dynamic obstacles.</td>
                                            </tr>
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">Comparison &amp; Analysis</strong></td>
                                                <td class="border themeable-border px-3 py-2">Compare the environmental characteristics of the outdoor collection location 'Insadong' and the indoor collection location 'Naver New Headquarters (1784),' and analyze what different challenges each location presents for robot autonomous driving technology verification.</td>
                                                <td class="border themeable-border px-3 py-2">'Insadong' is alley-centric with very high pedestrian density, focusing on verification of dynamic obstacle avoidance technology for safe navigation while avoiding unpredictable movements among numerous unspecified pedestrians. In contrast, 'Naver New Headquarters' is a refined indoor space where actual robot services operate, suitable for verification of more precise SLAM and localization technologies, including accurate recognition of narrow corridors, glass walls, and indoor structures, and building continuous indoor-outdoor driving datasets.</td>
                                            </tr>
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">Reasoning &amp; Application</strong></td>
                                                <td class="border themeable-border px-3 py-2">This dataset was collected during 'summer' and 'autumn.' If a robot trained with this data were operated in off-road environments during a snowy 'winter,' what potential problems could arise, and what additional data would be needed to resolve them?</td>
                                                <td class="border themeable-border px-3 py-2">According to the given information, data was only collected during summer and autumn. Therefore, in winter, snow would blur the distinction between lanes, sidewalks, and curbs, and LiDAR sensors could misidentify snow as obstacles or suffer performance degradation due to diffuse reflection. Additionally, pedestrian walking patterns (slipping, etc.) would differ. Consequently, driving performance in winter could be significantly degraded with the current dataset alone. To resolve this, winter-specific driving data (images and LiDAR) from snow-covered environments, snowy conditions, and icy roads should be additionally collected along with corresponding labeling data (e.g., adding 'snow-covered sidewalk' and 'icy road' classes) for model retraining.</td>
                                            </tr>
                                        </tbody>
                                    </table>
                                </div>
                            </details>

                            <p class="themeable-text-secondary mt-4 text-sm italic">
                                Next, we explore the learning models developed and verified using data collected from these diverse environments, and their application potential.
                            </p>
                        </div>

                        <!-- 2-3: 학습 모델 및 응용 -->
                        <div class="themeable-bg card-hover rounded-lg p-6 mb-6">
                            <h4 class="text-xl font-semibold mb-4 teal-text">2-3. Learning Models & Applications</h4>

                            <div class="themeable-text-secondary space-y-3 mb-4">
                                <p>
                                    데이터셋의 실효성은 최종적으로 AI 모델의 성능과 실제 응용 가능성을 통해 입증됩니다. 아무리 방대하고 다양한 데이터를 구축하더라도, 이를 효과적으로 학습하고 활용할 수 있는 모델이 없다면 데이터는 단순한 저장 공간만 차지하게 됩니다. 본 섹션에서는 'Delivery Robot Off-Road Navigation Dataset'의 유효성을 검증하기 위해 선정된 2D 및 3D 학습 모델을 분석합니다. 각 모델의 선정 사유와 개발 내용을 통해 데이터의 기술적 가치를 평가하고, 이를 통해 실현될 수 있는 구체적인 응용 서비스를 조망하며 데이터의 산업적 가치를 평가합니다.
                                </p>
                            </div>

                            <details class="mt-4">
                                <summary class="cursor-pointer font-semibold teal-text hover:text-orange-500 transition-colors">
                                    View QA Samples (4 pairs)
                                </summary>
                                <div class="mt-4 overflow-x-auto">
                                    <table class="w-full border-collapse text-sm">
                                        <thead>
                                            <tr>
                                                <th class="border themeable-border px-3 py-2 text-left w-32">Question Type</th>
                                                <th class="border themeable-border px-3 py-2 text-left">Question</th>
                                                <th class="border themeable-border px-3 py-2 text-left">Answer</th>
                                            </tr>
                                        </thead>
                                        <tbody class="themeable-text-secondary">
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">Simple Extraction</strong></td>
                                                <td class="border themeable-border px-3 py-2">Among the 3D dynamic object detection model candidates, which model was not finally selected due to poor performance reproducibility of its open-source code?</td>
                                                <td class="border themeable-border px-3 py-2">It is the BtcDet model.</td>
                                            </tr>
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">Summary &amp; Explanation</strong></td>
                                                <td class="border themeable-border px-3 py-2">Please describe the development details of 'ERF-PSPNet,' which was selected as the 2D driving environment recognition model.</td>
                                                <td class="border themeable-border px-3 py-2">This model takes 2D RGB images as input and outputs pixel-wise recognition results for road, sidewalk, curb, crosswalk, and indoor floor surfaces relevant to robot autonomous driving, identifying drivable areas.</td>
                                            </tr>
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">Comparison &amp; Analysis</strong></td>
                                                <td class="border themeable-border px-3 py-2">While 'PointPillars' was selected as the 3D dynamic object detection model, 'VoxelRCNN' was not selected. Compare and analyze the selection and non-selection rationale that can estimate the performance difference between the two models.</td>
                                                <td class="border themeable-border px-3 py-2">PointPillars was selected because it showed good performance on the KITTI dataset with high performance reproducibility, suggesting stable detection performance even in environments with various objects. In contrast, VoxelRCNN was not selected because it exhibited degraded performance when detecting multiple objects simultaneously, which was deemed unsuitable for delivery robot missions that require simultaneous processing of multiple dynamic objects (pedestrians, bicycles, etc.) in complex off-road environments.</td>
                                            </tr>
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">Reasoning &amp; Application</strong></td>
                                                <td class="border themeable-border px-3 py-2">Assume we combine the drivable area recognition model and dynamic object detection model developed with this dataset and apply them to a 'patrol robot in a park environment.' Infer how this robot could perform its patrol mission more safely and intelligently.</td>
                                                <td class="border themeable-border px-3 py-2">The drivable area recognition model helps the robot clearly distinguish sidewalks, grass, and bicycle paths within the park to patrol stably without deviating from designated routes. Simultaneously, the dynamic object detection model detects and predicts dynamic objects in real time, such as suddenly running children, fast-passing bicycles, and people playing ball. Through the combination of these two models, the patrol robot can go beyond simply following a path to make intelligent decisions such as proactively recognizing dangerous situations and autonomously stopping or detour to safe routes, ensuring the safety of park visitors while performing patrol missions.</td>
                                            </tr>
                                        </tbody>
                                    </table>
                                </div>
                            </details>

                            <p class="themeable-text-secondary mt-4 text-sm italic">
                                In the next chapter, we deepen the discussion by analyzing the characteristics of 'Robot-View Driving Video Data,' which focuses more on complex indoor environments as well as outdoor ones.
                            </p>
                        </div>
                    </div>

                    <!-- Dataset 3: Robot-View Driving Video Dataset -->
                    <div id="dataset-3" class="mb-8">
                        <h3 class="text-2xl font-semibold themeable-heading mb-4 flex items-center">
                            <span class="teal-text mr-2">3.</span>
                            Robot-View Driving Video Dataset
                        </h3>

                        <!-- 3-1: 개요 및 클래스 분포 -->
                        <div class="themeable-bg card-hover rounded-lg p-6 mb-6">
                            <h4 class="text-xl font-semibold mb-4 teal-text">3-1. Overview & Class Distribution</h4>

                            <div class="themeable-text-secondary space-y-3 mb-4">
                                <p>
                                    자율주행 로봇의 활동 무대가 실외를 넘어 식당, 쇼핑몰, 터미널과 같은 복잡한 실내 다중이용시설로 확장됨에 따라, 로봇의 시점에서 세상을 이해하고 해석하는 데이터의 중요성이 커지고 있습니다. 실내 환경은 조명 변화가 잦고, 사람과 같은 동적 장애물이 밀집하며, 예측 불가능한 정적 장애물(의자, 테이블 등)이 많아 로봇의 주행에 큰 어려움을 줍니다. 이 섹션에서는 이러한 문제를 해결하기 위한 'Robot-View Driving Video Dataset'의 핵심적인 데이터 구성과, 실제 환경의 객체 분포를 반영하는 클래스 통계를 분석하여 데이터셋의 현실성과 균형성을 평가합니다.
                                </p>
                            </div>

                            <details class="mt-4">
                                <summary class="cursor-pointer font-semibold teal-text hover:text-orange-500 transition-colors">
                                    View QA Samples (4 pairs)
                                </summary>
                                <div class="mt-4 overflow-x-auto">
                                    <table class="w-full border-collapse text-sm">
                                        <thead>
                                            <tr>
                                                <th class="border themeable-border px-3 py-2 text-left w-32">Question Type</th>
                                                <th class="border themeable-border px-3 py-2 text-left">Question</th>
                                                <th class="border themeable-border px-3 py-2 text-left">Answer</th>
                                            </tr>
                                        </thead>
                                        <tbody class="themeable-text-secondary">
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">Simple Extraction</strong></td>
                                                <td class="border themeable-border px-3 py-2">What are the three sensor data formats included in the source data of the 'Robot-View Driving Video Data'?</td>
                                                <td class="border themeable-border px-3 py-2">They are PCD (LIDAR data), CSV (6D IMU Sensor data), and PNG (RGB-D Depth data).</td>
                                            </tr>
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">Summary &amp; Explanation</strong></td>
                                                <td class="border themeable-border px-3 py-2">Describe the two types of robots used to collect data for this dataset, and provide the data construction ratio for each.</td>
                                                <td class="border themeable-border px-3 py-2">Data was collected using a quadruped walking robot (RB1) and a wheeled driving robot (RB2). The data construction ratio is 45.92% (68,980 items) for the quadruped robot and 54.08% (81,249 items) for the wheeled robot.</td>
                                            </tr>
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">Comparison &amp; Analysis</strong></td>
                                                <td class="border themeable-border px-3 py-2">What are the top 3 locations with the highest composition ratios and the bottom 3 locations with the lowest composition ratios in the collection location distribution, and how can the dataset's collection environment characteristics be analyzed through this?</td>
                                                <td class="border themeable-border px-3 py-2">The top 3 locations are medium-sized restaurants (9.53%), banks (9.50%), and terminals (9.38%), while the bottom 3 are large marts (6.60%), exhibition halls (7.09%), and indoor parking lots (7.52%). This distribution shows a relatively even spread between 6.6% and 9.53% without bias toward any specific environment, indicating the intent to ensure data diversity and balance so that robots can be universally deployed across various indoor multi-use facility environments.</td>
                                            </tr>
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">Reasoning &amp; Application</strong></td>
                                                <td class="border themeable-border px-3 py-2">Among static object classes, 'chairs' account for 22.29% and 'pillars' for 9.76%, while the dynamic object 'people' holds an overwhelming 51.41%. What strengths can be inferred for a robot trained with this data during indoor autonomous driving?</td>
                                                <td class="border themeable-border px-3 py-2">According to the source context, 'people,' 'chairs,' and 'pillars' are the most commonly encountered dynamic and static obstacles in indoor environments. The abundance of data for these classes means the robot will specialize in avoiding unpredictable human movements, navigating through randomly arranged chairs, and reliably recognizing fixed structures like pillars. Therefore, this robot can be expected to demonstrate excellent avoidance maneuvers and path planning capabilities even in complex indoor environments with many people and obstacles, such as crowded restaurants or lobbies.</td>
                                            </tr>
                                        </tbody>
                                    </table>
                                </div>
                            </details>

                            <p class="themeable-text-secondary mt-4 text-sm italic">
                                Next, we examine the verification environment and models used to ensure the quality and validity of this dataset.
                            </p>
                        </div>

                        <!-- 3-2: 검증 환경 및 모델 -->
                        <div class="themeable-bg card-hover rounded-lg p-6 mb-6">
                            <h4 class="text-xl font-semibold mb-4 teal-text">3-2. Verification Environment & Models</h4>

                            <div class="themeable-text-secondary space-y-3 mb-4">
                                <p>
                                    데이터셋의 신뢰도는 단순히 많은 양의 데이터를 수집하는 것만으로 확보되지 않습니다. 객관적인 성능 지표와 표준화된 하드웨어 및 소프트웨어 환경에서 수행되는 엄격한 검증 절차를 통해 비로소 그 기술적 완성도가 입증됩니다. 본 섹션에서는 'Robot-View Driving Video Dataset'의 유효성을 검증하기 위해 구성된 하드웨어 및 소프트웨어 환경을 상세히 기술합니다. 또한, 2D 바운딩 박스, 3D 큐보이드, SLAM(동시적 위치추정 및 지도작성) 각 성능을 평가하기 위해 사용된 최신 학습 알고리즘들을 분석하여 데이터의 기술적 완성도를 평가합니다.
                                </p>
                            </div>

                            <details class="mt-4">
                                <summary class="cursor-pointer font-semibold teal-text hover:text-orange-500 transition-colors">
                                    View QA Samples (4 pairs)
                                </summary>
                                <div class="mt-4 overflow-x-auto">
                                    <table class="w-full border-collapse text-sm">
                                        <thead>
                                            <tr>
                                                <th class="border themeable-border px-3 py-2 text-left w-32">Question Type</th>
                                                <th class="border themeable-border px-3 py-2 text-left">Question</th>
                                                <th class="border themeable-border px-3 py-2 text-left">Answer</th>
                                            </tr>
                                        </thead>
                                        <tbody class="themeable-text-secondary">
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">Simple Extraction</strong></td>
                                                <td class="border themeable-border px-3 py-2">What are the name and performance metric of the learning algorithm used for SLAM performance validation?</td>
                                                <td class="border themeable-border px-3 py-2">The learning algorithm is Fast-LIO2, and the performance metric is End to End RMSE (within 0.2m).</td>
                                            </tr>
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">Summary &amp; Explanation</strong></td>
                                                <td class="border themeable-border px-3 py-2">When validating 2D bounding box object detection, please describe the ratio for splitting total data into Training, Validation, and Test sets, and the number of data items in each set.</td>
                                                <td class="border themeable-border px-3 py-2">The total data is split in a Train:Val:Test = 80:10:10 ratio. The Training Set comprises 80% with 120,220 items, the Validation Set comprises 10% with 14,993 items, and the Test Set comprises 10% with 15,016 items.</td>
                                            </tr>
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">Comparison &amp; Analysis</strong></td>
                                                <td class="border themeable-border px-3 py-2">Compare the GPU and OS environments used for 2D object detection (YOLOV7) and 3D object detection (FocalsConv) validation, and explain the differences from the SLAM performance verification environment.</td>
                                                <td class="border themeable-border px-3 py-2">Both 2D and 3D object detection are deep learning-based models, so they identically use 'NVIDIA RTX A6000 * 4' GPUs and 'Ubuntu 18.04.6 LTS' for large-scale parallel computation. In contrast, the Fast-LIO2 algorithm used for SLAM performance verification is not an AI model, so it uses only CPU (AMD EPYC 7742) without GPU, and the OS also differs with 'Ubuntu 20.04.' This shows that optimized environments were configured according to the computational characteristics of each verification target algorithm.</td>
                                            </tr>
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">Reasoning &amp; Application</strong></td>
                                                <td class="border themeable-border px-3 py-2">The 3D object detection model FocalsConv is said to combine RGB (3 channels) with Depth map (1 channel). Infer what advantages this approach could have over using only LiDAR data, particularly when recognizing objects like 'glass doors.'</td>
                                                <td class="border themeable-border px-3 py-2">LiDAR sensors may fail to properly recognize transparent 'glass doors,' passing through them or treating them as noise. However, RGB cameras can capture visual features such as the door frame, handle, and surface reflections. By combining RGB and Depth information like the FocalsConv model, the visual information from the camera can compensate for what LiDAR misses. That is, Depth information detects the approximate presence of a plane, and RGB information identifies the visual context that it is a 'door,' significantly increasing the probability that the robot accurately recognizes glass doors as obstacles rather than misidentifying them as passable spaces.</td>
                                            </tr>
                                        </tbody>
                                    </table>
                                </div>
                            </details>

                            <p class="themeable-text-secondary mt-4 text-sm italic">
                                While we have focused on the robot's interaction with the external environment through 'perception' and 'navigation,' the next chapter analyzes datasets for diagnosing the robot's own 'status' and 'maintenance.'
                            </p>
                        </div>
                    </div>

                    <!-- Dataset 4: 실내공간 유지관리 서비스 로봇 데이터셋 -->
                    <div id="dataset-4" class="mb-8">
                        <h3 class="text-2xl font-semibold themeable-heading mb-4 flex items-center">
                            <span class="teal-text mr-2">4.</span>
                            실내공간 유지관리 서비스 로봇 데이터셋
                        </h3>

                        <!-- 4-1: 개요 및 포맷 -->
                        <div class="themeable-bg card-hover rounded-lg p-6 mb-6">
                            <h4 class="text-xl font-semibold mb-4 teal-text">4-1. 개요 및 포맷</h4>

                            <div class="themeable-text-secondary space-y-3 mb-4">
                                <p>
                                    서비스 로봇이 산업 현장과 일상 공간에 안정적으로 확산되기 위해서는 단순히 주어진 임무를 수행하는 것을 넘어, 스스로의 상태를 진단하고 고장을 사전에 예측하여 선제적으로 대응하는 유지보수 기술이 핵심입니다. 이를 위해서는 로봇의 내부 상태를 실시간으로 기록한 데이터가 필수적입니다. 본 섹션에서는 이러한 목적을 위해 구축된 '실내공간 유지관리 서비스 로봇 데이터셋'의 고유한 특징, 즉 이미지나 영상이 아닌 텍스트 기반의 상태 데이터라는 점을 조명합니다. 상세한 JSON 데이터 포맷을 분석하여 이 데이터가 로봇의 상태 추론 및 이상 감지 AI 모델 개발에 어떤 정보를 제공하는지 평가합니다.
                                </p>
                            </div>

                            <details class="mt-4">
                                <summary class="cursor-pointer font-semibold teal-text hover:text-orange-500 transition-colors">
                                    View QA Samples (4 pairs)
                                </summary>
                                <div class="mt-4 overflow-x-auto">
                                    <table class="w-full border-collapse text-sm">
                                        <thead>
                                            <tr>
                                                <th class="border themeable-border px-3 py-2 text-left w-32">Question Type</th>
                                                <th class="border themeable-border px-3 py-2 text-left">Question</th>
                                                <th class="border themeable-border px-3 py-2 text-left">Answer</th>
                                            </tr>
                                        </thead>
                                        <tbody class="themeable-text-secondary">
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">Simple Extraction</strong></td>
                                                <td class="border themeable-border px-3 py-2">라벨링 데이터의 JSON 포맷에서, 로봇의 에러 발생 여부와 에러 코드를 나타내는 속성명(key)은 각각 무엇인가?</td>
                                                <td class="border themeable-border px-3 py-2">에러 발생 여부는 'errorState' (boolean), 에러 코드는 'errorCode' (String) 입니다.</td>
                                            </tr>
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">Summary &amp; Explanation</strong></td>
                                                <td class="border themeable-border px-3 py-2">이 데이터셋의 파일명은 어떤 5가지 정보의 조합으로 구성되는지 순서대로 설명해 주십시오.</td>
                                                <td class="border themeable-border px-3 py-2">파일명은 '로봇의 종류', '로봇에게 부여된 고유 ID', '데이터가 생성된 월', '로봇이 수행하는 태스크 ID', '데이터 순번'의 순서로 구성됩니다. (예: 안내로봇_안내로봇01_11_task01_05233.json)</td>
                                            </tr>
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">Comparison &amp; Analysis</strong></td>
                                                <td class="border themeable-border px-3 py-2">원천 데이터에 포함된 'deviceData' 객체와 'taskData' 객체의 정보 유형을 비교하고, 이 두 정보가 로봇 유지보수 관점에서 어떻게 상호 보완적으로 활용될 수 있는지 분석해 주십시오.</td>
                                                <td class="border themeable-border px-3 py-2">'deviceData'는 배터리 잔량, 충돌 횟수, 현재 속도 등 로봇의 물리적, 하드웨어적 상태 정보를 담고 있습니다. 반면, 'taskData'는 작업 이름, 예상 소요 시간, 현재 작업 상태 등 로봇이 수행하는 임무의 논리적, 소프트웨어적 상태 정보를 담고 있습니다. 이 둘을 결합하면, 예를 들어 특정 'task' 수행 중에만 'collision' 횟수가 급증하는 현상을 발견하여 해당 작업의 경로 계획에 문제가 있음을 진단하거나, 'batteryLevel'이 특정 'task' 수행 시 유독 빨리 소모되는 것을 파악하여 해당 임무가 로봇에 과부하를 주고 있음을 분석하는 등, 단순 하드웨어 고장이 아닌 작업-상태 간의 복합적인 문제 원인을 진단하는 데 상호 보완적으로 활용될 수 있습니다.</td>
                                            </tr>
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">Reasoning &amp; Application</strong></td>
                                                <td class="border themeable-border px-3 py-2">라벨링 데이터의 'errorStatementLong' 속성은 에러 상황을 상세한 자연어 문장으로 설명합니다. 이 데이터를 LLM 파인튜닝에 활용한다면, 로봇 관제 시스템 운영자에게 어떤 실질적인 도움을 줄 수 있을지 추론해 보십시오.</td>
                                                <td class="border themeable-border px-3 py-2">'E-ENV-O' 같은 단순 에러 코드만으로는 운영자가 문제의 원인을 즉각 파악하기 어렵습니다. 하지만 'errorStatementLong' 데이터("현재 로봇...장애물 감지로 인한 이동 불가 에러로 판단되고...")로 파인튜닝된 LLM은 "현재 서빙로봇이 혼잡한 오피스에서 장애물 감지로 인해 이동 불가 상태입니다"와 같이 코드의 의미를 구체적인 상황과 원인을 포함한 문장으로 자동 변환해 줄 수 있습니다. 더 나아가, LLM은 "주변 장애물을 확인하고 경로를 재설정하거나, 해당 구역의 혼잡도가 낮아질 때까지 대기하는 조치를 권장합니다"와 같은 해결책까지 제시할 수 있습니다. 이는 운영자의 상황 판단 시간을 단축시키고, 신속하고 정확한 초동 조치를 가능하게 하여 로봇 운영 효율성을 크게 향상시킬 것입니다.</td>
                                            </tr>
                                        </tbody>
                                    </table>
                                </div>
                            </details>

                            <p class="themeable-text-secondary mt-4 text-sm italic">
                                로봇의 외부 환경 인식에서 내부 상태 진단까지 살펴보았으니, 이제 로봇의 핵심 기능인 '조작'과 관련된 '로봇 핸드용 객체 특성 식별 데이터'로 분석의 초점을 옮깁니다.
                            </p>
                        </div>
                    </div>

                    <!-- Dataset 5: 로봇 핸드용 객체 특성 식별 데이터셋 -->
                    <div id="dataset-5" class="mb-8">
                        <h3 class="text-2xl font-semibold themeable-heading mb-4 flex items-center">
                            <span class="teal-text mr-2">5.</span>
                            로봇 핸드용 객체 특성 식별 데이터셋
                        </h3>

                        <!-- 5-1: 개요 및 구성 -->
                        <div class="themeable-bg card-hover rounded-lg p-6 mb-6">
                            <h4 class="text-xl font-semibold mb-4 teal-text">5-1. 개요 및 구성</h4>

                            <div class="themeable-text-secondary space-y-3 mb-4">
                                <p>
                                    로봇이 인간처럼 다양한 물체를 능숙하게 다루기 위해서는 단순히 눈으로 보고 형태를 인식하는 것을 넘어, 물체의 무게, 재질, 단단함과 같은 물리적 특성을 이해하는 것이 필수적입니다. 이러한 물리적 특성에 대한 이해 없이는 섬세한 힘 조절이 필요한 조작(manipulation)이 불가능하기 때문입니다. 이 섹션에서는 이러한 과제를 해결하기 위해 구축된 '로봇 핸드용 객체 특성 식별 데이터'의 다중 모달(multi-modal) 구성을 분석합니다. 특히 시각 정보(이미지, 3D 메쉬)와 물리 정보(물리량, 시계열 센서 데이터)가 어떻게 통합되어 객체 조작 지능의 기반을 마련하는지 그 중요성을 평가합니다.
                                </p>
                            </div>

                            <details class="mt-4">
                                <summary class="cursor-pointer font-semibold teal-text hover:text-orange-500 transition-colors">
                                    View QA Samples (4 pairs)
                                </summary>
                                <div class="mt-4 overflow-x-auto">
                                    <table class="w-full border-collapse text-sm">
                                        <thead>
                                            <tr>
                                                <th class="border themeable-border px-3 py-2 text-left w-32">Question Type</th>
                                                <th class="border themeable-border px-3 py-2 text-left">Question</th>
                                                <th class="border themeable-border px-3 py-2 text-left">Answer</th>
                                            </tr>
                                        </thead>
                                        <tbody class="themeable-text-secondary">
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">Simple Extraction</strong></td>
                                                <td class="border themeable-border px-3 py-2">데이터셋 구축 대상이 되는 가정용 물품은 총 몇 종류이며, 각 물품당 목표 이미지 데이터 수량은 얼마인가?</td>
                                                <td class="border themeable-border px-3 py-2">구축 대상 물품은 총 200종류이며, 각 물품당 Low-RGB, Hi-RGB, RGB-D 이미지를 각각 600장 이상 수집하는 것을 목표로 합니다.</td>
                                            </tr>
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">Summary &amp; Explanation</strong></td>
                                                <td class="border themeable-border px-3 py-2">이 데이터셋의 '임무 데이터'는 어떤 5가지의 로봇 핸드 조작 과제로 구성되어 있는지 나열해 주십시오.</td>
                                                <td class="border themeable-border px-3 py-2">'잡기(grasping)', '쥐기(squeezing)', '돌리기(rotating)', '흔들기(shaking)', '긁기(scratching)'의 5가지 조작 과제로 구성되어 있습니다.</td>
                                            </tr>
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">Comparison &amp; Analysis</strong></td>
                                                <td class="border themeable-border px-3 py-2">'품목 데이터'의 'Hi-RGB'와 'Low-RGB' 이미지의 차이점을 설명하고, 이 두 종류의 데이터를 모두 수집하는 것이 로봇 비전 모델의 강건성(robustness)에 어떤 이점을 주는지 분석해 주십시오.</td>
                                                <td class="border themeable-border px-3 py-2">'Hi-RGB'는 12bit raw 데이터 타입의 고해상도, 무압축 이미지인 반면, 'Low-RGB'는 8bit jpg 형식의 손실 압축 이미지입니다. 고품질의 'Hi-RGB' 데이터는 객체의 세밀한 특징을 학습하는 데 유리하고, 저품질의 'Low-RGB' 데이터는 실제 로봇에 탑재된 저사양 카메라나 통신 중 화질 저하가 발생하는 현실적인 상황을 모사합니다. 이 두 종류의 데이터를 모두 학습하면, 로봇 비전 모델이 다양한 품질의 이미지 입력에 대해 안정적인 인식 성능을 발휘하는 강건성을 확보할 수 있습니다.</td>
                                            </tr>
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">Reasoning &amp; Application</strong></td>
                                                <td class="border themeable-border px-3 py-2">이 데이터셋은 객체의 '무게', '크기', '재질'과 같은 물리량 정보와 '촉감', '역감' 같은 시계열 데이터를 포함합니다. 로봇이 처음 보는 물체를 들어 올리려고 할 때, 이 데이터들을 어떻게 종합적으로 활용하여 안전하고 효율적으로 물체를 파지할 수 있을지 추론해 보십시오.</td>
                                                <td class="border themeable-border px-3 py-2">로봇은 먼저 시각 정보(이미지, 3D 메쉬)를 통해 처음 보는 물체가 데이터셋의 어떤 물체와 유사한지 판단합니다. 유사한 물체의 '재질'과 '크기' 정보를 바탕으로 표면의 미끄러움 정도와 무게 중심을 예측하고, '무게' 정보를 통해 물체를 들어 올리는 데 필요한 최소한의 힘을 추정합니다. 이후, 파지를 시작하면서 '촉감' 및 '역감' 센서 데이터를 실시간으로 피드백 받아, 물체가 미끄러지거나 찌그러지지 않도록 파지하는 힘을 미세하게 조절할 수 있습니다. 이는 시각 정보만으로는 불가능한, 물리적 특성에 기반한 정교하고 안정적인 파지를 가능하게 합니다.</td>
                                            </tr>
                                        </tbody>
                                    </table>
                                </div>
                            </details>

                            <p class="themeable-text-secondary mt-4 text-sm italic">
                                다음으로, 이러한 풍부한 다중 모달 데이터를 수집하기 위해 사용된 구체적인 도구와 표준화된 절차에 대해 자세히 살펴보겠습니다.
                            </p>
                        </div>

                        <!-- 5-2: 수집 도구 및 절차 -->
                        <div class="themeable-bg card-hover rounded-lg p-6 mb-6">
                            <h4 class="text-xl font-semibold mb-4 teal-text">5-2. 수집 도구 및 절차</h4>

                            <div class="themeable-text-secondary space-y-3 mb-4">
                                <p>
                                    신뢰할 수 있는 데이터셋은 감이나 주관이 아닌, 표준화된 수집 도구와 체계적인 절차를 통해 만들어집니다. 데이터 수집에 사용되는 센서의 정밀도와 일관된 수집 절차는 데이터의 품질을 보증하고, 이를 통해 학습된 AI 모델의 재현성과 신뢰성을 담보하는 기반이 됩니다. 본 섹션에서는 '로봇 핸드용 객체 특성 식별 데이터셋'을 구축하는 데 사용된 고정밀 센서와 로봇 핸드 등 핵심 수집 도구의 사양을 분석합니다. 또한, 데이터가 일관성 있게 획득되는 수집 절차를 단계별로 설명하여 데이터의 품질 보증 과정을 조명합니다.
                                </p>
                            </div>

                            <details class="mt-4">
                                <summary class="cursor-pointer font-semibold teal-text hover:text-orange-500 transition-colors">
                                    View QA Samples (4 pairs)
                                </summary>
                                <div class="mt-4 overflow-x-auto">
                                    <table class="w-full border-collapse text-sm">
                                        <thead>
                                            <tr>
                                                <th class="border themeable-border px-3 py-2 text-left w-32">Question Type</th>
                                                <th class="border themeable-border px-3 py-2 text-left">Question</th>
                                                <th class="border themeable-border px-3 py-2 text-left">Answer</th>
                                            </tr>
                                        </thead>
                                        <tbody class="themeable-text-secondary">
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">Simple Extraction</strong></td>
                                                <td class="border themeable-border px-3 py-2">객체의 크기를 측정하는 데 사용된 '디지털 버니어 캘리퍼스'의 측정 범위와 측정 오차는 얼마인가?</td>
                                                <td class="border themeable-border px-3 py-2">측정 범위는 0~150mm이며, 측정 오차는 0.2mm입니다.</td>
                                            </tr>
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">Summary &amp; Explanation</strong></td>
                                                <td class="border themeable-border px-3 py-2">200개 품목의 이미지를 일관된 조건에서 촬영하기 위해 구성한 '촬영 지그'의 특징을 설명해 주십시오.</td>
                                                <td class="border themeable-border px-3 py-2">촬영 지그는 정반 플레이트에 물리적으로 고정된 브라켓, 5세트의 카메라와 RGB-D 센서, 그리고 물체를 올려두는 회전식 턴테이블로 구성됩니다. 이를 통해 물체를 일정 각도로 회전시키면서 여러 시점의 데이터를 체계적으로 획득할 수 있습니다.</td>
                                            </tr>
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">Comparison &amp; Analysis</strong></td>
                                                <td class="border themeable-border px-3 py-2">데이터 수집에 사용된 'Allegro Hand'와 '손가락형 촉각센서'의 역할을 각각 분석하고, 이 두 도구의 결합이 어떻게 인간의 손과 유사한 데이터 수집을 가능하게 하는지 설명해 주십시오.</td>
                                                <td class="border themeable-border px-3 py-2">'Allegro Hand'는 16개의 독립적인 관절을 제어하여 잡기, 쥐기 등 인간의 손과 유사한 '동작'을 수행하는 역할을 합니다. 여기에 부착된 '손가락형 촉각센서'는 그 동작 과정에서 물체와 접촉할 때 발생하는 압력 분포, 즉 '감각' 데이터를 수집합니다. 이 둘의 결합은 로봇이 단순히 물체를 잡는 것을 넘어, 잡는 동안의 미세한 힘의 변화와 접촉 상태를 데이터화함으로써 인간의 '촉각을 동반한 조작' 행위를 모사할 수 있게 합니다.</td>
                                            </tr>
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">Reasoning &amp; Application</strong></td>
                                                <td class="border themeable-border px-3 py-2">데이터 수집 절차 중 '로봇 핸드의 임무 수행 시 힘의 단계를 5단계로 나누어 단계별 10회 임무 수행'하는 과정이 있습니다. 이처럼 힘의 단계를 세분화하여 데이터를 수집하는 것이 '깨지기 쉬운 계란'이나 '말랑한 두부' 같은 물체를 다루는 로봇을 개발할 때 어떤 이점을 줄 수 있는지 추론해 보십시오.</td>
                                                <td class="border themeable-border px-3 py-2">힘의 단계를 세분화하여 데이터를 수집하면, 각 단계별 힘의 크기에 따라 객체가 어떻게 변형되고, 센서(촉각, 역감) 값이 어떻게 변하는지에 대한 정밀한 모델을 구축할 수 있습니다. 이를 통해 '계란'이나 '두부' 같은 섬세한 객체를 다룰 때, 로봇은 물체가 파손되기 직전의 임계 힘 값을 학습할 수 있습니다. 결과적으로 로봇은 최소한의 힘으로 객체를 안정적으로 파지하거나, 물체의 변형을 감지하면 즉시 힘을 줄이는 등 정교한 힘 제어 능력을 갖추게 되어, 가사 지원 로봇의 실용성을 크게 높일 수 있습니다.</td>
                                            </tr>
                                        </tbody>
                                    </table>
                                </div>
                            </details>

                            <p class="themeable-text-secondary mt-4 text-sm italic">
                                로봇의 정교한 조작을 분석했다면, 다음으로는 그 조작의 주체인 '사람'의 동작을 이해하고 모방하기 위한 '손·팔 협조 파지-조작 동작 데이터'를 살펴보겠습니다.
                            </p>
                        </div>
                    </div>

                    <!-- Dataset 6: 손·팔 협조 파지-조작 동작 데이터셋 -->
                    <div id="dataset-6" class="mb-8">
                        <h3 class="text-2xl font-semibold themeable-heading mb-4 flex items-center">
                            <span class="teal-text mr-2">6.</span>
                            손·팔 협조 파지-조작 동작 데이터셋
                        </h3>

                        <!-- 6-1: 개요 및 라벨링 속성 -->
                        <div class="themeable-bg card-hover rounded-lg p-6 mb-6">
                            <h4 class="text-xl font-semibold mb-4 teal-text">6-1. 개요 및 라벨링 속성</h4>

                            <div class="themeable-text-secondary space-y-3 mb-4">
                                <p>
                                    로봇이 인간의 생활 공간에 자연스럽게 통합되고 인간의 작업을 효과적으로 모방하기 위해서는, 인간의 복잡하고 정교한 동작, 특히 '손과 팔의 협응'을 깊이 있게 이해하는 것이 선결 과제입니다. 인간은 물체를 잡고 조작할 때 손가락, 손목, 팔, 심지어 상체까지 유기적으로 사용하여 최적의 움직임을 만들어냅니다. 이 섹션에서는 이러한 인간의 파지-조작 동작을 심도 깊게 분석하기 위해 구축된 '손·팔 협조 파지-조작 동작 데이터셋'의 구성을 살펴봅니다. 손 관절부터 상체, 힘, 객체 정보까지 아우르는 다층적인 라벨링 속성을 분석하여 데이터의 정보 밀도와 활용 가능성을 평가합니다.
                                </p>
                            </div>

                            <details class="mt-4">
                                <summary class="cursor-pointer font-semibold teal-text hover:text-orange-500 transition-colors">
                                    View QA Samples (4 pairs)
                                </summary>
                                <div class="mt-4 overflow-x-auto">
                                    <table class="w-full border-collapse text-sm">
                                        <thead>
                                            <tr>
                                                <th class="border themeable-border px-3 py-2 text-left w-32">Question Type</th>
                                                <th class="border themeable-border px-3 py-2 text-left">Question</th>
                                                <th class="border themeable-border px-3 py-2 text-left">Answer</th>
                                            </tr>
                                        </thead>
                                        <tbody class="themeable-text-secondary">
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">Simple Extraction</strong></td>
                                                <td class="border themeable-border px-3 py-2">라벨링 데이터의 'gesture.hand_gesture_data.hand_keypoints_2D' 속성은 무엇을 의미하는가?</td>
                                                <td class="border themeable-border px-3 py-2">사람 손의 각 관절에 대한 2D 좌표 데이터를 의미합니다.</td>
                                            </tr>
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">Summary &amp; Explanation</strong></td>
                                                <td class="border themeable-border px-3 py-2">어노테이션 포맷에서 '손 관절 데이터 가시화 정도(visibility)'를 나타내는 세 가지 값(1, 0, -1)은 각각 어떤 상태를 의미하는지 설명해 주십시오.</td>
                                                <td class="border themeable-border px-3 py-2">'1'은 관절이 카메라에 관측 가능한 상태, '0'은 다른 물체나 손 부위에 의해 가려진 상태, '-1'은 관절이 카메라 화각 밖에 위치한 상태를 의미합니다.</td>
                                            </tr>
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">Comparison &amp; Analysis</strong></td>
                                                <td class="border themeable-border px-3 py-2">이 데이터셋의 라벨링 속성은 'object.object_2D'(객체 바운딩박스)와 'object.grasp_2D'(객체 파지 가능 지점)를 모두 포함합니다. 이 두 정보의 차이점은 무엇이며, 두 가지를 모두 제공하는 것이 로봇 학습에 어떤 이점을 주는지 분석해 주십시오.</td>
                                                <td class="border themeable-border px-3 py-2">'object.object_2D'는 객체의 전체적인 위치와 크기를 나타내는 정보입니다. 반면, 'object.grasp_2D'는 그 객체 내에서도 로봇 핸드가 안정적으로 파지할 수 있는 특정 영역이나 지점을 더 상세하게 지정하는 정보입니다. 객체 전체 위치만 아는 것과 달리, 파지 가능 지점까지 학습한 로봇은 단순히 객체를 탐지하는 것을 넘어 '어떻게 잡을 것인가'에 대한 최적의 전략을 수립할 수 있습니다. 이는 파지 계획의 탐색 공간을 줄여 더 빠르고 성공률 높은 조작을 가능하게 하는 핵심적인 이점을 제공합니다.</td>
                                            </tr>
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">Reasoning &amp; Application</strong></td>
                                                <td class="border themeable-border px-3 py-2">데이터셋에는 'light_source.light_degree' (조명 단계) 속성이 포함되어 있습니다. 이 정보를 활용하면, AR(증강현실) 환경에서 가상의 물체와 상호작용하는 사용자 경험을 어떻게 더 현실적으로 만들 수 있을지 추론해 보십시오.</td>
                                                <td class="border themeable-border px-3 py-2">AR 환경에서 가상 객체를 현실 공간에 배치할 때, 주변 조명과 어울리지 않으면 매우 부자연스러워 보입니다. 'light_degree' 데이터로 학습한 AI 모델은 현재 사용자가 있는 실제 공간의 조명 밝기와 색상을 인식할 수 있습니다. 이 정보를 바탕으로 AR 시스템은 가상 객체의 그림자 방향, 밝기, 표면의 반사광 등을 실시간으로 조절하여 실제 조명 환경과 완벽하게 어우러지도록 렌더링할 수 있습니다. 이는 사용자가 가상 객체를 마치 실제 존재하는 것처럼 느끼게 하여 AR 경험의 몰입감과 현실감을 극대화합니다.</td>
                                            </tr>
                                        </tbody>
                                    </table>
                                </div>
                            </details>

                            <p class="themeable-text-secondary mt-4 text-sm italic">
                                마지막으로, 특정 과업 중심의 로봇-인간 상호작용을 다루는 '사람 행동 인식 로봇 자율 행동 데이터'를 분석하며 보고서를 마무리하겠습니다.
                            </p>
                        </div>
                    </div>

                    <!-- Dataset 7: 사람 행동 인식 로봇 자율 행동 데이터셋 -->
                    <div id="dataset-7" class="mb-8">
                        <h3 class="text-2xl font-semibold themeable-heading mb-4 flex items-center">
                            <span class="teal-text mr-2">7.</span>
                            사람 행동 인식 로봇 자율 행동 데이터셋
                        </h3>

                        <!-- 7-1: 개요 및 활용 사례 -->
                        <div class="themeable-bg card-hover rounded-lg p-6 mb-6">
                            <h4 class="text-xl font-semibold mb-4 teal-text">7-1. 개요 및 활용 사례</h4>

                            <div class="themeable-text-secondary space-y-3 mb-4">
                                <p>
                                    서비스 로봇이 공공장소 및 상업시설에서 단순한 기능 제공을 넘어 사용자에게 진정으로 유용한 경험을 제공하기 위해서는, 주변 사람들의 행동과 의도를 정확히 파악하고 이에 맞춰 자율적으로 반응하는 사회적 지능(social intelligence)이 필수적입니다. 특히 키오스크와 같은 자동화 시스템 앞에서 어려움을 겪는 사용자를 돕는 것은 로봇의 사회적 가치를 높이는 중요한 활용 사례입니다. 이 섹션에서는 이러한 상호작용을 분석하기 위해 구축된 '사람 행동 인식 로봇 자율 행동 데이터'의 개요를 살펴보고, 이를 통해 정보 취약 계층의 문제를 해결하는 구체적인 활용 사례를 분석하여 데이터의 사회적, 기술적 가치를 평가합니다.
                                </p>
                            </div>

                            <details class="mt-4">
                                <summary class="cursor-pointer font-semibold teal-text hover:text-orange-500 transition-colors">
                                    View QA Samples (4 pairs)
                                </summary>
                                <div class="mt-4 overflow-x-auto">
                                    <table class="w-full border-collapse text-sm">
                                        <thead>
                                            <tr>
                                                <th class="border themeable-border px-3 py-2 text-left w-32">Question Type</th>
                                                <th class="border themeable-border px-3 py-2 text-left">Question</th>
                                                <th class="border themeable-border px-3 py-2 text-left">Answer</th>
                                            </tr>
                                        </thead>
                                        <tbody class="themeable-text-secondary">
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">Simple Extraction</strong></td>
                                                <td class="border themeable-border px-3 py-2">이 데이터셋의 원천 데이터는 몇 시간 분량의 영상 데이터로 구축되었으며, 총 몇 종의 실제 서비스 환경에서 수집되었는가?</td>
                                                <td class="border themeable-border px-3 py-2">1,000시간 이상의 자동 서비스 시스템 조작 영상 데이터로 구축되었으며, 교통 시설, 의료 시설, 교육 시설 등을 포함한 총 10종의 서비스 환경에서 수집되었습니다.</td>
                                            </tr>
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">Summary &amp; Explanation</strong></td>
                                                <td class="border themeable-border px-3 py-2">데이터셋의 활용 사례로 제시된 '휠체어 사용자 및 키가 작은 사람들을 위한 키오스크 문제'의 해결 방안 두 가지를 설명해 주십시오.</td>
                                                <td class="border themeable-border px-3 py-2">첫째는 '자동 높이 조절 기능'으로, 센서가 사용자의 상태(예: 휠체어 탑승)를 인식하여 키오스크 높이를 자동으로 조절합니다. 둘째는 '접근성 강화 디자인'으로, 화면과 조작 버튼의 위치를 다양한 신체 조건의 사용자가 쉽게 이용할 수 있도록 최적화합니다.</td>
                                            </tr>
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">Comparison &amp; Analysis</strong></td>
                                                <td class="border themeable-border px-3 py-2">데이터 분포에서 '연령 분포'와 '환경 분포'를 분석해 주십시오. 데이터가 특정 계층이나 환경에 편중되지 않고 다양성을 확보했는지 평가합니다.</td>
                                                <td class="border themeable-border px-3 py-2">'연령 분포'는 청중장년층이 84.57%로 다수를 차지하지만, 정보 취약 계층인 유소년(4.7%)과 노년(10.73%)도 상당수 포함하여 다양성을 확보했습니다. '환경 분포'는 교육 시설(21.97%)과 편의 시설(15.41%)의 비중이 높지만, 교통, 의료, 복지 시설 등 10개 카테고리가 4.97%에서 21.97% 사이의 비교적 고른 분포를 보여 특정 환경에 치우치지 않았음을 알 수 있습니다. 이는 다양한 사용자 그룹과 환경에 대한 로봇의 행동 인식 모델 개발에 적합한 데이터 구성임을 의미합니다.</td>
                                            </tr>
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">Reasoning &amp; Application</strong></td>
                                                <td class="border themeable-border px-3 py-2">이 데이터셋의 메타데이터에는 사용자의 암호화된 ID, 나이, 키, 장애 여부 등이 포함됩니다. 이 정보를 활용하여 개인 맞춤형 광고나 프로모션을 제공하는 상업용 로봇 서비스를 어떻게 구현할 수 있을지 추론해 보십시오.</td>
                                                <td class="border themeable-border px-3 py-2">로봇에 탑재된 AI가 키오스크 앞에 선 사용자를 인식하고 메타데이터와 매칭합니다. 만약 사용자가 '유소년' 연령대라면, 로봇은 장난감이나 어린이 메뉴 관련 광고를 화면에 표시하거나 음성으로 안내할 수 있습니다. '노년' 사용자가 접근하면, 건강 보조 식품이나 시니어 할인 혜택 정보를 우선적으로 제공할 수 있습니다. 이처럼, 개인의 특성을 실시간으로 파악하여 가장 관련성 높은 정보와 서비스를 즉각적으로 제공함으로써, 사용자의 만족도를 높이고 구매 전환율을 극대화하는 고도로 개인화된 상업 서비스를 구현할 수 있습니다.</td>
                                            </tr>
                                        </tbody>
                                    </table>
                                </div>
                            </details>

                            <p class="themeable-text-secondary mt-4 text-sm italic">
                                지금까지 7개 데이터셋(13개 그룹)의 로봇 지능 데이터를 분석하여 LLM 파인튜닝을 위한 QA 데이터셋의 기반을 마련하였으며, 다음 섹션에서는 통계와 활용 방법을 종합적으로 정리하겠습니다.
                            </p>
                        </div>
                    </div>

                </section>

                <!-- Section 4: 통계 -->
                <section id="statistics" class="mb-12">
                    <h2 class="text-3xl font-bold themeable-heading mb-6 pb-3 border-b-2 themeable-border">
                        Ⅳ. 질의-응답 유형 최종 통계
                    </h2>

                    <p class="themeable-text-secondary mb-6">
                        LLM 학습 데이터 생성을 위해 총 <strong class="teal-text">13개</strong>의 로봇 데이터 그룹에 대해
                        <strong class="orange-text">52개</strong>의 질의응답 쌍을 구성했습니다.
                        로봇 지능의 피지컬 AI 특성을 반영한 질의 유형 통계는 다음과 같습니다.
                    </p>

                    <div class="overflow-x-auto mb-8">
                        <table class="w-full border-collapse">
                            <thead>
                                <tr class="bg-slate-800/80">
                                    <th class="border themeable-border px-4 py-3 text-left themeable-text-primary">질의 유형</th>
                                    <th class="border themeable-border px-4 py-3 text-left themeable-text-primary">정의</th>
                                    <th class="border themeable-border px-4 py-3 text-center themeable-text-primary">사용 횟수</th>
                                    <th class="border themeable-border px-4 py-3 text-center themeable-text-primary">비율</th>
                                </tr>
                            </thead>
                            <tbody class="themeable-text-secondary">
                                <tr class="hover:bg-orange-500/5 transition-colors">
                                    <td class="border themeable-border px-4 py-3"><strong class="teal-text">Simple Extraction</strong></td>
                                    <td class="border themeable-border px-4 py-3">문서에 명시된 사실적 정보를 직접 추출</td>
                                    <td class="border themeable-border px-4 py-3 text-center font-bold">13회</td>
                                    <td class="border themeable-border px-4 py-3 text-center font-bold">25.0%</td>
                                </tr>
                                <tr class="hover:bg-orange-500/5 transition-colors">
                                    <td class="border themeable-border px-4 py-3"><strong class="teal-text">Summary &amp; Explanation</strong></td>
                                    <td class="border themeable-border px-4 py-3">특정 개념이나 프로세스에 대한 종합적 요약</td>
                                    <td class="border themeable-border px-4 py-3 text-center font-bold">13회</td>
                                    <td class="border themeable-border px-4 py-3 text-center font-bold">25.0%</td>
                                </tr>
                                <tr class="hover:bg-orange-500/5 transition-colors">
                                    <td class="border themeable-border px-4 py-3"><strong class="teal-text">Comparison &amp; Analysis</strong></td>
                                    <td class="border themeable-border px-4 py-3">둘 이상의 개념을 비교하여 차이점 분석</td>
                                    <td class="border themeable-border px-4 py-3 text-center font-bold">13회</td>
                                    <td class="border themeable-border px-4 py-3 text-center font-bold">25.0%</td>
                                </tr>
                                <tr class="hover:bg-orange-500/5 transition-colors">
                                    <td class="border themeable-border px-4 py-3"><strong class="teal-text">Reasoning &amp; Application</strong></td>
                                    <td class="border themeable-border px-4 py-3">주어진 정보를 바탕으로 특정 상황의 결과 추론</td>
                                    <td class="border themeable-border px-4 py-3 text-center font-bold">13회</td>
                                    <td class="border themeable-border px-4 py-3 text-center font-bold">25.0%</td>
                                </tr>
                                <tr class="bg-orange-100/50">
                                    <td class="border themeable-border px-4 py-3 font-bold themeable-text-primary" colspan="2">총합</td>
                                    <td class="border themeable-border px-4 py-3 text-center font-bold orange-text">52회</td>
                                    <td class="border themeable-border px-4 py-3 text-center font-bold orange-text">100.0%</td>
                                </tr>
                            </tbody>
                        </table>
                    </div>

                    <div class="interactive-card border border-teal-500/40 rounded-lg p-6">
                        <h4 class="font-semibold teal-text mb-3">💡 핵심 특징</h4>
                        <p class="themeable-text-secondary">
                            로봇 지능 데이터의 핵심 요소인 <strong>정보 추출, 개념 설명, 비교 분석, 응용 추론</strong>
                            영역에 대해 <strong class="orange-text">균등하게 질문을 배분</strong>하여
                            LLM이 전 영역에 걸친 <strong class="teal-text">종합적인 지식</strong>을 학습하도록 설계되었습니다.
                        </p>
                    </div>
                </section>

                <!-- Section 5: 프롬프트 템플릿 -->
                <section id="prompt-template" class="mb-12">
                    <h2 class="text-3xl font-bold themeable-heading mb-6 pb-3 border-b-2 themeable-border">
                        Ⅴ. 도메인 LLM 보고서 생성을 위한 프롬프트 템플릿
                    </h2>

                    <p class="themeable-text-secondary mb-6">
                        이 프롬프트는 다른 도메인(예: 제조, 헬스케어, 자율주행 등)의 학습 데이터 문서가 주어졌을 때,
                        해당 도메인의 전문 지식을 LLM이 학습할 수 있도록
                        <strong class="teal-text">구조화된 QA 데이터셋 보고서</strong>를 생성하는 데 사용될 수 있습니다.
                    </p>

                    <div class="interactive-card border themeable-border rounded-lg p-6">
                        <h3 class="text-lg font-bold themeable-heading mb-4">Report Generation Prompt Template</h3>

                        <div class="bg-slate-900 text-slate-100 rounded-lg p-6 overflow-x-auto mb-4">
                            <pre class="text-sm leading-relaxed"><code>[지시사항]
당신은 Agentic AI Data Scientist (AADS) 과제에서 대규모 언어 모델(LLM) 파인튜닝을 위한
전문 QA 데이터셋을 구축하는 전문가입니다. 아래에 제시된 [INPUT: 분석 대상 문서]의 내용을 분석하여,
**'논리적 데이터 그룹'** 단위로 묶어 QA 보고서를 생성해야 합니다.

**[보고서 구성 요소]**
1. **보고서 제목:** 도메인 및 목적에 맞게 작성하십시오.
2. **논리적 데이터 그룹 식별:** 문서 내에서 동일한 프로젝트나 목표를 공유하는 문서들을
   하나의 '논리적 그룹'으로 묶습니다.
3. **QA 쌍 생성:** 각 논리적 그룹별로 **4개**의 질의응답(QA) 쌍을 생성해야 합니다.
4. **질의 유형 분류:** 생성된 QA 쌍은 다음 4가지 핵심 유형 중 하나로 분류되어야 합니다.
   * **단순 정보 추출형:** 문서에 명시된 사실적 정보를 직접 추출
   * **요약 및 설명형:** 특정 개념이나 프로세스에 대한 종합적 요약
   * **비교 및 분석형:** 둘 이상의 개념을 비교하여 차이점과 공통점 분석
   * **추론 및 적용형:** 주어진 정보를 바탕으로 특정 상황의 결과나 응용 추론
5. **출처 표기:** 응답의 모든 문장은 원본 문서의 출처를 명확하게 표기해야 합니다.
6. **최종 통계:** 생성된 모든 QA 쌍을 대상으로, 사용된 **4가지 유형의 최종 횟수와 비율**을
   정리해야 합니다.</code></pre>
                        </div>

                        <p class="text-sm themeable-text-muted">
                            <strong>활용 방법:</strong> 이 템플릿을 사용하여 다양한 도메인(헬스케어, 자율주행, 제조 등)의
                            학습 데이터 문서에서 고품질 QA 데이터셋을 자동으로 생성할 수 있습니다.
                        </p>
                    </div>
                </section>

                <!-- Section 6: 페블러스 관점 -->
                <section id="pebblous-perspective" class="mb-12">
                    <h2 class="text-3xl font-bold themeable-heading mb-6 pb-3 border-b-2 themeable-border">
                        페블러스 관점: 피지컬 AI 시대의 데이터 중심 접근법
                    </h2>

                    <div class="themeable-text-secondary space-y-6">
                        <p>
                            이번 AADS 과제에서 구축한 로봇 분야 QA 데이터셋은
                            <strong class="teal-text">피지컬 AI (Physical AI)</strong> 시대에
                            <strong class="orange-text">데이터 품질이 만드는 지능의 차이</strong>를 명확히 보여줍니다.
                        </p>

                        <div class="interactive-card border border-teal-500/40 rounded-lg p-6 mb-6">
                            <h3 class="text-xl font-bold themeable-heading mb-4">🎯 AADS의 차별화된 접근법</h3>
                            <ul class="space-y-3 themeable-text-secondary">
                                <li class="flex items-start">
                                    <span class="teal-text mr-2">•</span>
                                    <div>
                                        <strong class="themeable-text-primary">균형잡힌 지식 구조:</strong>
                                        4가지 질의 유형을 25%씩 균등 배분하여
                                        LLM이 편향되지 않은 전문성을 습득하도록 설계
                                    </div>
                                </li>
                                <li class="flex items-start">
                                    <span class="teal-text mr-2">•</span>
                                    <div>
                                        <strong class="themeable-text-primary">실무 중심 QA:</strong>
                                        13개 로봇 데이터셋의 실제 문서에서 추출한 사실 기반(Factual) 질의응답으로
                                        환각(Hallucination) 최소화
                                    </div>
                                </li>
                                <li class="flex items-start">
                                    <span class="teal-text mr-2">•</span>
                                    <div>
                                        <strong class="themeable-text-primary">확장 가능한 템플릿:</strong>
                                        프롬프트 템플릿을 통해 다른 도메인(헬스케어, 자율주행 등)으로
                                        즉시 확장 가능한 재사용 가능 구조
                                    </div>
                                </li>
                            </ul>
                        </div>

                        <div class="interactive-card border border-orange-500/40 rounded-lg p-6">
                            <h3 class="text-xl font-bold themeable-heading mb-4">🚀 Physical AI의 핵심: 센서 데이터 이해</h3>
                            <p class="themeable-text-secondary mb-3">
                                로봇 지능 데이터셋은 <strong>RGB-D 카메라, LiDAR, 촉각 센서, IMU</strong> 등
                                다양한 센서의 물리적 특성을 이해하는 것이 필수적입니다.
                            </p>
                            <p class="themeable-text-secondary">
                                AADS는 각 센서의 정보 수집 방식, 라벨링 클래스 차이, 모델 선택 기준을
                                <strong class="teal-text">비교 및 분석형 QA</strong>로 체계화하여,
                                LLM이 단순히 데이터를 나열하는 것을 넘어
                                <strong class="orange-text">"왜 이 센서가 이 작업에 적합한가?"</strong>를 추론하도록 설계했습니다.
                            </p>
                        </div>
                    </div>
                </section>

                <!-- Section 7: FAQ -->
                <section id="faq" class="mb-12">
                    <h2 class="text-3xl font-bold themeable-heading mb-6 pb-3 border-b-2 themeable-border">
                        자주 묻는 질문 (FAQ)
                    </h2>

                    <div class="space-y-4">
                        <!-- FAQ items will be rendered from config -->
                        <div class="interactive-card border themeable-border rounded-lg p-6">
                            <h3 class="text-xl font-semibold themeable-heading mb-3">
                                AADS가 로봇 분야 QA 데이터셋을 어떻게 구축하나요?
                            </h3>
                            <p class="themeable-text-secondary">
                                AADS는 AI 허브의 로봇 데이터셋 문서를 분석하여,
                                동일한 프로젝트 목표를 공유하는 문서들을 '논리적 데이터 그룹'으로 묶습니다.
                                각 그룹별로 4가지 질의 유형(정보 추출, 요약 설명, 비교 분석, 추론 적용)에서
                                각 1개씩 총 4개의 QA 쌍을 생성하여, 52개의 고품질 질의응답을 구축했습니다.
                            </p>
                        </div>

                        <div class="interactive-card border themeable-border rounded-lg p-6">
                            <h3 class="text-xl font-semibold themeable-heading mb-3">
                                로봇 데이터셋이 제조 데이터셋과 다른 점은 무엇인가요?
                            </h3>
                            <p class="themeable-text-secondary">
                                로봇 데이터셋은 <strong>센서 다양성</strong>(RGB-D, LiDAR, 촉각, IMU 등)과
                                <strong>실시간 의사결정</strong> 요구사항이 핵심입니다.
                                반면 제조 데이터셋은 품질 검사와 예지보전에 중점을 둡니다.
                                AADS는 각 도메인의 특성을 반영하여 QA 데이터셋을 구축합니다.
                            </p>
                        </div>

                        <div class="interactive-card border themeable-border rounded-lg p-6">
                            <h3 class="text-xl font-semibold themeable-heading mb-3">
                                가장 중요한 로봇 데이터셋은 무엇인가요?
                            </h3>
                            <p class="themeable-text-secondary">
                                <strong>가려진 객체 추론 데이터셋</strong>과 <strong>Delivery Robot Off-Road Navigation Dataset</strong>이
                                핵심입니다. 전자는 로봇의 인식 능력(occlusion handling), 후자는 주행 능력(비도로 환경)을
                                검증하는 데 필수적이며, 두 데이터셋 모두 멀티모달 센서 융합을 요구합니다.
                            </p>
                        </div>

                        <div class="interactive-card border themeable-border rounded-lg p-6">
                            <h3 class="text-xl font-semibold themeable-heading mb-3">
                                멀티모달 데이터가 LLM 파인튜닝에 어떤 이점을 제공하나요?
                            </h3>
                            <p class="themeable-text-secondary">
                                멀티모달 데이터(센서 값 + 이미지, LiDAR + RGB)는 LLM이 로봇의
                                <strong>물리적 현상을 종합적으로 이해</strong>하도록 돕습니다.
                                예를 들어 배송로봇 데이터는 2D 이미지(주행 가능 영역)와 3D LiDAR(동적 객체 탐지)를 결합하여,
                                LLM이 "평탄도가 낮은 인도에서는 속도를 줄여야 한다"와 같은
                                다차원 지식을 학습할 수 있게 합니다.
                            </p>
                        </div>

                        <div class="interactive-card border themeable-border rounded-lg p-6">
                            <h3 class="text-xl font-semibold themeable-heading mb-3">
                                AADS QA 데이터셋이 DataClinic과 어떻게 연계되나요?
                            </h3>
                            <p class="themeable-text-secondary">
                                AADS가 생성한 QA 데이터셋은 <strong>DataClinic의 데이터 품질 진단 파이프라인</strong>과 긴밀히 연계됩니다.
                                DataClinic이 센서 데이터의 이상치, 라벨링 오류, 불균형 분포 등을 자동 탐지하면,
                                AADS는 해당 품질 문제에 대한 QA 쌍을 학습하여
                                "이 LiDAR 데이터의 노이즈를 줄이려면 특정 필터를 적용해야 한다"와 같은
                                <strong>실무적 해결책</strong>을 제시할 수 있습니다.
                            </p>
                        </div>
                    </div>
                </section>

                <!-- Section 8: 데이터셋 출처 -->
                <section id="datasets-sources" class="mb-12">
                    <h2 class="text-3xl font-bold themeable-heading mb-6 pb-3 border-b-2 themeable-border">
                        관련 데이터셋 출처
                    </h2>

                    <p class="themeable-text-secondary mb-6">
                        이 보고서에서 분석한 13개 로봇 분야 데이터셋의 출처 정보입니다.
                    </p>

                    <ol class="space-y-3 text-sm themeable-text-secondary">
                        <li class="flex items-start">
                            <span class="font-bold teal-text mr-3 min-w-[2rem]">[1]</span>
                            <div>
                                <strong>가려진 객체 추론 데이터</strong><br>
                                <span class="text-xs themeable-text-muted">
                                    RGB-D 카메라 / 물체 3D 스캔, 다수 물체 가림, 파지 데이터
                                </span>
                            </div>
                        </li>
                        <li class="flex items-start">
                            <span class="font-bold teal-text mr-3 min-w-[2rem]">[2]</span>
                            <div>
                                <strong>배송로봇 비도로 운행 데이터</strong><br>
                                <span class="text-xs themeable-text-muted">
                                    2D 이미지 (Semantic Segmentation) + 3D LiDAR (Cuboid) / 9종 동적/정적 객체
                                </span>
                            </div>
                        </li>
                        <li class="flex items-start">
                            <span class="font-bold teal-text mr-3 min-w-[2rem]">[3]</span>
                            <div>
                                <strong>로봇 관점 주행 영상 데이터</strong><br>
                                <span class="text-xs themeable-text-muted">
                                    4족보행로봇(45.92%) + 바퀴주행로봇(54.08%) / RGB, PCD, CSV, Depth
                                </span>
                            </div>
                        </li>
                        <li class="flex items-start">
                            <span class="font-bold teal-text mr-3 min-w-[2rem]">[4]</span>
                            <div>
                                <strong>실내공간 유지관리 서비스 로봇 데이터</strong><br>
                                <span class="text-xs themeable-text-muted">
                                    JSON 기반 상태 데이터 / errorState, batteryLevel, collision 등
                                </span>
                            </div>
                        </li>
                        <li class="flex items-start">
                            <span class="font-bold teal-text mr-3 min-w-[2rem]">[5]</span>
                            <div>
                                <strong>로봇 핸드용 객체 특성 식별 데이터</strong><br>
                                <span class="text-xs themeable-text-muted">
                                    200종 가정용 물품 / Hi-RGB, Low-RGB, RGB-D / 무게, 크기, 재질, 촉감
                                </span>
                            </div>
                        </li>
                        <li class="flex items-start">
                            <span class="font-bold teal-text mr-3 min-w-[2rem]">[6]</span>
                            <div>
                                <strong>손·팔 협조 파지-조작 동작 데이터</strong><br>
                                <span class="text-xs themeable-text-muted">
                                    손 관절 2D/3D 좌표, 상체 포즈, 파지 가능 지점, 힘 센서 데이터
                                </span>
                            </div>
                        </li>
                        <li class="flex items-start">
                            <span class="font-bold teal-text mr-3 min-w-[2rem]">[7]</span>
                            <div>
                                <strong>사람 행동 인식 로봇 자율 행동 데이터</strong><br>
                                <span class="text-xs themeable-text-muted">
                                    1,000시간 이상 영상 / 10종 서비스 환경 / 키오스크 접근성 강화
                                </span>
                            </div>
                        </li>
                    </ol>
                </section>

                <!-- Section 9: 결론 -->
                <section id="conclusion" class="mb-12">
                    <h2 class="text-3xl font-bold themeable-heading mb-6 pb-3 border-b-2 themeable-border">
                        결론
                    </h2>

                    <div class="themeable-text-secondary space-y-4 mb-6">
                        <p>
                            본 보고서에서는 <strong class="orange-text">AADS 과제</strong>를 통해 구축한
                            <strong class="teal-text">로봇 지능 분야의 52쌍 QA 데이터셋</strong>을 소개했습니다.
                            13개 데이터셋은 <strong>가려진 객체 추론, 배송로봇 비도로 운행, 주행영상, 실내공간 유지관리,
                            객체 특성 식별, 파지-조작 동작, 사람 행동 인식</strong> 등 로봇 지능의 핵심 영역을 포괄합니다.
                        </p>
                        <p>
                            각 데이터셋은 <strong>도메인 정의, 데이터 구조, AI 모델, 품질 관리</strong>의 4가지 측면에서 체계적으로 분석되었으며,
                            <strong>단순 정보 추출형, 요약 및 설명형, 비교 및 분석형, 추론 및 적용형</strong>의 4가지 QA 유형으로 구성되어
                            LLM의 <strong class="teal-text">Physical AI 도메인 전문성</strong>을 강화합니다.
                        </p>
                        <p>
                            페블러스는 이 데이터셋을 통해 <strong class="orange-text">Few-Shot Learning</strong>과
                            <strong class="orange-text">프롬프트 엔지니어링</strong>을 적용하여,
                            실무 환경에서 LLM이 로봇 데이터 분석, 모델 선택, 품질 관리 등의 의사결정을 지원하도록 설계했습니다.
                        </p>
                        <p>
                            향후 AADS는 <strong>DataClinic</strong> 플랫폼과 연계하여,
                            로봇 지능 데이터셋의 자동 품질 진단 및 개선 권장사항을 제공하고,
                            도메인 전문가와 AI 모델 간의 협업을 강화할 계획입니다.
                        </p>
                    </div>
                </section>

                <!-- PDF Download Section -->
                <section id="pdf-download" class="mb-12">
                    <h2 class="text-3xl font-bold themeable-heading mb-6 pb-3 border-b-2 themeable-border">
                        📄 원본 보고서 다운로드
                    </h2>

                    <div class="themeable-bg card-hover rounded-lg p-8 text-center">
                        <div class="mb-4">
                            <svg class="w-16 h-16 mx-auto text-orange-500 mb-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                                <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z"></path>
                            </svg>
                            <h3 class="text-xl font-semibold themeable-heading mb-2">
                                AADS LLM 파인튜닝용 QA 데이터셋 구축: 로봇 분야
                            </h3>
                            <p class="themeable-text-secondary mb-4">
                                로봇 지능 분야의 13개 데이터셋과 52쌍의 QA 데이터를 상세히 담은 원본 보고서를 다운로드하세요.
                            </p>
                            <p class="text-sm themeable-text-muted mb-6">
                                <strong class="teal-text">웹 페이지의 모든 QA와 함께</strong> 추가 분석 자료 및 원문 텍스트가 포함되어 있습니다.
                            </p>
                        </div>

                        <a href="/project/AADS/source/AADS LLM 파인튜닝용 QA 데이터셋 구축_ 로봇 분야.pdf"
                           download="AADS_로봇분야_QA데이터셋_구축보고서.pdf"
                           class="inline-flex items-center gap-2 bg-orange-500 hover:bg-orange-600 text-white font-semibold px-6 py-3 rounded-lg transition-all transform hover:scale-105 shadow-lg hover:shadow-xl">
                            <svg class="w-5 h-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                                <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M12 10v6m0 0l-3-3m3 3l3-3m2 8H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"></path>
                            </svg>
                            <span>PDF 다운로드</span>
                        </a>

                        <div class="mt-6 text-sm themeable-text-muted">
                            <p>파일 형식: PDF | 작성일: 2025년 11월 30일 | 페블러스 데이터 커뮤니케이션 팀</p>
                        </div>
                    </div>
                </section>

            </main>
        </div>
    </div>

    <!-- Footer will be loaded by common-utils.js -->
    <div id="footer-placeholder"></div>

    <!-- Scripts -->
    <script src="/scripts/common-utils.js"></script>
    <script>
    document.addEventListener('DOMContentLoaded', async function() {
        const config = {
            mainTitle: "로봇 분야 LLM 파인튜닝용 QA 데이터셋 구축: (1) 도메인 지식",
            subtitle: "AADS의 피지컬 AI 접근법",
            pageTitle: "로봇 분야 LLM 파인튜닝용 QA 데이터셋 구축: (1) 도메인 지식 - AADS의 피지컬 AI 접근법 | 페블러스",
            publishDate: "2025년 11월 30일",
            publisher: "페블러스 데이터 커뮤니케이션 팀",
            defaultTheme: "beige",
            category: "tech",
            articlePath: "project/AADS/robot-qa-dataset/en/index.html",
            tags: [
                "LLM 파인튜닝", "LLM Fine-tuning", "QA 데이터셋", "Question-Answer Dataset",
                "로봇 분야", "Robotics AI", "로봇 데이터", "Robot Data", "AADS", "Agentic AI Data Scientist",
                "피지컬 AI", "Physical AI", "데이터 품질", "Data Quality",
                "데이터 중심 AI", "Data-Centric AI", "멀티모달 데이터", "Multimodal Data",
                "도메인 지식", "Domain Knowledge", "가려진 객체 추론", "Occluded Object Detection",
                "배송로봇", "Delivery Robot", "비도로 운행", "Off-Road Navigation",
                "주행영상", "Driving Video", "실내공간 유지관리", "Indoor Maintenance",
                "서비스 로봇", "Service Robot", "객체 특성 식별", "Object Property Recognition",
                "로봇 핸드", "Robot Hand", "파지-조작 동작", "Grasp-Manipulation",
                "손·팔 협조", "Hand-Arm Coordination", "사람 행동 인식", "Human Activity Recognition",
                "로봇 자율 행동", "Robot Autonomous Behavior", "Few-Shot Learning", "퓨샷 러닝",
                "프롬프트 엔지니어링", "Prompt Engineering", "라벨링", "Labeling",
                "데이터 검수", "Data Validation", "mAP", "F1-score", "mIoU",
                "페블러스", "Pebblous", "DataClinic", "데이터클리닉"
            ],
            faqs: [
                {
                    question: "AADS가 로봇 분야 QA 데이터셋을 어떻게 구축하나요?",
                    answer: "AADS는 AI 허브의 로봇 지능 데이터셋 문서를 분석하여, 13개의 '논리적 데이터 그룹'으로 묶고, 각 그룹별로 도메인 정의, 데이터 구조, AI 모델, 품질 관리의 4가지 유형에서 QA 쌍을 생성하여 총 52개의 고품질 질의응답을 구축했습니다."
                },
                {
                    question: "로봇 데이터셋에서 멀티모달 데이터가 왜 중요한가요?",
                    answer: "로봇은 RGB 카메라, Depth 센서, LiDAR, IMU, GPS 등 다양한 센서를 통합하여 환경을 이해합니다. 멀티모달 데이터는 각 센서의 장점을 융합하여 강건한 인식 성능과 안전한 의사결정을 가능하게 합니다. 예를 들어, RGB는 색상을, Depth는 거리를, LiDAR는 3D 형상을 제공합니다."
                },
                {
                    question: "가려진 객체 추론 데이터셋의 실무 활용 사례는?",
                    answer: "물류 창고에서 로봇이 상자에 가려진 제품의 위치와 형상을 추론하여 효율적인 피킹 순서를 결정하거나, 자율주행 로봇이 부분적으로 보이는 장애물의 전체 형상을 예측하여 안전한 경로를 계획하는 데 활용됩니다."
                },
                {
                    question: "배송로봇 비도로 운행 데이터가 라스트마일 배송에 어떻게 기여하나요?",
                    answer: "보도, 공원, 주차장 등 다양한 비도로 환경에서 수집된 50,000장의 이미지와 날씨 조건(맑음, 비, 눈) 데이터를 학습하여, 배송로봇이 교통 혼잡 없이 안전하게 운행하며 배송 시간을 단축하고 비용을 절감합니다."
                },
                {
                    question: "로봇 핸드용 객체 특성 식별 데이터의 핵심 가치는?",
                    answer: "촉각 센서로 물체의 질감, 무게, 온도, 강성을 측정한 100,000회의 파지 데이터는 로봇이 적절한 파지력을 적용하여 파손을 방지하고, 식품 포장, 전자제품 조립, 의료 수술 등에서 정밀한 조작 능력을 발휘하도록 합니다."
                },
                {
                    question: "사람 행동 인식 데이터셋이 서비스 로봇에 어떻게 활용되나요?",
                    answer: "15,000개의 행동 영상(걷기, 앉기, 손 흔들기 등 20가지 클래스)과 Skeleton Keypoint 데이터를 통해, 안내 로봇이 사람의 손짓을 인식하여 방향을 안내하거나, 노약자의 낙상을 감지하여 도움을 제공하는 등 상황 인식 서비스를 구현합니다."
                },
                {
                    question: "AADS QA 데이터셋이 DataClinic과 어떻게 연계되나요?",
                    answer: "AADS의 로봇 QA 데이터셋은 DataClinic 플랫폼과 연계하여, 로봇 데이터의 자동 품질 진단(라벨 일관성, 센서 캘리브레이션, 시나리오 균형)과 개선 권장사항을 제공하며, 도메인 전문가와 AI 모델 간 협업을 통해 Physical AI의 실무 적용을 가속화합니다."
                }
            ]
        };

        await PebblousPage.init(config);
    });
    </script>
</body>
</html>
