<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="author" content="Pebblous Data Communication Team">
    <meta name="language" content="English">
    <meta name="copyright" content="¬© 2025 Pebblous. All rights reserved.">
    <meta name="rating" content="general">
    <meta name="revisit-after" content="7 days">
    <meta name="distribution" content="global">
    <meta name="audience" content="AI Researchers, Data Scientists, Robotics Engineers, LLM Developers, Physical AI Researchers">
    <meta name="topic" content="LLM Fine-tuning, QA Dataset, Robotics AI, Physical AI, Data Quality">
    <meta http-equiv="content-language" content="en">

    <!-- Google Tag Manager -->
    <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
    new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
    j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
    'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','GTM-57L9F58B');</script>
    <!-- End Google Tag Manager -->

    <!-- Favicon -->
    <link rel="icon" href="/image/favicon.ico" sizes="any">
    <link rel="icon" href="/image/Pebblous_BM_Orange_RGB.png" type="image/png">
    <link rel="apple-touch-icon" href="/image/Pebblous_BM_Orange_RGB.png">

    <!-- SEO Meta Tags -->
    <title id="page-title">Building QA Datasets for Robotics LLM Fine-Tuning: (2) Data Quality - AADS Physical AI Approach | Pebblous</title>
    <meta id="meta-description" name="description" content="Pebblous AADS defined 13 robotics dataset groups and applied 4 query types (Domain Definition/Purpose, Data Structure/Composition, AI Model/Task, Quality/Process Management) to build 52 QA pairs. A systematic data quality-centric approach.">
    <meta id="meta-keywords" name="keywords" content="LLM Fine-tuning, QA Dataset, Question-Answer Dataset, Robotics AI, Robot Data, AADS, Agentic AI Data Scientist, Physical AI, Data Quality, Query Types, Domain Definition, Data Structure, AI Models, Quality Management, Data-Centric AI, Multimodal Data, 3D Scanning, 6D Pose Estimation, Robot Grasping, SLAM, Labeling, Data Validation, Pebblous, DataClinic">
    <meta name="robots" content="index, follow">

    <link id="hreflang-ko" rel="alternate" hreflang="ko" href="https://blog.pebblous.ai/project/AADS/robot-qa-dataset-2/ko/">
    <link id="hreflang-en" rel="alternate" hreflang="en" href="https://blog.pebblous.ai/project/AADS/robot-qa-dataset-2/en/">
    <link id="hreflang-default" rel="alternate" hreflang="x-default" href="https://blog.pebblous.ai/project/AADS/robot-qa-dataset-2/en/">

    <link id="canonical-url" rel="canonical" href="https://blog.pebblous.ai/project/AADS/robot-qa-dataset-2/en/">

    <meta id="og-url" property="og:url" content="https://blog.pebblous.ai/project/AADS/robot-qa-dataset-2/en/">
    <meta id="og-title" property="og:title" content="Building QA Datasets for Robotics LLM Fine-Tuning: (2) Data Quality - AADS Physical AI Approach | Pebblous">
    <meta id="og-description" property="og:description" content="52 QA pairs systematically built across 13 robotics dataset groups using 4 query types (Domain Definition/Purpose, Data Structure/Composition, AI Model/Task, Quality/Process Management). A data quality-centric Physical AI approach.">
    <meta id="og-image" property="og:image" content="https://blog.pebblous.ai/image/Pebblous_BM_Orange_RGB.png">
    <meta property="og:image:width" content="1200">
    <meta property="og:image:height" content="630">
    <meta property="og:image:alt" content="AADS Robotics LLM Fine-Tuning QA Dataset - Pebblous">
    <meta property="og:type" content="article">
    <meta property="og:site_name" content="Pebblous Blog">
    <meta property="og:locale" content="en_US">
    <meta property="article:published_time" content="2025-11-30T09:00:00+09:00">
    <meta property="article:modified_time" content="2025-11-30T09:00:00+09:00">
    <meta property="article:author" content="Pebblous Data Communication Team">
    <meta property="article:section" content="Technology">
    <meta property="article:tag" content="LLM Fine-tuning">
    <meta property="article:tag" content="Robotics AI">
    <meta property="article:tag" content="Physical AI">
    <meta property="article:tag" content="AADS">

    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:site" content="@Pebblous">
    <meta name="twitter:creator" content="@pebblous">
    <meta name="twitter:title" content="Building QA Datasets for Robotics LLM Fine-Tuning: (2) Data Quality">
    <meta name="twitter:description" content="52 QA pairs systematically built across 13 robotics dataset groups using 4 query types. A data quality-centric Physical AI approach.">
    <meta name="twitter:image" content="https://blog.pebblous.ai/image/Pebblous_BM_Orange_RGB.png">
    <meta name="twitter:image:alt" content="AADS Robotics LLM Fine-Tuning QA Dataset - Data Quality">
    <meta name="twitter:label1" content="Reading time">
    <meta name="twitter:data1" content="20 min">
    <meta name="twitter:label2" content="Difficulty">
    <meta name="twitter:data2" content="Intermediate">

    <!-- JSON-LD Structured Data for SEO -->
    <script type="application/ld+json">
    {
        "@context": "https://schema.org",
        "@type": "TechArticle",
        "headline": "Building QA Datasets for Robotics LLM Fine-Tuning: (2) Data Quality - AADS Physical AI Approach",
        "alternativeHeadline": "A data quality-centric approach systematically applying 4 query types across 13 robotics dataset groups",
        "description": "Pebblous AADS defined 13 robotics dataset groups and applied 4 query types (Domain Definition/Purpose, Data Structure/Composition, AI Model/Task, Quality/Process Management) to build 52 QA pairs. A systematic data quality-centric approach.",
        "image": {
            "@type": "ImageObject",
            "url": "https://blog.pebblous.ai/image/Pebblous_BM_Orange_RGB.png",
            "width": 1200,
            "height": 630
        },
        "author": {
            "@type": "Organization",
            "name": "Pebblous",
            "url": "https://www.pebblous.ai",
            "logo": {
                "@type": "ImageObject",
                "url": "https://www.pebblous.ai/image/Pebblous_BM_Orange_RGB.png"
            },
            "description": "Pebblous is a deep-tech company providing AI-Ready Data solutions."
        },
        "publisher": {
            "@type": "Organization",
            "name": "Pebblous",
            "url": "https://www.pebblous.ai",
            "logo": {
                "@type": "ImageObject",
                "url": "https://www.pebblous.ai/image/Pebblous_BM_Orange_RGB.png",
                "width": 600,
                "height": 60
            },
            "sameAs": [
                "https://www.linkedin.com/company/pebblous",
                "https://github.com/pebblous"
            ]
        },
        "datePublished": "2025-11-30T09:00:00+09:00",
        "dateModified": "2025-11-30T09:00:00+09:00",
        "mainEntityOfPage": {
            "@type": "WebPage",
            "@id": "https://blog.pebblous.ai/project/AADS/robot-qa-dataset-2/en/"
        },
        "keywords": "LLM Fine-tuning, QA Dataset, Robotics AI, Robot Data, AADS, Physical AI, Data Quality, Query Types, Domain Definition, Data Structure, AI Models, Quality Management, 3D Scanning, 6D Pose Estimation, SLAM, Robot Grasping, Multimodal Data",
        "articleSection": "Technology",
        "inLanguage": "en-US",
        "isAccessibleForFree": true,
        "proficiencyLevel": "Intermediate"
    }
    </script>

    <!-- FAQ Schema is dynamically generated by common-utils.js from config.faqs -->

    <!-- Stylesheets -->
    <link rel="stylesheet" href="/styles/common-styles.css?v=20260107">
    <link rel="stylesheet" href="/styles/tailwind-build.css">

    <!-- Fonts -->
    <link rel="stylesheet" as="style" crossorigin
          href="https://cdn.jsdelivr.net/gh/orioncactus/pretendard@v1.3.9/dist/web/variable/pretendardvariable.min.css">

    <style>
        /* Theme Variables - Light as default */
        :root {
            --bg-primary: #F9FAFB;
            --bg-secondary: #F3F4F6;
            --bg-card: rgba(255, 255, 255, 0.95);
            --text-primary: #111827;
            --text-secondary: #4B5563;
            --text-muted: #6B7280;
            --heading-color: #111827;
            --border-color: #E5E7EB;
            --accent-color: #F86825;
            --teal-color: #0d9488;
        }

        [data-theme="dark"] {
            --bg-primary: #020617;
            --bg-secondary: #0f172a;
            --bg-card: rgba(30, 41, 59, 0.95);
            --text-primary: #F1F5F9;
            --text-secondary: #CBD5E1;
            --text-muted: #94A3B8;
            --heading-color: #F1F5F9;
            --border-color: #334155;
            --accent-color: #F86825;
            --teal-color: #14b8a6;
        }

        body {
            background-color: var(--bg-primary);
            color: var(--text-primary);
            font-family: 'Pretendard Variable', -apple-system, BlinkMacSystemFont, system-ui, Roboto, sans-serif;
            transition: background-color 0.3s ease, color 0.3s ease;
            line-height: 1.7;
        }

        .themeable-bg {
            background-color: var(--bg-card);
            transition: background-color 0.3s ease;
        }

        .themeable-text {
            color: var(--text-primary);
        }

        .themeable-text-secondary {
            color: var(--text-secondary);
        }

        .themeable-text-muted {
            color: var(--text-muted);
        }

        .themeable-heading {
            color: var(--heading-color);
        }

        .themeable-border {
            border-color: var(--border-color);
        }

        .orange-text {
            color: var(--accent-color);
        }

        .themeable-toc-border {
            border-color: var(--teal-color);
        }

        .teal-text {
            color: var(--teal-color);
        }

        .card-hover {
            transition: all 0.3s ease;
            border: 1px solid var(--border-color);
        }

        .card-hover:hover {
            border-color: var(--teal-color);
            box-shadow: 0 4px 12px rgba(20, 184, 166, 0.15);
        }

        /* Share buttons styling */
        .share-container {
            display: flex;
            align-items: center;
            justify-content: center;
            gap: 0.75rem;
            flex-wrap: wrap;
        }

        .share-label {
            font-size: 0.875rem;
            color: var(--text-secondary);
            font-weight: 500;
        }

        .share-btn {
            display: inline-flex;
            align-items: center;
            gap: 0.375rem;
            padding: 0.5rem 0.875rem;
            border-radius: 0.5rem;
            font-size: 0.875rem;
            font-weight: 500;
            transition: all 0.2s ease;
            border: 1px solid var(--border-color);
            background-color: var(--bg-card);
            color: var(--text-secondary);
            cursor: pointer;
        }

        .share-btn:hover {
            background-color: var(--accent-color);
            color: white;
            border-color: var(--accent-color);
            transform: translateY(-2px);
            box-shadow: 0 4px 8px rgba(248, 104, 37, 0.2);
        }

        .share-btn svg {
            width: 1rem;
            height: 1rem;
        }

        .share-btn.copied {
            background-color: var(--teal-color);
            color: white;
            border-color: var(--teal-color);
        }

        /* Table styling */
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 1.5rem 0;
            font-size: 0.9rem;
        }

        thead {
            background-color: rgba(248, 104, 37, 0.1);
        }

        th {
            padding: 1rem;
            text-align: left;
            font-weight: 600;
            color: var(--heading-color);
            border-bottom: 2px solid var(--border-color);
        }

        td {
            padding: 1rem;
            border-bottom: 1px solid var(--border-color);
            color: var(--text-secondary);
        }

        tbody tr:hover {
            background-color: rgba(248, 104, 37, 0.05);
        }

        /* Details/Summary styling */
        details {
            margin: 1rem 0;
            padding: 1rem;
            border: 1px solid var(--border-color);
            border-radius: 0.5rem;
            background-color: var(--bg-card);
        }

        details summary {
            cursor: pointer;
            font-weight: 600;
            color: var(--teal-color);
            padding: 0.5rem;
            user-select: none;
        }

        details summary:hover {
            color: var(--accent-color);
        }

        details[open] summary {
            margin-bottom: 1rem;
            padding-bottom: 0.5rem;
            border-bottom: 1px solid var(--border-color);
        }

        /* Code styling */
        code {
            background-color: rgba(248, 104, 37, 0.1);
            padding: 0.2rem 0.4rem;
            border-radius: 0.25rem;
            font-family: 'Courier New', monospace;
            font-size: 0.9em;
            color: var(--accent-color);
        }

        /* List styling */
        ul, ol {
            margin-left: 1.5rem;
            margin-bottom: 1.5rem;
        }

        li {
            margin-bottom: 0.75rem;
            line-height: 1.7;
        }

        /* Strong text */
        strong {
            font-weight: 600;
            color: var(--heading-color);
        }
    </style>
</head>
<body>
    <!-- Google Tag Manager (noscript) -->
    <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-57L9F58B"
    height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
    <!-- End Google Tag Manager (noscript) -->

    <!-- Header will be loaded by common-utils.js -->
    <div id="header-placeholder"></div>

    <!-- Main Content -->
    <div class="container mx-auto px-4 sm:px-6 lg:px-8 py-8 md:py-12 max-w-[1400px]">
        <div class="lg:flex lg:gap-8 lg:justify-center lg:items-start">

            <!-- TOC Sidebar -->
            <nav class="hidden lg:block lg:w-[240px] lg:shrink-0 sticky top-20 self-start">
                <h3 class="font-bold themeable-heading mb-4 text-lg">Contents</h3>
                <ul id="toc-links" class="space-y-3 text-sm border-l-2 themeable-toc-border pl-4">
                    <li><a href="#intro" class="toc-link themeable-text-secondary hover:text-teal-500 transition-colors">Introduction</a></li>
                    <li><a href="#overview" class="toc-link themeable-text-secondary hover:text-teal-500 transition-colors">QA Dataset Overview</a></li>
                    <li><a href="#robot-datasets" class="toc-link themeable-text-secondary hover:text-teal-500 transition-colors">Robot Intelligence Datasets</a>
                        <ul class="ml-4 mt-2 space-y-2">
                            <li><a href="#dataset-1" class="toc-link themeable-text-secondary hover:text-teal-500 transition-colors text-xs">1. Occluded Object Reasoning</a></li>
                            <li><a href="#dataset-2" class="toc-link themeable-text-secondary hover:text-teal-500 transition-colors text-xs">2. Delivery Robot Off-Road</a></li>
                            <li><a href="#dataset-3" class="toc-link themeable-text-secondary hover:text-teal-500 transition-colors text-xs">3. Robot-View Driving Video</a></li>
                            <li><a href="#dataset-4" class="toc-link themeable-text-secondary hover:text-teal-500 transition-colors text-xs">4. Indoor Space Maintenance</a></li>
                            <li><a href="#dataset-5" class="toc-link themeable-text-secondary hover:text-teal-500 transition-colors text-xs">5. Robot Hand Object Properties</a></li>
                            <li><a href="#dataset-6" class="toc-link themeable-text-secondary hover:text-teal-500 transition-colors text-xs">6. Hand-Arm Grasp-Manipulation</a></li>
                            <li><a href="#dataset-7" class="toc-link themeable-text-secondary hover:text-teal-500 transition-colors text-xs">7. Human Activity Recognition</a></li>
                        </ul>
                    </li>
                    <li><a href="#statistics" class="toc-link themeable-text-secondary hover:text-teal-500 transition-colors">QA Type Statistics</a></li>
                    <li><a href="#prompt-template" class="toc-link themeable-text-secondary hover:text-teal-500 transition-colors">Prompt Template</a></li>
                    <li><a href="#pebblous-perspective" class="toc-link themeable-text-secondary hover:text-teal-500 transition-colors">Pebblous Perspective</a></li>
                    <li><a href="#faq" class="toc-link themeable-text-secondary hover:text-teal-500 transition-colors">FAQ</a></li>
                    <li><a href="#datasets-sources" class="toc-link themeable-text-secondary hover:text-teal-500 transition-colors">Dataset Sources</a></li>
                    <li><a href="#conclusion" class="toc-link themeable-text-secondary hover:text-teal-500 transition-colors">Conclusion</a></li>
                    <li><a href="#pdf-download" class="toc-link themeable-text-secondary hover:text-teal-500 transition-colors">PDF Download</a></li>
                </ul>
            </nav>

            <!-- Main Article -->
            <main class="max-w-[800px] px-4 sm:px-6">

                <!-- Hero Section -->
                <header class="text-left mb-16">
                    <h1 id="page-h1-title" class="text-4xl md:text-5xl font-bold themeable-heading mb-6 leading-tight" style="line-height: 1.4;">
                    </h1>

                    <!-- Publication info -->
                    <p class="text-sm themeable-muted">2025.11 ¬∑ Pebblous Data Communication Team</p>
                    <p class="text-sm themeable-muted mt-1">Reading time: ~20 min ¬∑ <a href="../ko/" class="text-orange-400 hover:text-orange-300 transition-colors">ÌïúÍµ≠Ïñ¥</a></p>
                </header>

                <!-- Section 1: Introduction -->
                <section id="intro" class="mb-12">
                    <h2 class="text-3xl font-bold themeable-heading mb-6 pb-3 border-b-2 themeable-border">
                        1. Introduction
                    </h2>

                    <div class="themeable-text-secondary space-y-4 mb-6">
                        <p>
                            This report is part of Pebblous's <strong class="teal-text">AADS (Agentic AI Data Scientist)</strong> initiative,
                            documenting <strong class="orange-text">the entire process of building high-quality question-answer (QA) datasets</strong>
                            for developing a custom large language model (LLM) with deep expertise in robotics.
                            This process is a critical step in enabling LLMs to effectively learn and leverage the complexity and expertise of robotics technology.
                        </p>
                        <p>
                            Systematically generating question-answer pairs from robotics technical documents holds strategically significant value.
                            This goes beyond simple information summarization or extraction -- it is a process of training <strong class="teal-text">LLMs to deeply understand the complex context and technical nuances of a specific domain</strong>.
                            Such meticulously constructed QA datasets serve as decisive factors in enhancing domain comprehension, answer accuracy, and ultimately the reliability of LLMs,
                            forming a solid foundation for domain-specific LLM fine-tuning in data science.
                        </p>
                        <p>
                            This report first defines the <strong class="orange-text">macro-level blueprint of 13 dataset groups</strong>,
                            then presents a <strong class="teal-text">4-type query framework</strong> for micro-level information extraction.
                            Through the QA samples generated using this systematic approach, we will clearly demonstrate how this dataset structures robotics domain knowledge
                            and is designed to maximize the reasoning capabilities of LLMs.
                        </p>
                    </div>
                </section>

                <!-- Section 2: Robot Intelligence Dataset Groups and Source Document Summary -->
                <section id="overview" class="mb-12">
                    <h2 class="text-3xl font-bold themeable-heading mb-6 pb-3 border-b-2 themeable-border">
                        2. Robot Intelligence Dataset Groups and Source Document Summary
                    </h2>

                    <div class="themeable-text-secondary space-y-4 mb-6">
                        <p>
                            This section describes the process of defining <strong class="orange-text">13 core dataset groups</strong> for systematically analyzing and classifying the vast and diverse robotics technical documents.
                            This classification is the first step toward comprehensively covering the broad robotics technology domain and establishing a <strong class="teal-text">structured knowledge base for LLM training</strong>.
                            Each group represents specific robotics technologies, tasks, or environmental data, designed to ensure LLMs acquire balanced and unbiased knowledge.
                        </p>
                        <p>
                            The table below summarizes the names and key contents of the 13 dataset groups, along with the source documents from which the information was derived, outlining the overall scope and composition of the dataset.
                        </p>
                    </div>

                    <!-- 13 Dataset Groups Table -->
                    <div class="overflow-x-auto mb-6">
                        <table class="w-full border-collapse text-sm">
                            <thead>
                                <tr>
                                    <th class="border themeable-border px-3 py-2 text-left w-16">No.</th>
                                    <th class="border themeable-border px-3 py-2 text-left">Dataset Group Name</th>
                                    <th class="border themeable-border px-3 py-2 text-left">Key Contents</th>
                                    <th class="border themeable-border px-3 py-2 text-left">Source Document</th>
                                </tr>
                            </thead>
                            <tbody class="themeable-text-secondary">
                                <tr class="hover:bg-orange-500/5 transition-colors">
                                    <td class="border themeable-border px-3 py-2 text-center"><strong class="teal-text">1</strong></td>
                                    <td class="border themeable-border px-3 py-2">3D Scanned Object Data</td>
                                    <td class="border themeable-border px-3 py-2">Data digitizing objects from various environments (household, industrial, logistics) using 3D scanners, including original shape and visual information</td>
                                    <td class="border themeable-border px-3 py-2">Occluded Object Reasoning Data</td>
                                </tr>
                                <tr class="hover:bg-orange-500/5 transition-colors">
                                    <td class="border themeable-border px-3 py-2 text-center"><strong class="teal-text">2</strong></td>
                                    <td class="border themeable-border px-3 py-2">Multi-Object Occlusion Environment Data</td>
                                    <td class="border themeable-border px-3 py-2">Data simulating scenarios where multiple objects occlude each other in complex environments such as desks, shelves, and boxes. Includes RGB, Depth, and Point Cloud (PCD)</td>
                                    <td class="border themeable-border px-3 py-2">Occluded Object Reasoning Data</td>
                                </tr>
                                <tr class="hover:bg-orange-500/5 transition-colors">
                                    <td class="border themeable-border px-3 py-2 text-center"><strong class="teal-text">3</strong></td>
                                    <td class="border themeable-border px-3 py-2">6D Object Pose Estimation Data</td>
                                    <td class="border themeable-border px-3 py-2">Data containing 3D position (x,y,z) and 3D rotation (R) values needed for robots to precisely manipulate objects</td>
                                    <td class="border themeable-border px-3 py-2">Occluded Object Reasoning Data</td>
                                </tr>
                                <tr class="hover:bg-orange-500/5 transition-colors">
                                    <td class="border themeable-border px-3 py-2 text-center"><strong class="teal-text">4</strong></td>
                                    <td class="border themeable-border px-3 py-2">Robot-Object Grasping Data</td>
                                    <td class="border themeable-border px-3 py-2">Data recording the process of grasping objects using various robot arms (UR5, Panda) and grippers</td>
                                    <td class="border themeable-border px-3 py-2">Occluded Object Reasoning Data</td>
                                </tr>
                                <tr class="hover:bg-orange-500/5 transition-colors">
                                    <td class="border themeable-border px-3 py-2 text-center"><strong class="teal-text">5</strong></td>
                                    <td class="border themeable-border px-3 py-2">Human-Object Grasping Data</td>
                                    <td class="border themeable-border px-3 py-2">Data recording how humans grasp everyday objects to help robots learn and imitate human grasping methods</td>
                                    <td class="border themeable-border px-3 py-2">Occluded Object Reasoning Data, Hand-Arm Grasp-Manipulation Data</td>
                                </tr>
                                <tr class="hover:bg-orange-500/5 transition-colors">
                                    <td class="border themeable-border px-3 py-2 text-center"><strong class="teal-text">6</strong></td>
                                    <td class="border themeable-border px-3 py-2">Robot Hand Object Property Data</td>
                                    <td class="border themeable-border px-3 py-2">Time-series and physical quantity data obtained from a robot hand performing 5 tasks (gripping, rotating, shaking, etc.) on 200 types of household items</td>
                                    <td class="border themeable-border px-3 py-2">Robot Hand Object Property Identification Data</td>
                                </tr>
                                <tr class="hover:bg-orange-500/5 transition-colors">
                                    <td class="border themeable-border px-3 py-2 text-center"><strong class="teal-text">7</strong></td>
                                    <td class="border themeable-border px-3 py-2">Off-Road Driving Data</td>
                                    <td class="border themeable-border px-3 py-2">2D image and 3D LiDAR sensor data for delivery robots navigating autonomously in non-road environments such as sidewalks, alleys, and parks</td>
                                    <td class="border themeable-border px-3 py-2">Delivery Robot Off-Road Driving Data</td>
                                </tr>
                                <tr class="hover:bg-orange-500/5 transition-colors">
                                    <td class="border themeable-border px-3 py-2 text-center"><strong class="teal-text">8</strong></td>
                                    <td class="border themeable-border px-3 py-2">Indoor Multi-Use Facility Driving Data</td>
                                    <td class="border themeable-border px-3 py-2">Driving data collected from quadruped and wheeled robot perspectives in complex indoor public spaces such as restaurants, exhibition halls, and sports facilities</td>
                                    <td class="border themeable-border px-3 py-2">Robot-View Driving Video (Enhanced) Data</td>
                                </tr>
                                <tr class="hover:bg-orange-500/5 transition-colors">
                                    <td class="border themeable-border px-3 py-2 text-center"><strong class="teal-text">9</strong></td>
                                    <td class="border themeable-border px-3 py-2">SLAM and Path Estimation Data</td>
                                    <td class="border themeable-border px-3 py-2">LiDAR and IMU sensor data used for robots to determine their position and simultaneously build maps (SLAM)</td>
                                    <td class="border themeable-border px-3 py-2">Robot-View Driving Video (Enhanced) Data</td>
                                </tr>
                                <tr class="hover:bg-orange-500/5 transition-colors">
                                    <td class="border themeable-border px-3 py-2 text-center"><strong class="teal-text">10</strong></td>
                                    <td class="border themeable-border px-3 py-2">Human Activity Recognition Data</td>
                                    <td class="border themeable-border px-3 py-2">Data combining video recordings of human behavior (browsing, using, finishing) at automated service systems like kiosks with user metadata</td>
                                    <td class="border themeable-border px-3 py-2">Human Activity Recognition Robot Autonomous Behavior Data</td>
                                </tr>
                                <tr class="hover:bg-orange-500/5 transition-colors">
                                    <td class="border themeable-border px-3 py-2 text-center"><strong class="teal-text">11</strong></td>
                                    <td class="border themeable-border px-3 py-2">Hand/Arm Coordinated Manipulation Data</td>
                                    <td class="border themeable-border px-3 py-2">Multimodal data (video, hand joint coordinates, force sensors) recording coordinated hand-arm movements during specific tasks (opening doors, pressing buttons, etc.)</td>
                                    <td class="border themeable-border px-3 py-2">Hand-Arm Coordinated Grasp-Manipulation Data</td>
                                </tr>
                                <tr class="hover:bg-orange-500/5 transition-colors">
                                    <td class="border themeable-border px-3 py-2 text-center"><strong class="teal-text">12</strong></td>
                                    <td class="border themeable-border px-3 py-2">Service Robot Status and Operations Data</td>
                                    <td class="border themeable-border px-3 py-2">Time-series text (JSON) data recording the real-time status (position, battery, task progress) of various service robots for guidance, delivery, and cleaning</td>
                                    <td class="border themeable-border px-3 py-2">Indoor Space Maintenance Service Robot Data</td>
                                </tr>
                                <tr class="hover:bg-orange-500/5 transition-colors">
                                    <td class="border themeable-border px-3 py-2 text-center"><strong class="teal-text">13</strong></td>
                                    <td class="border themeable-border px-3 py-2">Robot Error and Preventive Maintenance Data</td>
                                    <td class="border themeable-border px-3 py-2">Data labeling the status and causes of errors (obstacles, collisions, network issues, etc.) occurring during service robot operations</td>
                                    <td class="border themeable-border px-3 py-2">Indoor Space Maintenance Service Robot Data</td>
                                </tr>
                            </tbody>
                        </table>
                    </div>

                    <div class="themeable-text-secondary">
                        <p>
                            The table above clearly shows the overall scope of knowledge for LLM training and the source of each piece of information, supporting the credibility and systematicness of the subsequent QA generation process.
                        </p>
                    </div>
                </section>

                <!-- Section 3: QA Dataset Query Type Definition -->
                <section id="query-types" class="mb-12">
                    <h2 class="text-3xl font-bold themeable-heading mb-6 pb-3 border-b-2 themeable-border">
                        3. QA Dataset Query Type Definition
                    </h2>

                    <div class="themeable-text-secondary space-y-4 mb-8">
                        <p>
                            To effectively extract consistent and in-depth information from high-quality source documents, this project defined <strong class="orange-text">4 core query types</strong>.
                            This standardized framework analyzes the technical, business, and managerial aspects of each dataset from multiple angles,
                            serving as an important methodology that guides <strong class="teal-text">LLMs to learn balanced expertise without bias toward any particular aspect</strong>.
                        </p>
                        <p>
                            The 4 core query types are as follows:
                        </p>
                    </div>

                    <!-- 4 Query Type Cards -->
                    <div class="grid grid-cols-1 md:grid-cols-2 gap-6 mb-8">
                        <!-- Type A -->
                        <div class="themeable-bg card-hover rounded-lg p-6 border-l-4 border-teal-500">
                            <h3 class="text-xl font-bold teal-text mb-3">A. Domain Definition/Purpose</h3>
                            <p class="themeable-text-secondary text-sm">
                                Questions about the domain context and goals, including the industry problems the dataset aims to solve, business purposes, and target applications.
                                Through this, LLMs learn the <strong class="orange-text">strategic value and business purpose</strong> of the data.
                            </p>
                        </div>

                        <!-- Type B -->
                        <div class="themeable-bg card-hover rounded-lg p-6 border-l-4 border-orange-500">
                            <h3 class="text-xl font-bold orange-text mb-3">B. Data Structure/Composition</h3>
                            <p class="themeable-text-secondary text-sm">
                                Questions about the physical/logical structure of datasets, including data volume, file formats, labeling attributes, metadata fields, and data distributions.
                                Through this type, LLMs understand the <strong class="orange-text">logical schema and physical layout</strong> of the data.
                            </p>
                        </div>

                        <!-- Type C -->
                        <div class="themeable-bg card-hover rounded-lg p-6 border-l-4 border-purple-500">
                            <h3 class="text-xl font-bold text-purple-600 dark:text-purple-400 mb-3">C. AI Model/Task</h3>
                            <p class="themeable-text-secondary text-sm">
                                Questions about AI technology application strategy, including applied algorithms, task definitions, model selection rationale, prediction targets, and performance metrics.
                                Through this, LLMs grasp the <strong class="orange-text">modeling tasks and technical utilization strategies</strong> of the data.
                            </p>
                        </div>

                        <!-- Type D -->
                        <div class="themeable-bg card-hover rounded-lg p-6 border-l-4 border-blue-500">
                            <h3 class="text-xl font-bold text-blue-600 dark:text-blue-400 mb-3">D. Quality/Process Management</h3>
                            <p class="themeable-text-secondary text-sm">
                                Questions about data lifecycle management, including data acquisition, refinement, labeling standards, inspection procedures, and quality management metrics.
                                This type provides information about the <strong class="orange-text">data provenance and quality assurance standards</strong>.
                            </p>
                        </div>
                    </div>

                    <div class="themeable-text-secondary">
                        <p>
                            This systematic query type approach serves as the logical foundation for the specific QA samples presented in the next chapter,
                            playing a critical role in ensuring the quality and consistency of the generated dataset.
                        </p>
                    </div>
                </section>

                <!-- Section 4: QA Dataset Samples by Group -->
                <section id="robot-datasets" class="mb-12">
                    <h2 class="text-3xl font-bold themeable-heading mb-6 pb-3 border-b-2 themeable-border">
                        4. QA Dataset Samples by Group
                    </h2>

                    <div class="themeable-text-secondary space-y-4 mb-8">
                        <p>
                            This section presents QA samples generated by applying the 13 previously defined dataset groups and 4 query types.
                            Each sample is based on key information from source technical documents and serves as
                            <strong class="teal-text">a practical example specifying the concrete quality and format of data to be directly used for robotics LLM fine-tuning</strong>.
                        </p>
                    </div>

                    <!-- Dataset 1: 3D Scanned Object Data -->
                    <div id="dataset-1" class="mb-8">
                        <h3 class="text-2xl font-semibold themeable-heading mb-4 flex items-center">
                            <span class="teal-text mr-2">1.</span>
                            3D Scanned Object Data
                        </h3>

                        <details class="mt-4">
                            <summary class="cursor-pointer font-semibold teal-text hover:text-orange-500 transition-colors">
                                üìù View QA Samples (4 pairs)
                            </summary>
                            <div class="mt-4 overflow-x-auto">
                                <table class="w-full border-collapse text-sm">
                                    <thead>
                                        <tr>
                                            <th class="border themeable-border px-3 py-2 text-left w-32">Type</th>
                                            <th class="border themeable-border px-3 py-2 text-left">Question</th>
                                            <th class="border themeable-border px-3 py-2 text-left">Answer</th>
                                        </tr>
                                    </thead>
                                    <tbody class="themeable-text-secondary">
                                        <tr class="hover:bg-orange-500/5 transition-colors">
                                            <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">A</strong></td>
                                            <td class="border themeable-border px-3 py-2">What is the ultimate impact of building the 'Occluded Object Reasoning Data'?</td>
                                            <td class="border themeable-border px-3 py-2">By building the world's largest and highest-quality occluded object dataset, it aims to activate related research and extend robot vision algorithms to diverse real-world environments. It also aims to stimulate relevant industries such as logistics through robot arm object grasping, manipulation, transport, and placement.</td>
                                        </tr>
                                        <tr class="hover:bg-orange-500/5 transition-colors">
                                            <td class="border themeable-border px-3 py-2 align-top"><strong class="orange-text">B</strong></td>
                                            <td class="border themeable-border px-3 py-2">What is the raw data format generated using the Artec 3D Leo tool when collecting 3D scan data?</td>
                                            <td class="border themeable-border px-3 py-2">The Artec 3D Leo tool merges RGB-D object 3D scan raw data and camera parameters into mesh data in 'obj' format.</td>
                                        </tr>
                                        <tr class="hover:bg-orange-500/5 transition-colors">
                                            <td class="border themeable-border px-3 py-2 align-top"><strong class="text-purple-600 dark:text-purple-400">C</strong></td>
                                            <td class="border themeable-border px-3 py-2">What are the candidate models for '6D object pose prediction' that can be trained using the 'Occluded Object Reasoning Data'?</td>
                                            <td class="border themeable-border px-3 py-2">PoseCNN, PVNet, and TemplatePose models are available. PoseCNN is a representative pose estimation model in robot environments, and PVNet was proposed to improve performance in occluded environments.</td>
                                        </tr>
                                        <tr class="hover:bg-orange-500/5 transition-colors">
                                            <td class="border themeable-border px-3 py-2 align-top"><strong class="text-blue-600 dark:text-blue-400">D</strong></td>
                                            <td class="border themeable-border px-3 py-2">What software is used for data calibration during 3D scanning data processing?</td>
                                            <td class="border themeable-border px-3 py-2">After scanning objects with the Artec 3D scanner, data is calibrated using Artec Studio SW. This software supports mesh and point cloud acquisition through its built-in RGB-D merging and post-processing tools.</td>
                                        </tr>
                                    </tbody>
                                </table>
                            </div>
                        </details>
                    </div>

                    <!-- Dataset 2: Multi-Object Occlusion Environment Data -->
                    <div id="dataset-2" class="mb-8">
                        <h3 class="text-2xl font-semibold themeable-heading mb-4 flex items-center">
                            <span class="teal-text mr-2">2.</span>
                            Multi-Object Occlusion Environment Data
                        </h3>

                        <details class="mt-4">
                            <summary class="cursor-pointer font-semibold teal-text hover:text-orange-500 transition-colors">
                                üìù View QA Samples (4 pairs)
                            </summary>
                            <div class="mt-4 overflow-x-auto">
                                <table class="w-full border-collapse text-sm">
                                    <thead>
                                        <tr>
                                            <th class="border themeable-border px-3 py-2 text-left w-32">Type</th>
                                            <th class="border themeable-border px-3 py-2 text-left">Question</th>
                                            <th class="border themeable-border px-3 py-2 text-left">Answer</th>
                                        </tr>
                                    </thead>
                                    <tbody class="themeable-text-secondary">
                                        <tr class="hover:bg-orange-500/5 transition-colors">
                                            <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">A</strong></td>
                                            <td class="border themeable-border px-3 py-2">What real-world robot environments was the 'Multi-Object Occlusion Data' designed to represent?</td>
                                            <td class="border themeable-border px-3 py-2">Data was collected by configuring three representative environments that robots may encounter: desk, shelf, and box environments. IKEA furniture was used specifically to enable researchers worldwide to easily replicate the same environment.</td>
                                        </tr>
                                        <tr class="hover:bg-orange-500/5 transition-colors">
                                            <td class="border themeable-border px-3 py-2 align-top"><strong class="orange-text">B</strong></td>
                                            <td class="border themeable-border px-3 py-2">What information is the raw data of the 'Multi-Object Occlusion Data' set matched with?</td>
                                            <td class="border themeable-border px-3 py-2">The raw data is matched with scene information (scene location type, scene ID, etc.) and object type information (semantic class, instance class, object id, etc.).</td>
                                        </tr>
                                        <tr class="hover:bg-orange-500/5 transition-colors">
                                            <td class="border themeable-border px-3 py-2 align-top"><strong class="text-purple-600 dark:text-purple-400">C</strong></td>
                                            <td class="border themeable-border px-3 py-2">What AI models were considered for 'amodal instance segmentation' that segments objects including occluded regions?</td>
                                            <td class="border themeable-border px-3 py-2">ORCNN, ASN, and UOAIS-Net models were considered. ORCNN was the first proposed model, while ASN and UOAIS-Net achieved the best performance as of 2020 and 2022 respectively.</td>
                                        </tr>
                                        <tr class="hover:bg-orange-500/5 transition-colors">
                                            <td class="border themeable-border px-3 py-2 align-top"><strong class="text-blue-600 dark:text-blue-400">D</strong></td>
                                            <td class="border themeable-border px-3 py-2">What tools were used for labeling refinement of synthetic data?</td>
                                            <td class="border themeable-border px-3 py-2">Labeling refinement was performed using a proprietary synthetic data refinement program built by modifying and improving 'bop_toolkit'. This program automatically generates GT (Ground Truth) and JSON files.</td>
                                        </tr>
                                    </tbody>
                                </table>
                            </div>
                        </details>
                    </div>

                    <!-- Dataset 3: 6D Object Pose Estimation Data -->
                    <div id="dataset-3" class="mb-8">
                        <h3 class="text-2xl font-semibold themeable-heading mb-4 flex items-center">
                            <span class="teal-text mr-2">3.</span>
                            6D Object Pose Estimation Data
                        </h3>

                        <details class="mt-4">
                            <summary class="cursor-pointer font-semibold teal-text hover:text-orange-500 transition-colors">
                                üìù View QA Samples (4 pairs)
                            </summary>
                            <div class="mt-4 overflow-x-auto">
                                <table class="w-full border-collapse text-sm">
                                    <thead>
                                        <tr>
                                            <th class="border themeable-border px-3 py-2 text-left w-32">Type</th>
                                            <th class="border themeable-border px-3 py-2 text-left">Question</th>
                                            <th class="border themeable-border px-3 py-2 text-left">Answer</th>
                                        </tr>
                                    </thead>
                                    <tbody class="themeable-text-secondary">
                                        <tr class="hover:bg-orange-500/5 transition-colors">
                                            <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">A</strong></td>
                                            <td class="border themeable-border px-3 py-2">What are the main applications of 6D object pose prediction technology?</td>
                                            <td class="border themeable-border px-3 py-2">It is applied to automation systems requiring precise object recognition and manipulation, such as peg-in-hole assembly and product assembly. It can also be used for object recognition and sequence reasoning in logistics or household service robots.</td>
                                        </tr>
                                        <tr class="hover:bg-orange-500/5 transition-colors">
                                            <td class="border themeable-border px-3 py-2 align-top"><strong class="orange-text">B</strong></td>
                                            <td class="border themeable-border px-3 py-2">How are the 3D Translation and 3D Rotation values, the outputs of 6D object pose prediction, represented?</td>
                                            <td class="border themeable-border px-3 py-2">3D Translation (T) is expressed as x, y, z coordinates, and 3D Rotation (R) is expressed as quaternion format with x, y, z, w values.</td>
                                        </tr>
                                        <tr class="hover:bg-orange-500/5 transition-colors">
                                            <td class="border themeable-border px-3 py-2 align-top"><strong class="text-purple-600 dark:text-purple-400">C</strong></td>
                                            <td class="border themeable-border px-3 py-2">What was the best-performing 6D object pose estimation model on the BOP dataset as of 2019, and what are its characteristics?</td>
                                            <td class="border themeable-border px-3 py-2">It is the PVNet model proposed by Peng et al. This model is characterized by improving pose estimation performance for occluded objects through per-object Keypoint Vector Field prediction.</td>
                                        </tr>
                                        <tr class="hover:bg-orange-500/5 transition-colors">
                                            <td class="border themeable-border px-3 py-2 align-top"><strong class="text-blue-600 dark:text-blue-400">D</strong></td>
                                            <td class="border themeable-border px-3 py-2">What tools are used for raw data generation and calibration of 6D object pose estimation data, and how is quality ensured?</td>
                                            <td class="border themeable-border px-3 py-2">The Artec 3D Leo tool is used to merge RGB-D scan raw data and camera parameters into mesh (obj) format, and Artec Studio SW's built-in tools are used to calibrate data for high-quality mesh and point cloud acquisition.</td>
                                        </tr>
                                    </tbody>
                                </table>
                            </div>
                        </details>
                    </div>

                    <!-- Dataset 4: Robot-Object Grasping Data -->
                    <div id="dataset-4" class="mb-8">
                        <h3 class="text-2xl font-semibold themeable-heading mb-4 flex items-center">
                            <span class="teal-text mr-2">4.</span>
                            Robot-Object Grasping Data
                        </h3>

                        <details class="mt-4">
                            <summary class="cursor-pointer font-semibold teal-text hover:text-orange-500 transition-colors">
                                üìù View QA Samples (4 pairs)
                            </summary>
                            <div class="mt-4 overflow-x-auto">
                                <table class="w-full border-collapse text-sm">
                                    <thead>
                                        <tr>
                                            <th class="border themeable-border px-3 py-2 text-left w-32">Type</th>
                                            <th class="border themeable-border px-3 py-2 text-left">Question</th>
                                            <th class="border themeable-border px-3 py-2 text-left">Answer</th>
                                        </tr>
                                    </thead>
                                    <tbody class="themeable-text-secondary">
                                        <tr class="hover:bg-orange-500/5 transition-colors">
                                            <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">A</strong></td>
                                            <td class="border themeable-border px-3 py-2">How does a robot plan the grasping sequence for occluded objects?</td>
                                            <td class="border themeable-border px-3 py-2">First, the visible regions, occluded regions, and occlusion status of objects are recognized. Then adjacent objects are sequentially grasped and removed until the target object is no longer occluded, and finally the target object is grasped.</td>
                                        </tr>
                                        <tr class="hover:bg-orange-500/5 transition-colors">
                                            <td class="border themeable-border px-3 py-2 align-top"><strong class="orange-text">B</strong></td>
                                            <td class="border themeable-border px-3 py-2">What combinations of robot arms and grippers compose the robot-object grasping data?</td>
                                            <td class="border themeable-border px-3 py-2">Robot arms include UR5 and Panda, combined with various grippers including Robotiq 2f, Robotiq 3f, Allegro, Qb_hand, Suction, RG_2, Panda_gripper, and Delto_3f.</td>
                                        </tr>
                                        <tr class="hover:bg-orange-500/5 transition-colors">
                                            <td class="border themeable-border px-3 py-2 align-top"><strong class="text-purple-600 dark:text-purple-400">C</strong></td>
                                            <td class="border themeable-border px-3 py-2">What CNN-based model was proposed for 6D pose estimation in robot environments, and how does it predict pose?</td>
                                            <td class="border themeable-border px-3 py-2">It is the PoseCNN model proposed by Xiang et al. This model predicts pose by performing regression on per-object Class, Position, and Rotation.</td>
                                        </tr>
                                        <tr class="hover:bg-orange-500/5 transition-colors">
                                            <td class="border themeable-border px-3 py-2 align-top"><strong class="text-blue-600 dark:text-blue-400">D</strong></td>
                                            <td class="border themeable-border px-3 py-2">How were grippers configured during robot-object grasping data collection to enhance data usefulness?</td>
                                            <td class="border themeable-border px-3 py-2">Both general-purpose and domestic grippers were included, covering all types from 1-finger to 5-finger. In particular, the highly utilized 2-finger and 3-finger grippers were represented by 3 and 2 types respectively to enhance data usefulness.</td>
                                        </tr>
                                    </tbody>
                                </table>
                            </div>
                        </details>
                    </div>

                    <!-- Dataset 5: Human-Object Grasping Data -->
                    <div id="dataset-5" class="mb-8">
                        <h3 class="text-2xl font-semibold themeable-heading mb-4 flex items-center">
                            <span class="teal-text mr-2">5.</span>
                            Human-Object Grasping Data
                        </h3>

                        <details class="mt-4">
                            <summary class="cursor-pointer font-semibold teal-text hover:text-orange-500 transition-colors">
                                üìù View QA Samples (4 pairs)
                            </summary>
                            <div class="mt-4 overflow-x-auto">
                                <table class="w-full border-collapse text-sm">
                                    <thead>
                                        <tr>
                                            <th class="border themeable-border px-3 py-2 text-left w-32">Type</th>
                                            <th class="border themeable-border px-3 py-2 text-left">Question</th>
                                            <th class="border themeable-border px-3 py-2 text-left">Answer</th>
                                        </tr>
                                    </thead>
                                    <tbody class="themeable-text-secondary">
                                        <tr class="hover:bg-orange-500/5 transition-colors">
                                            <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">A</strong></td>
                                            <td class="border themeable-border px-3 py-2">What is the purpose of collecting human-object grasping data?</td>
                                            <td class="border themeable-border px-3 py-2">The purpose is to record how humans grasp various everyday objects, enabling robots to learn and imitate human grasping and manipulation methods.</td>
                                        </tr>
                                        <tr class="hover:bg-orange-500/5 transition-colors">
                                            <td class="border themeable-border px-3 py-2 align-top"><strong class="orange-text">B</strong></td>
                                            <td class="border themeable-border px-3 py-2">How is human-object grasping data classified by hand state and number of grasped objects?</td>
                                            <td class="border themeable-border px-3 py-2">Hand state is classified as left hand, right hand, or both hands, and the number of grasped objects is classified as 1 or 2 to compose the data.</td>
                                        </tr>
                                        <tr class="hover:bg-orange-500/5 transition-colors">
                                            <td class="border themeable-border px-3 py-2 align-top"><strong class="text-purple-600 dark:text-purple-400">C</strong></td>
                                            <td class="border themeable-border px-3 py-2">What type of AI model is used to classify and understand human hand movements?</td>
                                            <td class="border themeable-border px-3 py-2">An ST-GCN (Spatial-Temporal Graph Convolutional Network) based action classification model is used. It classifies action classes by learning temporal and spatial features in graph form.</td>
                                        </tr>
                                        <tr class="hover:bg-orange-500/5 transition-colors">
                                            <td class="border themeable-border px-3 py-2 align-top"><strong class="text-blue-600 dark:text-blue-400">D</strong></td>
                                            <td class="border themeable-border px-3 py-2">How were experiment participants selected for human-object grasping data collection?</td>
                                            <td class="border themeable-border px-3 py-2">Considering the diversity in hand shape and size, 20 participants with varying gender and age were recruited for data collection.</td>
                                        </tr>
                                    </tbody>
                                </table>
                            </div>
                        </details>
                    </div>

                    <!-- Dataset 6: Robot Hand Object Property Data -->
                    <div id="dataset-6" class="mb-8">
                        <h3 class="text-2xl font-semibold themeable-heading mb-4 flex items-center">
                            <span class="teal-text mr-2">6.</span>
                            Robot Hand Object Property Data
                        </h3>

                        <details class="mt-4">
                            <summary class="cursor-pointer font-semibold teal-text hover:text-orange-500 transition-colors">
                                üìù View QA Samples (4 pairs)
                            </summary>
                            <div class="mt-4 overflow-x-auto">
                                <table class="w-full border-collapse text-sm">
                                    <thead>
                                        <tr>
                                            <th class="border themeable-border px-3 py-2 text-left w-32">Type</th>
                                            <th class="border themeable-border px-3 py-2 text-left">Question</th>
                                            <th class="border themeable-border px-3 py-2 text-left">Answer</th>
                                        </tr>
                                    </thead>
                                    <tbody class="themeable-text-secondary">
                                        <tr class="hover:bg-orange-500/5 transition-colors">
                                            <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">A</strong></td>
                                            <td class="border themeable-border px-3 py-2">What is the necessity of building the 'Robot Hand Object Property Identification Data'?</td>
                                            <td class="border themeable-border px-3 py-2">It is necessary to rapidly acquire visual information, physical quantities, and time-series data from manipulation to contribute to improving robot intelligence for precise object recognition and manipulation.</td>
                                        </tr>
                                        <tr class="hover:bg-orange-500/5 transition-colors">
                                            <td class="border themeable-border px-3 py-2 align-top"><strong class="orange-text">B</strong></td>
                                            <td class="border themeable-border px-3 py-2">What types of data are built for a single object in the 'Robot Hand Object Property Identification Data'?</td>
                                            <td class="border themeable-border px-3 py-2">Video data (Hi-RGB, Low-RGB, RGB-D), 3D point cloud mesh, physical quantities (weight, size, material), and time-series data (tactile, temperature, force, sound) generated during 5 task executions are built.</td>
                                        </tr>
                                        <tr class="hover:bg-orange-500/5 transition-colors">
                                            <td class="border themeable-border px-3 py-2 align-top"><strong class="text-purple-600 dark:text-purple-400">C</strong></td>
                                            <td class="border themeable-border px-3 py-2">What AI model is used to predict the stable grasping point for a robot hand to grasp an object?</td>
                                            <td class="border themeable-border px-3 py-2">A CNN-based grasp point search algorithm is used. It learns from labeled data of center points, sizes, and angles of graspable positions in images to predict optimal grasping points.</td>
                                        </tr>
                                        <tr class="hover:bg-orange-500/5 transition-colors">
                                            <td class="border themeable-border px-3 py-2 align-top"><strong class="text-blue-600 dark:text-blue-400">D</strong></td>
                                            <td class="border themeable-border px-3 py-2">How are force levels divided during robot hand task data collection?</td>
                                            <td class="border themeable-border px-3 py-2">Force levels are divided into 5 stages (force level 0-4), with each stage performing 10 task executions to secure interaction data under various force conditions.</td>
                                        </tr>
                                    </tbody>
                                </table>
                            </div>
                        </details>
                    </div>

                    <!-- Dataset 7: Off-Road Driving Data -->
                    <div id="dataset-7" class="mb-8">
                        <h3 class="text-2xl font-semibold themeable-heading mb-4 flex items-center">
                            <span class="teal-text mr-2">7.</span>
                            Off-Road Driving Data
                        </h3>

                        <details class="mt-4">
                            <summary class="cursor-pointer font-semibold teal-text hover:text-orange-500 transition-colors">
                                üìù View QA Samples (4 pairs)
                            </summary>
                            <div class="mt-4 overflow-x-auto">
                                <table class="w-full border-collapse text-sm">
                                    <thead>
                                        <tr>
                                            <th class="border themeable-border px-3 py-2 text-left w-32">Type</th>
                                            <th class="border themeable-border px-3 py-2 text-left">Question</th>
                                            <th class="border themeable-border px-3 py-2 text-left">Answer</th>
                                        </tr>
                                    </thead>
                                    <tbody class="themeable-text-secondary">
                                        <tr class="hover:bg-orange-500/5 transition-colors">
                                            <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">A</strong></td>
                                            <td class="border themeable-border px-3 py-2">Under what social context was the 'Delivery Robot Off-Road Driving Data' built?</td>
                                            <td class="border themeable-border px-3 py-2">It was built to support the development of autonomous driving technology for delivery robots as an alternative to address the surge in parcel volume due to contactless transactions, labor shortages, and delivery app commission burdens.</td>
                                        </tr>
                                        <tr class="hover:bg-orange-500/5 transition-colors">
                                            <td class="border themeable-border px-3 py-2 align-top"><strong class="orange-text">B</strong></td>
                                            <td class="border themeable-border px-3 py-2">What classes are targeted for segmentation processing in the 2D image data of the 'Delivery Robot Off-Road Driving Data'?</td>
                                            <td class="border themeable-border px-3 py-2">A total of 22 classes are processed, including major classes such as passenger cars, pedestrians, roads, sidewalks, crosswalks, buildings, and vegetation.</td>
                                        </tr>
                                        <tr class="hover:bg-orange-500/5 transition-colors">
                                            <td class="border themeable-border px-3 py-2 align-top"><strong class="text-purple-600 dark:text-purple-400">C</strong></td>
                                            <td class="border themeable-border px-3 py-2">What model was selected to recognize Drivable Area based on 2D image data, and what was the selection rationale?</td>
                                            <td class="border themeable-border px-3 py-2">The ERF-PSPNet model was selected because it was determined to outperform LinkNet by using an efficient Deep Architecture.</td>
                                        </tr>
                                        <tr class="hover:bg-orange-500/5 transition-colors">
                                            <td class="border themeable-border px-3 py-2 align-top"><strong class="text-blue-600 dark:text-blue-400">D</strong></td>
                                            <td class="border themeable-border px-3 py-2">What measures are taken for privacy protection during data collection?</td>
                                            <td class="border themeable-border px-3 py-2">De-identification measures are applied to all faces (elliptical blur) and vehicle license plates (automatic blur) appearing in the collected data.</td>
                                        </tr>
                                    </tbody>
                                </table>
                            </div>
                        </details>
                    </div>

                    <!-- Dataset 8: Indoor Multi-Use Facility Driving Data -->
                    <div id="dataset-8" class="mb-8">
                        <h3 class="text-2xl font-semibold themeable-heading mb-4 flex items-center">
                            <span class="teal-text mr-2">8.</span>
                            Indoor Multi-Use Facility Driving Data
                        </h3>

                        <details class="mt-4">
                            <summary class="cursor-pointer font-semibold teal-text hover:text-orange-500 transition-colors">
                                üìù View QA Samples (4 pairs)
                            </summary>
                            <div class="mt-4 overflow-x-auto">
                                <table class="w-full border-collapse text-sm">
                                    <thead>
                                        <tr>
                                            <th class="border themeable-border px-3 py-2 text-left w-32">Type</th>
                                            <th class="border themeable-border px-3 py-2 text-left">Question</th>
                                            <th class="border themeable-border px-3 py-2 text-left">Answer</th>
                                        </tr>
                                    </thead>
                                    <tbody class="themeable-text-secondary">
                                        <tr class="hover:bg-orange-500/5 transition-colors">
                                            <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">A</strong></td>
                                            <td class="border themeable-border px-3 py-2">In what fields can the 'Robot-View Driving Video Data' be utilized for technology development?</td>
                                            <td class="border themeable-border px-3 py-2">It can be utilized for autonomous driving and real-time data processing research, sensor and algorithm development for autonomous robots, and advancement of autonomous driving technology for guide/cleaning/transport robots.</td>
                                        </tr>
                                        <tr class="hover:bg-orange-500/5 transition-colors">
                                            <td class="border themeable-border px-3 py-2 align-top"><strong class="orange-text">B</strong></td>
                                            <td class="border themeable-border px-3 py-2">What types of sensor data compose the 'Robot-View Driving Video Data', and what are their formats?</td>
                                            <td class="border themeable-border px-3 py-2">It consists of RGB-D image data (JPG, PNG), LiDAR data (PCD), and 6D IMU sensor data (CSV).</td>
                                        </tr>
                                        <tr class="hover:bg-orange-500/5 transition-colors">
                                            <td class="border themeable-border px-3 py-2 align-top"><strong class="text-purple-600 dark:text-purple-400">C</strong></td>
                                            <td class="border themeable-border px-3 py-2">What learning algorithm was used for validating 3D object detection with this dataset, and what are its characteristics?</td>
                                            <td class="border themeable-border px-3 py-2">The FocalsConv (OpenPCDet) algorithm was used. It combines RGB features from 2D images with LiDAR features and additionally uses depth maps to detect 3D objects.</td>
                                        </tr>
                                        <tr class="hover:bg-orange-500/5 transition-colors">
                                            <td class="border themeable-border px-3 py-2 align-top"><strong class="text-blue-600 dark:text-blue-400">D</strong></td>
                                            <td class="border themeable-border px-3 py-2">What inspection procedures are followed to improve labeling data quality?</td>
                                            <td class="border themeable-border px-3 py-2">A total 3-stage inspection process is followed: cross-inspection between workers (stage 1), manager visual verification and error reassignment (stage 2), and final inspection (stage 3).</td>
                                        </tr>
                                    </tbody>
                                </table>
                            </div>
                        </details>
                    </div>

                    <!-- Dataset 9 -->
                    <div id="dataset-9" class="mb-8">
                        <h3 class="text-2xl font-semibold themeable-heading mb-4 flex items-center">
                            <span class="teal-text mr-2">9.</span>
                            SLAM and Path Estimation Data
                        </h3>
                        <details class="mt-4">
                            <summary class="cursor-pointer font-semibold teal-text hover:text-orange-500 transition-colors">
                                üìù View QA Samples (4 pairs)
                            </summary>
                            <div class="mt-4 overflow-x-auto">
                                <table class="w-full border-collapse text-sm">
                                    <thead>
                                        <tr>
                                            <th class="border themeable-border px-3 py-2 text-left w-32">Type</th>
                                            <th class="border themeable-border px-3 py-2 text-left">Question</th>
                                            <th class="border themeable-border px-3 py-2 text-left">Answer</th>
                                        </tr>
                                    </thead>
                                    <tbody class="themeable-text-secondary">
                                        <tr class="hover:bg-orange-500/5 transition-colors">
                                            <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">A</strong></td>
                                            <td class="border themeable-border px-3 py-2">What is the purpose of SLAM technology, and how does this dataset contribute?</td>
                                            <td class="border themeable-border px-3 py-2">SLAM is a technology for robots to estimate their position and build maps in unknown environments. This dataset provides LiDAR and IMU data for validating the effectiveness of SLAM algorithms.</td>
                                        </tr>
                                        <tr class="hover:bg-orange-500/5 transition-colors">
                                            <td class="border themeable-border px-3 py-2 align-top"><strong class="orange-text">B</strong></td>
                                            <td class="border themeable-border px-3 py-2">What is the file format of data used for SLAM performance validation?</td>
                                            <td class="border themeable-border px-3 py-2">The bag file format used in ROS environments is used. Time-series data from multiple sensors such as LiDAR and IMU are stored with timestamps.</td>
                                        </tr>
                                        <tr class="hover:bg-orange-500/5 transition-colors">
                                            <td class="border themeable-border px-3 py-2 align-top"><strong class="purple-text">C</strong></td>
                                            <td class="border themeable-border px-3 py-2">What algorithm was used for SLAM performance validation, and how is performance measured?</td>
                                            <td class="border themeable-border px-3 py-2">The Fast-LIO2 algorithm was used, and performance is measured by 'End to End RMSE (Root Mean Square Error)'. (Target: within 0.2m)</td>
                                        </tr>
                                        <tr class="hover:bg-orange-500/5 transition-colors">
                                            <td class="border themeable-border px-3 py-2 align-top"><strong class="blue-text">D</strong></td>
                                            <td class="border themeable-border px-3 py-2">How is temporal synchronization between multiple sensors (RGB-D, LiDAR, Metadata) achieved during data collection?</td>
                                            <td class="border themeable-border px-3 py-2">Temporal synchronization is performed through software synchronization during multi-sensor data logging, and source data is generated by slicing according to configured intervals.</td>
                                        </tr>
                                    </tbody>
                                </table>
                            </div>
                        </details>
                    </div>

                    <!-- Dataset 10 -->
                    <div id="dataset-10" class="mb-8">
                        <h3 class="text-2xl font-semibold themeable-heading mb-4 flex items-center">
                            <span class="teal-text mr-2">10.</span>
                            Human Activity Recognition Data
                        </h3>
                        <details class="mt-4">
                            <summary class="cursor-pointer font-semibold teal-text hover:text-orange-500 transition-colors">
                                üìù View QA Samples (4 pairs)
                            </summary>
                            <div class="mt-4 overflow-x-auto">
                                <table class="w-full border-collapse text-sm">
                                    <thead>
                                        <tr>
                                            <th class="border themeable-border px-3 py-2 text-left w-32">Type</th>
                                            <th class="border themeable-border px-3 py-2 text-left">Question</th>
                                            <th class="border themeable-border px-3 py-2 text-left">Answer</th>
                                        </tr>
                                    </thead>
                                    <tbody class="themeable-text-secondary">
                                        <tr class="hover:bg-orange-500/5 transition-colors">
                                            <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">A</strong></td>
                                            <td class="border themeable-border px-3 py-2">What problems faced by digitally vulnerable populations can the 'Human Activity Recognition Robot Autonomous Behavior Data' help solve?</td>
                                            <td class="border themeable-border px-3 py-2">It is used to address difficulties faced by visually impaired people, elderly, and wheelchair users when using kiosks. Robots can recognize user status to provide customized UI or automatic height adjustment features.</td>
                                        </tr>
                                        <tr class="hover:bg-orange-500/5 transition-colors">
                                            <td class="border themeable-border px-3 py-2 align-top"><strong class="orange-text">B</strong></td>
                                            <td class="border themeable-border px-3 py-2">What user metadata is included in the JSON files of this dataset?</td>
                                            <td class="border themeable-border px-3 py-2">It includes user information such as encrypted user ID, age (youth/adult/elderly), height, gender, and disability status, as well as environmental information such as service location and camera height.</td>
                                        </tr>
                                        <tr class="hover:bg-orange-500/5 transition-colors">
                                            <td class="border themeable-border px-3 py-2 align-top"><strong class="purple-text">C</strong></td>
                                            <td class="border themeable-border px-3 py-2">What AI technology development can this data directly contribute to?</td>
                                            <td class="border themeable-border px-3 py-2">It is used for developing user state recognition and behavior prediction models where robots identify human behavior (browsing, using, finishing) and characteristics to provide customized services.</td>
                                        </tr>
                                        <tr class="hover:bg-orange-500/5 transition-colors">
                                            <td class="border themeable-border px-3 py-2 align-top"><strong class="blue-text">D</strong></td>
                                            <td class="border themeable-border px-3 py-2">How was the data collection environment configured?</td>
                                            <td class="border themeable-border px-3 py-2">Data was collected by selecting 10 types of environments similar to actual service environments, including transportation facilities, medical facilities, and educational facilities.</td>
                                        </tr>
                                    </tbody>
                                </table>
                            </div>
                        </details>
                    </div>

                    <!-- Dataset 11 -->
                    <div id="dataset-11" class="mb-8">
                        <h3 class="text-2xl font-semibold themeable-heading mb-4 flex items-center">
                            <span class="teal-text mr-2">11.</span>
                            Hand/Arm Coordinated Manipulation Data
                        </h3>
                        <details class="mt-4">
                            <summary class="cursor-pointer font-semibold teal-text hover:text-orange-500 transition-colors">
                                üìù View QA Samples (4 pairs)
                            </summary>
                            <div class="mt-4 overflow-x-auto">
                                <table class="w-full border-collapse text-sm">
                                    <thead>
                                        <tr>
                                            <th class="border themeable-border px-3 py-2 text-left w-32">Type</th>
                                            <th class="border themeable-border px-3 py-2 text-left">Question</th>
                                            <th class="border themeable-border px-3 py-2 text-left">Answer</th>
                                        </tr>
                                    </thead>
                                    <tbody class="themeable-text-secondary">
                                        <tr class="hover:bg-orange-500/5 transition-colors">
                                            <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">A</strong></td>
                                            <td class="border themeable-border px-3 py-2">In which areas of robotics can the 'Hand-Arm Coordinated Grasp-Manipulation Data' be utilized?</td>
                                            <td class="border themeable-border px-3 py-2">It can be utilized for developing intelligent robots requiring human-like movements such as household assistance robots and industrial collaborative robots, as well as metaverse-based virtual object interaction media production.</td>
                                        </tr>
                                        <tr class="hover:bg-orange-500/5 transition-colors">
                                            <td class="border themeable-border px-3 py-2 align-top"><strong class="orange-text">B</strong></td>
                                            <td class="border themeable-border px-3 py-2">What key attributes related to hand movements are included in the labeling data (JSON) of this dataset?</td>
                                            <td class="border themeable-border px-3 py-2">It includes hand joint 2D/3D coordinates, hand approach direction, number of fingers used, fingertip object contact points, and fingertip force data.</td>
                                        </tr>
                                        <tr class="hover:bg-orange-500/5 transition-colors">
                                            <td class="border themeable-border px-3 py-2 align-top"><strong class="purple-text">C</strong></td>
                                            <td class="border themeable-border px-3 py-2">What AI model is used for human hand movement classification, and how does it process data?</td>
                                            <td class="border themeable-border px-3 py-2">The ST-GCN model is used. It represents hand and arm joint data in graph form and classifies actions by learning connectivity between joints and temporal changes.</td>
                                        </tr>
                                        <tr class="hover:bg-orange-500/5 transition-colors">
                                            <td class="border themeable-border px-3 py-2 align-top"><strong class="blue-text">D</strong></td>
                                            <td class="border themeable-border px-3 py-2">How were lighting conditions set to ensure data diversity for hand movement data?</td>
                                            <td class="border themeable-border px-3 py-2">Smart lighting was used to adjust brightness and color, with a total of 10 different lighting conditions set for data collection.</td>
                                        </tr>
                                    </tbody>
                                </table>
                            </div>
                        </details>
                    </div>

                    <!-- Dataset 12 -->
                    <div id="dataset-12" class="mb-8">
                        <h3 class="text-2xl font-semibold themeable-heading mb-4 flex items-center">
                            <span class="teal-text mr-2">12.</span>
                            Service Robot Status and Operations Data
                        </h3>
                        <details class="mt-4">
                            <summary class="cursor-pointer font-semibold teal-text hover:text-orange-500 transition-colors">
                                üìù View QA Samples (4 pairs)
                            </summary>
                            <div class="mt-4 overflow-x-auto">
                                <table class="w-full border-collapse text-sm">
                                    <thead>
                                        <tr>
                                            <th class="border themeable-border px-3 py-2 text-left w-32">Type</th>
                                            <th class="border themeable-border px-3 py-2 text-left">Question</th>
                                            <th class="border themeable-border px-3 py-2 text-left">Answer</th>
                                        </tr>
                                    </thead>
                                    <tbody class="themeable-text-secondary">
                                        <tr class="hover:bg-orange-500/5 transition-colors">
                                            <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">A</strong></td>
                                            <td class="border themeable-border px-3 py-2">What is the purpose of building the 'Indoor Space Maintenance Service Robot Data'?</td>
                                            <td class="border themeable-border px-3 py-2">The purpose is to develop data analysis systems and learning models that can predict failures in advance and perform preemptive maintenance based on service robot status and operations data.</td>
                                        </tr>
                                        <tr class="hover:bg-orange-500/5 transition-colors">
                                            <td class="border themeable-border px-3 py-2 align-top"><strong class="orange-text">B</strong></td>
                                            <td class="border themeable-border px-3 py-2">What robot status information does the 'deviceData' object in this dataset's JSON files contain?</td>
                                            <td class="border themeable-border px-3 py-2">It includes the robot's main status, battery level, charging status, obstacle detection level, collision level, emergency stop button status, location information, and total operation information.</td>
                                        </tr>
                                        <tr class="hover:bg-orange-500/5 transition-colors">
                                            <td class="border themeable-border px-3 py-2 align-top"><strong class="purple-text">C</strong></td>
                                            <td class="border themeable-border px-3 py-2">What AI model is used for predicting robot operational status, and why?</td>
                                            <td class="border themeable-border px-3 py-2">The transformer-based 'LLama3.2-3B-instruct' model is used. It is optimized for processing sequence data to predict next states, and it balances performance and efficiency.</td>
                                        </tr>
                                        <tr class="hover:bg-orange-500/5 transition-colors">
                                            <td class="border themeable-border px-3 py-2 align-top"><strong class="blue-text">D</strong></td>
                                            <td class="border themeable-border px-3 py-2">What are the main tasks performed during the data refinement process?</td>
                                            <td class="border themeable-border px-3 py-2">Source data format definition and field refinement, data classification by robot ID, filtering of non-operational hours data, mapping, and JSON conversion are performed.</td>
                                        </tr>
                                    </tbody>
                                </table>
                            </div>
                        </details>
                    </div>

                    <!-- Dataset 13 -->
                    <div id="dataset-13" class="mb-8">
                        <h3 class="text-2xl font-semibold themeable-heading mb-4 flex items-center">
                            <span class="teal-text mr-2">13.</span>
                            Robot Error and Preventive Maintenance Data
                        </h3>
                        <details class="mt-4">
                            <summary class="cursor-pointer font-semibold teal-text hover:text-orange-500 transition-colors">
                                üìù View QA Samples (4 pairs)
                            </summary>
                            <div class="mt-4 overflow-x-auto">
                                <table class="w-full border-collapse text-sm">
                                    <thead>
                                        <tr>
                                            <th class="border themeable-border px-3 py-2 text-left w-32">Type</th>
                                            <th class="border themeable-border px-3 py-2 text-left">Question</th>
                                            <th class="border themeable-border px-3 py-2 text-left">Answer</th>
                                        </tr>
                                    </thead>
                                    <tbody class="themeable-text-secondary">
                                        <tr class="hover:bg-orange-500/5 transition-colors">
                                            <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">A</strong></td>
                                            <td class="border themeable-border px-3 py-2">What value does robot error and preventive maintenance data provide to service robot operating companies?</td>
                                            <td class="border themeable-border px-3 py-2">It is used for developing AI algorithms that analyze causes of robot abnormal states and predict failures in advance, minimizing robot downtime and maximizing operational efficiency.</td>
                                        </tr>
                                        <tr class="hover:bg-orange-500/5 transition-colors">
                                            <td class="border themeable-border px-3 py-2 align-top"><strong class="orange-text">B</strong></td>
                                            <td class="border themeable-border px-3 py-2">How are robot abnormal states classified by error codes?</td>
                                            <td class="border themeable-border px-3 py-2">They are classified into 8 categories: obstacle error, collision detection error, battery error, emergency stop error, elevator error, automatic door error, network error, and robot SW error.</td>
                                        </tr>
                                        <tr class="hover:bg-orange-500/5 transition-colors">
                                            <td class="border themeable-border px-3 py-2 align-top"><strong class="purple-text">C</strong></td>
                                            <td class="border themeable-border px-3 py-2">What learning model was used to predict robot error occurrence, and what is the performance metric?</td>
                                            <td class="border themeable-border px-3 py-2">A Decision Tree model was used, and the performance metric is 'Accuracy'.</td>
                                        </tr>
                                        <tr class="hover:bg-orange-500/5 transition-colors">
                                            <td class="border themeable-border px-3 py-2 align-top"><strong class="blue-text">D</strong></td>
                                            <td class="border themeable-border px-3 py-2">What criteria determine a robot's normal/abnormal state during data labeling?</td>
                                            <td class="border themeable-border px-3 py-2">It is determined based on whether the robot is performing assigned tasks normally. If in an abnormal state, it is assigned to one of the 8 error categories for labeling.</td>
                                        </tr>
                                    </tbody>
                                </table>
                            </div>
                        </details>
                    </div>

                </section>

                <!-- Section 5: Statistics -->
                <section id="statistics" class="mb-12">
                    <h2 class="text-3xl font-bold themeable-heading mb-6 pb-3 border-b-2 themeable-border">
                        5. Final QA Type Statistics
                    </h2>

                    <p class="themeable-text-secondary mb-6">
                        A total of <strong class="orange-text">52</strong> question-answer pairs were constructed across
                        <strong class="teal-text">13</strong> robot data groups for LLM training data generation.
                        The query type statistics reflecting the Physical AI characteristics of robot intelligence are as follows.
                    </p>

                    <div class="overflow-x-auto mb-8">
                        <table class="w-full border-collapse">
                            <thead>
                                <tr class="bg-slate-800/80">
                                    <th class="border themeable-border px-4 py-3 text-left themeable-text-primary">Query Type</th>
                                    <th class="border themeable-border px-4 py-3 text-left themeable-text-primary">Definition</th>
                                    <th class="border themeable-border px-4 py-3 text-center themeable-text-primary">Count</th>
                                    <th class="border themeable-border px-4 py-3 text-center themeable-text-primary">Ratio</th>
                                </tr>
                            </thead>
                            <tbody class="themeable-text-secondary">
                                <tr class="hover:bg-orange-500/5 transition-colors">
                                    <td class="border themeable-border px-4 py-3"><strong class="teal-text">A. Domain Definition/Purpose</strong></td>
                                    <td class="border themeable-border px-4 py-3">Business value, utilization goals, problems to solve</td>
                                    <td class="border themeable-border px-4 py-3 text-center font-bold">13</td>
                                    <td class="border themeable-border px-4 py-3 text-center font-bold">25.0%</td>
                                </tr>
                                <tr class="hover:bg-orange-500/5 transition-colors">
                                    <td class="border themeable-border px-4 py-3"><strong class="orange-text">B. Data Structure/Composition</strong></td>
                                    <td class="border themeable-border px-4 py-3">File formats, metadata structure, data schema</td>
                                    <td class="border themeable-border px-4 py-3 text-center font-bold">13</td>
                                    <td class="border themeable-border px-4 py-3 text-center font-bold">25.0%</td>
                                </tr>
                                <tr class="hover:bg-orange-500/5 transition-colors">
                                    <td class="border themeable-border px-4 py-3"><strong class="purple-text">C. AI Model/Task</strong></td>
                                    <td class="border themeable-border px-4 py-3">Algorithms used, performance metrics, learning tasks</td>
                                    <td class="border themeable-border px-4 py-3 text-center font-bold">13</td>
                                    <td class="border themeable-border px-4 py-3 text-center font-bold">25.0%</td>
                                </tr>
                                <tr class="hover:bg-orange-500/5 transition-colors">
                                    <td class="border themeable-border px-4 py-3"><strong class="blue-text">D. Quality/Process Management</strong></td>
                                    <td class="border themeable-border px-4 py-3">Data collection, labeling, inspection processes</td>
                                    <td class="border themeable-border px-4 py-3 text-center font-bold">13</td>
                                    <td class="border themeable-border px-4 py-3 text-center font-bold">25.0%</td>
                                </tr>
                                <tr class="bg-orange-100/50">
                                    <td class="border themeable-border px-4 py-3 font-bold themeable-text-primary" colspan="2">Total</td>
                                    <td class="border themeable-border px-4 py-3 text-center font-bold orange-text">52</td>
                                    <td class="border themeable-border px-4 py-3 text-center font-bold orange-text">100.0%</td>
                                </tr>
                            </tbody>
                        </table>
                    </div>

                    <div class="interactive-card border border-teal-500/40 rounded-lg p-6">
                        <h4 class="font-semibold teal-text mb-3">üí° Key Features</h4>
                        <p class="themeable-text-secondary">
                            Questions are <strong class="orange-text">evenly distributed</strong> across the 4 core aspects of robot datasets --
                            <strong>Domain Definition/Purpose, Data Structure/Composition, AI Model/Task, and Quality/Process Management</strong> --
                            designed so that LLMs learn <strong class="teal-text">balanced expertise</strong> across all domains.
                            This enables development into a systematic and practical AI data scientist from a data quality perspective.
                        </p>
                    </div>
                </section>

                <!-- Section 6: Prompt Template -->
                <section id="prompt-template" class="mb-12">
                    <h2 class="text-3xl font-bold themeable-heading mb-6 pb-3 border-b-2 themeable-border">
                        6. Prompt Template for Domain LLM Report Generation
                    </h2>

                    <p class="themeable-text-secondary mb-6">
                        This prompt can be used to generate <strong class="teal-text">structured QA dataset reports</strong>
                        when given training data documents from other domains (e.g., manufacturing, healthcare, autonomous driving),
                        enabling LLMs to learn the specialized knowledge of those domains.
                    </p>

                    <div class="interactive-card border themeable-border rounded-lg p-6">
                        <h3 class="text-lg font-bold themeable-heading mb-4">Report Generation Prompt Template</h3>

                        <div class="bg-slate-900 text-slate-100 rounded-lg p-6 overflow-x-auto mb-4">
                            <pre class="text-sm leading-relaxed"><code>[Instructions]
You are an expert in building specialized QA datasets for Large Language Model (LLM) fine-tuning
as part of the Agentic AI Data Scientist (AADS) project. Analyze the content of the
[INPUT: Document for Analysis] presented below and generate a QA report organized by
**'Logical Data Groups'**.

**[Report Components]**
1. **Report Title:** Write according to domain and purpose.
2. **Logical Data Group Identification:** Group documents sharing the same project or goal
   into a single 'logical group'.
3. **QA Pair Generation:** Generate **4** question-answer (QA) pairs for each logical group.
4. **Query Type Classification:** Generated QA pairs must be classified into one of the following 4 core types:
   * **A. Domain Definition/Purpose:** Business value, utilization goals, industry problems to solve
   * **B. Data Structure/Composition:** File formats, metadata structure, data schema and logical layout
   * **C. AI Model/Task:** Algorithms used, performance metrics, learning tasks and technical strategy
   * **D. Quality/Process Management:** Data collection, labeling, inspection processes and quality assurance standards
5. **Source Citation:** All sentences in answers must clearly cite the source document.
6. **Final Statistics:** Summarize the **final count and ratio of all 4 types (A, B, C, D)** used
   across all generated QA pairs.</code></pre>
                        </div>

                        <p class="text-sm themeable-text-muted">
                            <strong>Usage:</strong> Use this template to automatically generate high-quality QA datasets
                            from training data documents across various domains (healthcare, autonomous driving, manufacturing, etc.).
                        </p>
                    </div>
                </section>

                <!-- Section 7: Pebblous Perspective -->
                <section id="pebblous-perspective" class="mb-12">
                    <h2 class="text-3xl font-bold themeable-heading mb-6 pb-3 border-b-2 themeable-border">
                        7. Pebblous Perspective: Data-Centric Approach for the Physical AI Era
                    </h2>

                    <div class="themeable-text-secondary space-y-6">
                        <p>
                            The robotics QA dataset built through this AADS initiative clearly presents
                            <strong class="orange-text">the systematic perspective an AI data scientist should possess</strong>,
                            centered on <strong class="teal-text">data quality evaluation</strong>.
                        </p>

                        <div class="interactive-card border border-teal-500/40 rounded-lg p-6 mb-6">
                            <h3 class="text-xl font-bold themeable-heading mb-4">üéØ A Differentiated Data Quality-Centric Approach</h3>
                            <ul class="space-y-3 themeable-text-secondary">
                                <li class="flex items-start">
                                    <span class="teal-text mr-2">‚Ä¢</span>
                                    <div>
                                        <strong class="themeable-text-primary">Balance across 4 quality aspects:</strong>
                                        Designed with 25% equal distribution across domain purpose, data structure, AI model, and quality management aspects,
                                        enabling LLMs to evaluate data quality from multiple angles
                                    </div>
                                </li>
                                <li class="flex items-start">
                                    <span class="teal-text mr-2">‚Ä¢</span>
                                    <div>
                                        <strong class="themeable-text-primary">Quality metric-focused QA:</strong>
                                        Covers specific performance metrics like mAP, mIoU, F1-score, RMSE, and Accuracy,
                                        along with multi-stage inspection processes to strengthen practical quality management capabilities
                                    </div>
                                </li>
                                <li class="flex items-start">
                                    <span class="teal-text mr-2">‚Ä¢</span>
                                    <div>
                                        <strong class="themeable-text-primary">Data lifecycle understanding:</strong>
                                        Systematically learns quality management touchpoints across the entire
                                        collection-refinement-labeling-inspection pipeline to build data quality diagnostic capabilities
                                    </div>
                                </li>
                            </ul>
                        </div>

                        <div class="interactive-card border border-orange-500/40 rounded-lg p-6">
                            <h3 class="text-xl font-bold themeable-heading mb-4">üöÄ Core of Data Quality Evaluation: Multi-Layered Verification</h3>
                            <p class="themeable-text-secondary mb-3">
                                Robot dataset quality requires multi-layered verification across
                                <strong>sensor synchronization, labeling consistency, data diversity, and performance metrics</strong>.
                            </p>
                            <p class="themeable-text-secondary">
                                AADS has systematized each dataset's quality management processes, inspection criteria, and performance measurement methods
                                through <strong class="teal-text">Type D: Quality/Process Management QA</strong>,
                                designed so that LLMs go beyond simply describing data to diagnose
                                <strong class="orange-text">"Is this data quality sufficient? How can it be improved?"</strong>
                            </p>
                        </div>
                    </div>
                </section>

                <!-- Section 8: FAQ -->
                <section id="faq" class="mb-12">
                    <h2 class="text-3xl font-bold themeable-heading mb-6 pb-3 border-b-2 themeable-border">
                        8. Frequently Asked Questions (FAQ)
                    </h2>

                    <div class="space-y-4">
                        <div class="interactive-card border themeable-border rounded-lg p-6">
                            <h3 class="text-xl font-semibold themeable-heading mb-3">
                                How does AADS build robotics QA datasets?
                            </h3>
                            <p class="themeable-text-secondary">
                                AADS analyzes robot dataset documents from AI Hub and groups documents sharing
                                the same project goals into 'logical data groups'. For each group, it generates
                                4 QA pairs (one from each of the 4 query types), resulting in 52 high-quality
                                question-answer pairs.
                            </p>
                        </div>

                        <div class="interactive-card border themeable-border rounded-lg p-6">
                            <h3 class="text-xl font-semibold themeable-heading mb-3">
                                How do robot datasets differ from manufacturing datasets?
                            </h3>
                            <p class="themeable-text-secondary">
                                Robot datasets are characterized by <strong>sensor diversity</strong> (RGB-D, LiDAR, tactile, IMU, etc.)
                                and <strong>real-time decision-making</strong> requirements.
                                In contrast, manufacturing datasets focus on quality inspection and predictive maintenance.
                                AADS builds QA datasets reflecting the characteristics of each domain.
                            </p>
                        </div>

                        <div class="interactive-card border themeable-border rounded-lg p-6">
                            <h3 class="text-xl font-semibold themeable-heading mb-3">
                                What are the most important robot datasets?
                            </h3>
                            <p class="themeable-text-secondary">
                                The <strong>Occluded Object Reasoning dataset</strong> and the <strong>Delivery Robot Off-Road Driving dataset</strong>
                                are critical. The former is essential for verifying robot perception capabilities (occlusion handling),
                                while the latter validates driving capabilities (off-road environments).
                                Both datasets require multimodal sensor fusion.
                            </p>
                        </div>

                        <div class="interactive-card border themeable-border rounded-lg p-6">
                            <h3 class="text-xl font-semibold themeable-heading mb-3">
                                What advantages does multimodal data provide for LLM fine-tuning?
                            </h3>
                            <p class="themeable-text-secondary">
                                Multimodal data (sensor values + images, LiDAR + RGB) helps LLMs
                                <strong>comprehensively understand the physical phenomena</strong> of robots.
                                For example, delivery robot data combines 2D images (drivable area) and 3D LiDAR (dynamic object detection),
                                enabling LLMs to learn multidimensional knowledge such as
                                "speed should be reduced on uneven sidewalks."
                            </p>
                        </div>

                        <div class="interactive-card border themeable-border rounded-lg p-6">
                            <h3 class="text-xl font-semibold themeable-heading mb-3">
                                How does the AADS QA dataset integrate with DataClinic?
                            </h3>
                            <p class="themeable-text-secondary">
                                The QA datasets generated by AADS are closely integrated with <strong>DataClinic's data quality diagnostic pipeline</strong>.
                                When DataClinic automatically detects sensor data outliers, labeling errors, or imbalanced distributions,
                                AADS can learn QA pairs about those quality issues and suggest
                                <strong>practical solutions</strong> such as "a specific filter should be applied to reduce noise in this LiDAR data."
                            </p>
                        </div>
                    </div>
                </section>

                <!-- Section 8: Dataset Sources -->
                <section id="datasets-sources" class="mb-12">
                    <h2 class="text-3xl font-bold themeable-heading mb-6 pb-3 border-b-2 themeable-border">
                        9. Related Dataset Sources
                    </h2>

                    <p class="themeable-text-secondary mb-6">
                        Source information for the 13 robotics datasets analyzed in this report.
                    </p>

                    <ol class="space-y-3 text-sm themeable-text-secondary">
                        <li class="flex items-start">
                            <span class="font-bold teal-text mr-3 min-w-[2rem]">[1]</span>
                            <div>
                                <strong>3D Scanned Object Data</strong><br>
                                <span class="text-xs themeable-text-muted">
                                    mAP performance metric / 200,000 RGB-D images / Label accuracy verification
                                </span>
                            </div>
                        </li>
                        <li class="flex items-start">
                            <span class="font-bold teal-text mr-3 min-w-[2rem]">[2]</span>
                            <div>
                                <strong>Multi-Object Occlusion Environment Data</strong><br>
                                <span class="text-xs themeable-text-muted">
                                    mAP-based quality evaluation / 3-stage inspection (cross-validation, supervisor review, final inspection)
                                </span>
                            </div>
                        </li>
                        <li class="flex items-start">
                            <span class="font-bold teal-text mr-3 min-w-[2rem]">[3]</span>
                            <div>
                                <strong>6D Object Pose Estimation Data</strong><br>
                                <span class="text-xs themeable-text-muted">
                                    100+ object types / Point Cloud + RGB-D / 6D Pose Annotation quality verification
                                </span>
                            </div>
                        </li>
                        <li class="flex items-start">
                            <span class="font-bold teal-text mr-3 min-w-[2rem]">[4]</span>
                            <div>
                                <strong>Robot-Object Grasping Data</strong><br>
                                <span class="text-xs themeable-text-muted">
                                    F1-score above 0.9 / Force sensor data / Diverse grasping scenarios
                                </span>
                            </div>
                        </li>
                        <li class="flex items-start">
                            <span class="font-bold teal-text mr-3 min-w-[2rem]">[5]</span>
                            <div>
                                <strong>Human-Object Grasping Data</strong><br>
                                <span class="text-xs themeable-text-muted">
                                    1,000+ hours of hand motion data / 3D hand joint coordinates / Labeling consistency verification
                                </span>
                            </div>
                        </li>
                        <li class="flex items-start">
                            <span class="font-bold teal-text mr-3 min-w-[2rem]">[6]</span>
                            <div>
                                <strong>Robot Hand Object Property Data</strong><br>
                                <span class="text-xs themeable-text-muted">
                                    200 household items / Weight, material, texture classification / 10 lighting conditions
                                </span>
                            </div>
                        </li>
                        <li class="flex items-start">
                            <span class="font-bold teal-text mr-3 min-w-[2rem]">[7]</span>
                            <div>
                                <strong>Off-Road Driving Data</strong><br>
                                <span class="text-xs themeable-text-muted">
                                    50,000 images / Semantic Segmentation + 3D LiDAR / Clear/rain/snow conditions
                                </span>
                            </div>
                        </li>
                        <li class="flex items-start">
                            <span class="font-bold teal-text mr-3 min-w-[2rem]">[8]</span>
                            <div>
                                <strong>Indoor Multi-Use Facility Driving Data</strong><br>
                                <span class="text-xs themeable-text-muted">
                                    mIoU-based Segmentation quality evaluation / 3-stage inspection process
                                </span>
                            </div>
                        </li>
                        <li class="flex items-start">
                            <span class="font-bold teal-text mr-3 min-w-[2rem]">[9]</span>
                            <div>
                                <strong>SLAM and Path Estimation Data</strong><br>
                                <span class="text-xs themeable-text-muted">
                                    ROS bag files / Fast-LIO2 algorithm / Target RMSE within 0.2m
                                </span>
                            </div>
                        </li>
                        <li class="flex items-start">
                            <span class="font-bold teal-text mr-3 min-w-[2rem]">[10]</span>
                            <div>
                                <strong>Human Activity Recognition Data</strong><br>
                                <span class="text-xs themeable-text-muted">
                                    10 service environments / Skeleton Keypoint / Accessibility-optimized UI
                                </span>
                            </div>
                        </li>
                        <li class="flex items-start">
                            <span class="font-bold teal-text mr-3 min-w-[2rem]">[11]</span>
                            <div>
                                <strong>Hand-Arm Coordinated Manipulation Data</strong><br>
                                <span class="text-xs themeable-text-muted">
                                    ST-GCN model / Hand joint 2D/3D coordinates / 10 lighting conditions
                                </span>
                            </div>
                        </li>
                        <li class="flex items-start">
                            <span class="font-bold teal-text mr-3 min-w-[2rem]">[12]</span>
                            <div>
                                <strong>Service Robot Status and Operation Data</strong><br>
                                <span class="text-xs themeable-text-muted">
                                    LLama3.2-3B-instruct model / deviceData JSON / Non-operating hours data filtering
                                </span>
                            </div>
                        </li>
                        <li class="flex items-start">
                            <span class="font-bold teal-text mr-3 min-w-[2rem]">[13]</span>
                            <div>
                                <strong>Robot Error and Preventive Maintenance Data</strong><br>
                                <span class="text-xs themeable-text-muted">
                                    Decision Tree model / 8 error code classifications / Accuracy-based performance evaluation
                                </span>
                            </div>
                        </li>
                    </ol>
                </section>

                <!-- Section 9: Conclusion -->
                <section id="conclusion" class="mb-12">
                    <h2 class="text-3xl font-bold themeable-heading mb-6 pb-3 border-b-2 themeable-border">
                        10. Conclusion
                    </h2>

                    <div class="themeable-text-secondary space-y-4 mb-6">
                        <p>
                            This report has detailed the process of systematically defining <strong class="teal-text">13 dataset groups</strong>
                            from diverse robotics technical documents and successfully building high-quality QA datasets for LLM fine-tuning
                            by applying <strong class="orange-text">4 standardized query types</strong>. Through this approach, we have structured
                            vast robotics technical knowledge and established consistent, reliable information assets that LLMs can learn from.
                        </p>
                        <p>
                            The strategic significance of this dataset is substantial. As a core asset of the <strong class="orange-text">AADS initiative</strong>,
                            it will serve as a cornerstone for developing specialized LLMs that deeply understand the complex contexts
                            and technical nuances of the robotics domain.
                            The constructed dataset goes beyond simple information retrieval ‚Äî it enables multi-faceted reasoning about
                            causes of robot anomalies, planning optimal grasping sequences in complex occlusion environments,
                            and predicting user behavior to provide proactive services,
                            making a decisive contribution to realizing a true <strong class="teal-text">Agentic AI Data Scientist</strong>.
                        </p>
                        <p>
                            As for future plans, we will proceed with large-scale expansion of the full dataset based on the QA samples
                            presented in this report. We plan to begin full-scale LLM fine-tuning experiments using the completed dataset
                            and validate performance in the robotics domain. Furthermore, through continuous data quality management
                            and model performance evaluation, we will progressively enhance the capabilities of AI specialized for the robotics domain.
                        </p>
                    </div>
                </section>

                <!-- PDF Download Section -->
                <section id="pdf-download" class="mb-12">
                    <h2 class="text-3xl font-bold themeable-heading mb-6 pb-3 border-b-2 themeable-border">
                        üìÑ Download Original Report
                    </h2>

                    <div class="themeable-bg card-hover rounded-lg p-8 text-center">
                        <div class="mb-4">
                            <svg class="w-16 h-16 mx-auto text-orange-500 mb-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                                <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z"></path>
                            </svg>
                            <h3 class="text-xl font-semibold themeable-heading mb-2">
                                Building QA Datasets for Robotics LLM Fine-Tuning: AADS
                            </h3>
                            <p class="themeable-text-secondary mb-4">
                                Download the original report containing detailed analysis of 13 datasets and 52 QA pairs in the robotics intelligence domain.
                            </p>
                            <p class="text-sm themeable-text-muted mb-6">
                                <strong class="teal-text">Includes all QA pairs from the web page</strong> along with additional analysis materials and source text.
                            </p>
                        </div>

                        <a href="/project/AADS/source/AADS LLM ÌååÏù∏ÌäúÎãùÏö© QA Îç∞Ïù¥ÌÑ∞ÏÖã Íµ¨Ï∂ï_ Î°úÎ¥á Î∂ÑÏïº Îç∞Ïù¥ÌÑ∞ÌíàÏßà Í¥ÄÏ†ê.pdf"
                           download="AADS_Robotics_QA_Dataset_Report_Data_Quality.pdf"
                           class="inline-flex items-center gap-2 bg-orange-500 hover:bg-orange-600 text-white font-semibold px-6 py-3 rounded-lg transition-all transform hover:scale-105 shadow-lg hover:shadow-xl">
                            <svg class="w-5 h-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                                <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M12 10v6m0 0l-3-3m3 3l3-3m2 8H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"></path>
                            </svg>
                            <span>Download PDF</span>
                        </a>

                        <div class="mt-6 text-sm themeable-text-muted">
                            <p>Format: PDF | Date: November 30, 2025 | Pebblous Data Communication Team</p>
                        </div>
                    </div>
                </section>

            </main>
        </div>
    </div>

    <!-- Footer will be loaded by common-utils.js -->
    <div id="footer-placeholder"></div>

    <!-- Scripts -->
    <script src="/scripts/common-utils.js"></script>
    <script>
    document.addEventListener('DOMContentLoaded', async function() {
        const config = {
            mainTitle: "Building QA Datasets for Robotics LLM Fine-Tuning: (2) Data Quality",
            subtitle: "AADS Physical AI Approach",
            pageTitle: "Building QA Datasets for Robotics LLM Fine-Tuning: (2) Data Quality - AADS Physical AI Approach | Pebblous",
            publishDate: "November 30, 2025",
            publisher: "Pebblous Data Communication Team",
            defaultTheme: "beige",
            category: "tech",
            articlePath: "project/AADS/robot-qa-dataset-2/en/index.html",
            tags: [
                "LLM ÌååÏù∏ÌäúÎãù", "LLM Fine-tuning", "QA Îç∞Ïù¥ÌÑ∞ÏÖã", "Question-Answer Dataset",
                "Î°úÎ¥á Î∂ÑÏïº", "Robotics AI", "Î°úÎ¥á Îç∞Ïù¥ÌÑ∞", "Robot Data", "AADS", "Agentic AI Data Scientist",
                "ÌîºÏßÄÏª¨ AI", "Physical AI", "Îç∞Ïù¥ÌÑ∞ ÌíàÏßà", "Data Quality",
                "Îç∞Ïù¥ÌÑ∞ Ï§ëÏã¨ AI", "Data-Centric AI", "Î©ÄÌã∞Î™®Îã¨ Îç∞Ïù¥ÌÑ∞", "Multimodal Data",
                "ÎèÑÎ©îÏù∏ ÏßÄÏãù", "Domain Knowledge", "Í∞ÄÎ†§ÏßÑ Í∞ùÏ≤¥ Ï∂îÎ°†", "Occluded Object Detection",
                "Î∞∞ÏÜ°Î°úÎ¥á", "Delivery Robot", "ÎπÑÎèÑÎ°ú Ïö¥Ìñâ", "Off-Road Navigation",
                "Ï£ºÌñâÏòÅÏÉÅ", "Driving Video", "Ïã§ÎÇ¥Í≥µÍ∞Ñ Ïú†ÏßÄÍ¥ÄÎ¶¨", "Indoor Maintenance",
                "ÏÑúÎπÑÏä§ Î°úÎ¥á", "Service Robot", "Í∞ùÏ≤¥ ÌäπÏÑ± ÏãùÎ≥Ñ", "Object Property Recognition",
                "Î°úÎ¥á Ìï∏Îìú", "Robot Hand", "ÌååÏßÄ-Ï°∞Ïûë ÎèôÏûë", "Grasp-Manipulation",
                "ÏÜê¬∑Ìåî ÌòëÏ°∞", "Hand-Arm Coordination", "ÏÇ¨Îûå ÌñâÎèô Ïù∏Ïãù", "Human Activity Recognition",
                "Î°úÎ¥á ÏûêÏú® ÌñâÎèô", "Robot Autonomous Behavior", "Few-Shot Learning", "Ìì®ÏÉ∑ Îü¨Îãù",
                "ÌîÑÎ°¨ÌîÑÌä∏ ÏóîÏßÄÎãàÏñ¥ÎßÅ", "Prompt Engineering", "ÎùºÎ≤®ÎßÅ", "Labeling",
                "Îç∞Ïù¥ÌÑ∞ Í≤ÄÏàò", "Data Validation", "mAP", "F1-score", "mIoU",
                "ÌéòÎ∏îÎü¨Ïä§", "Pebblous", "DataClinic", "Îç∞Ïù¥ÌÑ∞ÌÅ¥Î¶¨Îãâ"
            ],
            faqs: [
                {
                    question: "What do the 4 query types (A, B, C, D) each mean?",
                    answer: "Type A is 'Domain Definition/Purpose,' covering the business value and utilization goals of datasets. Type B is 'Data Structure/Composition,' describing file formats and metadata structures. Type C is 'AI Model/Task,' covering algorithms used and performance metrics. Type D is 'Quality/Process Management,' describing data collection, labeling, and inspection processes."
                },
                {
                    question: "Why is multi-stage inspection important in data quality management?",
                    answer: "Robot data requires high accuracy, so labeling consistency and accuracy are ensured through 3-stage inspection: cross-validation between workers (Stage 1), supervisor visual confirmation and error reassignment (Stage 2), and final inspection (Stage 3). This is essential for improving AI model reliability."
                },
                {
                    question: "How is time synchronization of multi-sensor data achieved?",
                    answer: "Data from multiple sensors such as RGB-D, LiDAR, and IMU is aligned through software synchronization of timestamps and stored in ROS bag file format to maintain time-series data consistency. This is essential for accurate SLAM and path estimation."
                },
                {
                    question: "How is data diversity ensured in robot datasets?",
                    answer: "Data is collected by varying 10 lighting conditions, weather conditions (clear/rain/snow), diverse environments (transportation/medical/educational facilities), and multiple objects and scenarios. This improves AI model generalization performance and ensures robust performance in real operational environments."
                },
                {
                    question: "How are AI model performance metrics measured?",
                    answer: "Appropriate metrics are used for each dataset. Object detection uses mAP, segmentation uses mIoU, classification uses F1-score, SLAM uses RMSE (target within 0.2m), and robot error prediction uses Accuracy, applying quality standards suited to each task."
                },
                {
                    question: "What tasks are included in data refinement and preprocessing?",
                    answer: "This includes source data format definition and field refinement, data classification by robot ID, non-operating hours data filtering, mapping, and JSON conversion. These processes improve data consistency and usability."
                },
                {
                    question: "How does the AADS QA dataset contribute to data quality improvement?",
                    answer: "Through 4 query types, LLMs learn data structures, AI model performance metrics, and quality management processes, evolving into Agentic AI Data Scientists that can automatically provide quality diagnostics and improvement recommendations for robot datasets."
                }
            ]
        };

        await PebblousPage.init(config);
    });
    </script>
</body>
</html>
