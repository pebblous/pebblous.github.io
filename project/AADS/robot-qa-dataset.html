<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="author" content="Pebblous Data Communication Team">
    <meta name="language" content="Korean">
    <meta name="copyright" content="© 2025 Pebblous. All rights reserved.">
    <meta name="rating" content="general">
    <meta name="revisit-after" content="7 days">
    <meta name="distribution" content="global">
    <meta name="audience" content="AI Researchers, Data Scientists, Robotics Engineers, LLM Developers, Physical AI Researchers">
    <meta name="topic" content="LLM Fine-tuning, QA Dataset, Robotics AI, Physical AI, Data Quality">
    <meta http-equiv="content-language" content="ko">

    <!-- Google Tag Manager -->
    <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
    new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
    j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
    'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','GTM-57L9F58B');</script>
    <!-- End Google Tag Manager -->

    <!-- Favicon -->
    <link rel="icon" href="/image/favicon.ico" sizes="any">
    <link rel="icon" href="/image/Pebblous_BM_Orange_RGB.png" type="image/png">
    <link rel="apple-touch-icon" href="/image/Pebblous_BM_Orange_RGB.png">

    <!-- SEO Meta Tags -->
    <title id="page-title">로봇 분야 LLM 파인튜닝용 QA 데이터셋 구축: AADS의 피지컬 AI 접근법 | 페블러스</title>
    <meta id="meta-description" name="description" content="페블러스 AADS가 로봇 지능 분야의 13개 도메인(가려진 객체 추론, 배송로봇, 주행영상, 실내공간 유지관리, 객체 특성 식별 등)에서 구축한 52쌍 LLM 파인튜닝용 QA 데이터셋. 로봇 데이터 수집부터 AI 모델 학습, 품질 관리까지 아우르는 데이터 중심 Physical AI 접근법을 소개합니다.">
    <meta id="meta-keywords" name="keywords" content="LLM 파인튜닝, LLM Fine-tuning, QA 데이터셋, Question-Answer Dataset, 로봇 분야, Robotics AI, 로봇 데이터, Robot Data, AADS, Agentic AI Data Scientist, 피지컬 AI, Physical AI, 데이터 품질, Data Quality, 데이터 중심 AI, Data-Centric AI, 멀티모달 데이터, Multimodal Data, 도메인 지식, Domain Knowledge, 가려진 객체 추론, Occluded Object Detection, 배송로봇, Delivery Robot, 비도로 운행, Off-Road Navigation, 주행영상, Driving Video, 실내공간 유지관리, Indoor Maintenance, 서비스 로봇, Service Robot, 객체 특성 식별, Object Property Recognition, 로봇 핸드, Robot Hand, 파지-조작 동작, Grasp-Manipulation, 손·팔 협조, Hand-Arm Coordination, 사람 행동 인식, Human Activity Recognition, 로봇 자율 행동, Robot Autonomous Behavior, Few-Shot Learning, 퓨샷 러닝, 프롬프트 엔지니어링, Prompt Engineering, 라벨링, Labeling, 데이터 검수, Data Validation, mAP, F1-score, mIoU, 페블러스, Pebblous, DataClinic, 데이터클리닉">
    <meta name="robots" content="index, follow">

    <link id="hreflang-ko" rel="alternate" hreflang="ko" href="https://blog.pebblous.ai/project/AADS/robot-qa-dataset.html">
    <link id="hreflang-en" rel="alternate" hreflang="en" href="https://blog.pebblous.ai/project/AADS/robot-qa-dataset.html">
    <link id="hreflang-default" rel="alternate" hreflang="x-default" href="https://blog.pebblous.ai/project/AADS/robot-qa-dataset.html">

    <link id="canonical-url" rel="canonical" href="https://blog.pebblous.ai/project/AADS/robot-qa-dataset.html">

    <meta id="og-url" property="og:url" content="https://blog.pebblous.ai/project/AADS/robot-qa-dataset.html">
    <meta id="og-title" property="og:title" content="로봇 분야 LLM 파인튜닝용 QA 데이터셋 구축: AADS의 피지컬 AI 접근법 | 페블러스">
    <meta id="og-description" property="og:description" content="13개 로봇 지능 도메인에서 52쌍의 고품질 QA 데이터셋을 구축한 AADS의 실무 사례. 도메인 정의, 데이터 구조, AI 모델, 품질 관리를 균형있게 다룬 데이터 중심 Physical AI 전략을 공개합니다.">
    <meta id="og-image" property="og:image" content="https://blog.pebblous.ai/image/Pebblous_BM_Orange_RGB.png">
    <meta property="og:image:width" content="1200">
    <meta property="og:image:height" content="630">
    <meta property="og:image:alt" content="AADS 로봇 분야 LLM 파인튜닝 QA 데이터셋 - 페블러스">
    <meta property="og:type" content="article">
    <meta property="og:site_name" content="Pebblous Blog">
    <meta property="og:locale" content="ko_KR">
    <meta property="article:published_time" content="2025-11-30T09:00:00+09:00">
    <meta property="article:modified_time" content="2025-11-30T09:00:00+09:00">
    <meta property="article:author" content="Pebblous Data Communication Team">
    <meta property="article:section" content="Technology">
    <meta property="article:tag" content="LLM Fine-tuning">
    <meta property="article:tag" content="Robotics AI">
    <meta property="article:tag" content="Physical AI">
    <meta property="article:tag" content="AADS">

    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:site" content="@Pebblous">
    <meta name="twitter:creator" content="@pebblous">
    <meta name="twitter:title" content="로봇 분야 LLM 파인튜닝용 QA 데이터셋 구축: AADS의 피지컬 AI 접근법">
    <meta name="twitter:description" content="13개 로봇 지능 도메인에서 52쌍의 고품질 QA 데이터셋을 구축한 AADS의 실무 사례. 도메인 정의, 데이터 구조, AI 모델, 품질 관리를 균형있게 다룬 데이터 중심 Physical AI 전략.">
    <meta name="twitter:image" content="https://blog.pebblous.ai/image/Pebblous_BM_Orange_RGB.png">
    <meta name="twitter:image:alt" content="AADS 로봇 분야 LLM 파인튜닝 QA 데이터셋">
    <meta name="twitter:label1" content="읽는 시간">
    <meta name="twitter:data1" content="15분">
    <meta name="twitter:label2" content="난이도">
    <meta name="twitter:data2" content="중급">

    <!-- JSON-LD Structured Data for SEO -->
    <script type="application/ld+json">
    {
        "@context": "https://schema.org",
        "@type": "TechArticle",
        "headline": "로봇 분야 LLM 파인튜닝용 QA 데이터셋 구축: AADS의 피지컬 AI 접근법",
        "alternativeHeadline": "13개 로봇 지능 도메인에서 52쌍의 고품질 QA 데이터셋을 구축한 데이터 중심 Physical AI 전략",
        "description": "페블러스 AADS가 로봇 지능 분야의 13개 도메인(가려진 객체 추론, 배송로봇, 주행영상, 실내공간 유지관리, 객체 특성 식별 등)에서 구축한 52쌍 LLM 파인튜닝용 QA 데이터셋. 로봇 데이터 수집부터 AI 모델 학습, 품질 관리까지 아우르는 데이터 중심 Physical AI 접근법을 소개합니다.",
        "image": {
            "@type": "ImageObject",
            "url": "https://blog.pebblous.ai/image/Pebblous_BM_Orange_RGB.png",
            "width": 1200,
            "height": 630
        },
        "author": {
            "@type": "Organization",
            "name": "Pebblous",
            "url": "https://www.pebblous.ai",
            "logo": {
                "@type": "ImageObject",
                "url": "https://www.pebblous.ai/image/Pebblous_BM_Orange_RGB.png"
            },
            "description": "페블러스는 AI-Ready Data 솔루션을 제공하는 딥테크 기업입니다."
        },
        "publisher": {
            "@type": "Organization",
            "name": "Pebblous",
            "url": "https://www.pebblous.ai",
            "logo": {
                "@type": "ImageObject",
                "url": "https://www.pebblous.ai/image/Pebblous_BM_Orange_RGB.png",
                "width": 600,
                "height": 60
            },
            "sameAs": [
                "https://www.linkedin.com/company/pebblous",
                "https://github.com/pebblous"
            ]
        },
        "datePublished": "2025-11-30T09:00:00+09:00",
        "dateModified": "2025-11-30T09:00:00+09:00",
        "mainEntityOfPage": {
            "@type": "WebPage",
            "@id": "https://blog.pebblous.ai/project/AADS/robot-qa-dataset.html"
        },
        "keywords": "LLM 파인튜닝, QA 데이터셋, 로봇 분야, 로봇 데이터, AADS, 피지컬 AI, 데이터 품질, 데이터 중심 AI, 멀티모달 데이터, 도메인 지식, Robotics AI, Robot Data, Physical AI",
        "articleSection": "Technology",
        "inLanguage": "ko-KR",
        "isAccessibleForFree": true,
        "proficiencyLevel": "Intermediate"
    }
    </script>

    <!-- FAQ Schema is dynamically generated by common-utils.js from config.faqs -->

    <!-- Stylesheets -->
    <link rel="stylesheet" href="/styles/common-styles.css?v=20260107">
    <script src="https://cdn.tailwindcss.com"></script>

    <!-- Fonts -->
    <link rel="stylesheet" as="style" crossorigin
          href="https://cdn.jsdelivr.net/gh/orioncactus/pretendard@v1.3.9/dist/web/variable/pretendardvariable.min.css">

    <style>
        /* Theme Variables - Light as default */
        :root {
            --bg-primary: #F9FAFB;
            --bg-secondary: #F3F4F6;
            --bg-card: rgba(255, 255, 255, 0.95);
            --text-primary: #111827;
            --text-secondary: #4B5563;
            --text-muted: #6B7280;
            --heading-color: #111827;
            --border-color: #E5E7EB;
            --accent-color: #F86825;
            --teal-color: #0d9488;
        }

        [data-theme="dark"] {
            --bg-primary: #020617;
            --bg-secondary: #0f172a;
            --bg-card: rgba(30, 41, 59, 0.95);
            --text-primary: #F1F5F9;
            --text-secondary: #CBD5E1;
            --text-muted: #94A3B8;
            --heading-color: #F1F5F9;
            --border-color: #334155;
            --accent-color: #F86825;
            --teal-color: #14b8a6;
        }

        body {
            background-color: var(--bg-primary);
            color: var(--text-primary);
            font-family: 'Pretendard Variable', -apple-system, BlinkMacSystemFont, system-ui, Roboto, sans-serif;
            transition: background-color 0.3s ease, color 0.3s ease;
            line-height: 1.7;
        }

        .themeable-bg {
            background-color: var(--bg-card);
            transition: background-color 0.3s ease;
        }

        .themeable-text {
            color: var(--text-primary);
        }

        .themeable-text-secondary {
            color: var(--text-secondary);
        }

        .themeable-text-muted {
            color: var(--text-muted);
        }

        .themeable-heading {
            color: var(--heading-color);
        }

        .themeable-border {
            border-color: var(--border-color);
        }

        .orange-text {
            color: var(--accent-color);
        }

        .themeable-toc-border {
            border-color: var(--teal-color);
        }

        .teal-text {
            color: var(--teal-color);
        }

        .card-hover {
            transition: all 0.3s ease;
            border: 1px solid var(--border-color);
        }

        .card-hover:hover {
            border-color: var(--teal-color);
            box-shadow: 0 4px 12px rgba(20, 184, 166, 0.15);
        }

        /* Share buttons styling */
        .share-container {
            display: flex;
            align-items: center;
            justify-content: center;
            gap: 0.75rem;
            flex-wrap: wrap;
        }

        .share-label {
            font-size: 0.875rem;
            color: var(--text-secondary);
            font-weight: 500;
        }

        .share-btn {
            display: inline-flex;
            align-items: center;
            gap: 0.375rem;
            padding: 0.5rem 0.875rem;
            border-radius: 0.5rem;
            font-size: 0.875rem;
            font-weight: 500;
            transition: all 0.2s ease;
            border: 1px solid var(--border-color);
            background-color: var(--bg-card);
            color: var(--text-secondary);
            cursor: pointer;
        }

        .share-btn:hover {
            background-color: var(--accent-color);
            color: white;
            border-color: var(--accent-color);
            transform: translateY(-2px);
            box-shadow: 0 4px 8px rgba(248, 104, 37, 0.2);
        }

        .share-btn svg {
            width: 1rem;
            height: 1rem;
        }

        .share-btn.copied {
            background-color: var(--teal-color);
            color: white;
            border-color: var(--teal-color);
        }

        /* Table styling */
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 1.5rem 0;
            font-size: 0.9rem;
        }

        thead {
            background-color: rgba(248, 104, 37, 0.1);
        }

        th {
            padding: 1rem;
            text-align: left;
            font-weight: 600;
            color: var(--heading-color);
            border-bottom: 2px solid var(--border-color);
        }

        td {
            padding: 1rem;
            border-bottom: 1px solid var(--border-color);
            color: var(--text-secondary);
        }

        tbody tr:hover {
            background-color: rgba(248, 104, 37, 0.05);
        }

        /* Details/Summary styling */
        details {
            margin: 1rem 0;
            padding: 1rem;
            border: 1px solid var(--border-color);
            border-radius: 0.5rem;
            background-color: var(--bg-card);
        }

        details summary {
            cursor: pointer;
            font-weight: 600;
            color: var(--teal-color);
            padding: 0.5rem;
            user-select: none;
        }

        details summary:hover {
            color: var(--accent-color);
        }

        details[open] summary {
            margin-bottom: 1rem;
            padding-bottom: 0.5rem;
            border-bottom: 1px solid var(--border-color);
        }

        /* Code styling */
        code {
            background-color: rgba(248, 104, 37, 0.1);
            padding: 0.2rem 0.4rem;
            border-radius: 0.25rem;
            font-family: 'Courier New', monospace;
            font-size: 0.9em;
            color: var(--accent-color);
        }

        /* List styling */
        ul, ol {
            margin-left: 1.5rem;
            margin-bottom: 1.5rem;
        }

        li {
            margin-bottom: 0.75rem;
            line-height: 1.7;
        }

        /* Strong text */
        strong {
            font-weight: 600;
            color: var(--heading-color);
        }
    </style>
</head>
<body>
    <!-- Google Tag Manager (noscript) -->
    <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-57L9F58B"
    height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
    <!-- End Google Tag Manager (noscript) -->

    <!-- Header will be loaded by common-utils.js -->
    <div id="header-placeholder"></div>

    <!-- Main Content -->
    <div class="container mx-auto px-4 sm:px-6 lg:px-8 py-8 md:py-12 max-w-[1400px]">
        <div class="lg:flex lg:gap-8 lg:justify-center lg:items-start">

            <!-- TOC Sidebar -->
            <nav class="hidden lg:block lg:w-[240px] lg:shrink-0 sticky top-20 self-start">
                <h3 class="font-bold themeable-heading mb-4 text-lg">목차</h3>
                <ul id="toc-links" class="space-y-3 text-sm border-l-2 themeable-toc-border pl-4">
                    <li><a href="#intro" class="toc-link themeable-text-secondary hover:text-teal-500 transition-colors">서론 및 구축 목표</a></li>
                    <li><a href="#overview" class="toc-link themeable-text-secondary hover:text-teal-500 transition-colors">QA 데이터셋 개요</a></li>
                    <li><a href="#robot-datasets" class="toc-link themeable-text-secondary hover:text-teal-500 transition-colors">로봇 지능 데이터셋</a>
                        <ul class="ml-4 mt-2 space-y-2">
                            <li><a href="#dataset-1" class="toc-link themeable-text-secondary hover:text-teal-500 transition-colors text-xs">1. 가려진 객체 추론</a></li>
                            <li><a href="#dataset-2" class="toc-link themeable-text-secondary hover:text-teal-500 transition-colors text-xs">2. 배송로봇 비도로 운행</a></li>
                            <li><a href="#dataset-3" class="toc-link themeable-text-secondary hover:text-teal-500 transition-colors text-xs">3. 로봇 관점 주행 영상</a></li>
                            <li><a href="#dataset-4" class="toc-link themeable-text-secondary hover:text-teal-500 transition-colors text-xs">4. 실내공간 유지관리</a></li>
                            <li><a href="#dataset-5" class="toc-link themeable-text-secondary hover:text-teal-500 transition-colors text-xs">5. 로봇 핸드 객체 특성</a></li>
                            <li><a href="#dataset-6" class="toc-link themeable-text-secondary hover:text-teal-500 transition-colors text-xs">6. 손·팔 협조 파지-조작</a></li>
                            <li><a href="#dataset-7" class="toc-link themeable-text-secondary hover:text-teal-500 transition-colors text-xs">7. 사람 행동 인식</a></li>
                        </ul>
                    </li>
                    <li><a href="#statistics" class="toc-link themeable-text-secondary hover:text-teal-500 transition-colors">질의-응답 유형 통계</a></li>
                    <li><a href="#prompt-template" class="toc-link themeable-text-secondary hover:text-teal-500 transition-colors">프롬프트 템플릿</a></li>
                    <li><a href="#pebblous-perspective" class="toc-link themeable-text-secondary hover:text-teal-500 transition-colors">페블러스 관점</a></li>
                    <li><a href="#faq" class="toc-link themeable-text-secondary hover:text-teal-500 transition-colors">FAQ</a></li>
                    <li><a href="#datasets-sources" class="toc-link themeable-text-secondary hover:text-teal-500 transition-colors">데이터셋 출처</a></li>
                    <li><a href="#conclusion" class="toc-link themeable-text-secondary hover:text-teal-500 transition-colors">결론</a></li>
                    <li><a href="#pdf-download" class="toc-link themeable-text-secondary hover:text-teal-500 transition-colors">PDF 다운로드</a></li>
                </ul>
            </nav>

            <!-- Main Article -->
            <main class="max-w-[800px] px-4 sm:px-6">

                <header class="text-center mb-16">
                    <h1 id="page-h1-title" class="text-4xl md:text-5xl font-bold themeable-heading mb-6 leading-tight" style="line-height: 1.4;">
                        로봇 분야 LLM 파인튜닝용 QA 데이터셋 구축
                    </h1>

                    <!-- 발행 정보 -->
                    <div class="flex flex-wrap justify-center items-center gap-2 text-sm text-slate-400 mb-8">
                        <span id="publish-date">작성일: 2025년 11월 30일</span>
                        <span class="text-slate-600">|</span>
                        <span id="publisher">기획: 페블러스 데이터 커뮤니케이션 팀</span>
                        <span class="text-slate-600">|</span>
                        <span>읽는 시간: 약 15분</span>
                    </div>

                    <!-- 공유하기 -->
                    <div class="share-container mb-8">
                        <span class="share-label">공유하기:</span>
                        <button id="copy-url-btn" class="share-btn" title="URL 복사">
                            <svg fill="none" stroke="currentColor" viewBox="0 0 24 24">
                                <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M8 16H6a2 2 0 01-2-2V6a2 2 0 012-2h8a2 2 0 012 2v2m-6 12h8a2 2 0 002-2v-8a2 2 0 00-2-2h-8a2 2 0 00-2 2v8a2 2 0 002 2z"></path>
                            </svg>
                            <span>URL</span>
                        </button>
                        <button id="share-twitter-btn" class="share-btn" title="Twitter에 공유">
                            <svg fill="currentColor" viewBox="0 0 24 24">
                                <path d="M18.244 2.25h3.308l-7.227 8.26 8.502 11.24H16.17l-5.214-6.817L4.99 21.75H1.68l7.73-8.835L1.254 2.25H8.08l4.713 6.231zm-1.161 17.52h1.833L7.084 4.126H5.117z"></path>
                            </svg>
                        </button>
                        <button id="share-facebook-btn" class="share-btn" title="Facebook에 공유">
                            <svg fill="currentColor" viewBox="0 0 24 24">
                                <path d="M24 12.073c0-6.627-5.373-12-12-12s-12 5.373-12 12c0 5.99 4.388 10.954 10.125 11.854v-8.385H7.078v-3.47h3.047V9.43c0-3.007 1.792-4.669 4.533-4.669 1.312 0 2.686.235 2.686.235v2.953H15.83c-1.491 0-1.956.925-1.956 1.874v2.25h3.328l-.532 3.47h-2.796v8.385C19.612 23.027 24 18.062 24 12.073z"></path>
                            </svg>
                        </button>
                        <button id="share-linkedin-btn" class="share-btn" title="LinkedIn에 공유">
                            <svg fill="currentColor" viewBox="0 0 24 24">
                                <path d="M20.447 20.452h-3.554v-5.569c0-1.328-.027-3.037-1.852-3.037-1.853 0-2.136 1.445-2.136 2.939v5.667H9.351V9h3.414v1.561h.046c.477-.9 1.637-1.85 3.37-1.85 3.601 0 4.267 2.37 4.267 5.455v6.286zM5.337 7.433c-1.144 0-2.063-.926-2.063-2.065 0-1.138.92-2.063 2.063-2.063 1.14 0 2.064.925 2.064 2.063 0 1.139-.925 2.065-2.064 2.065zm1.782 13.019H3.555V9h3.564v11.452zM22.225 0H1.771C.792 0 0 .774 0 1.729v20.542C0 23.227.792 24 1.771 24h20.451C23.2 24 24 23.227 24 22.271V1.729C24 .774 23.2 0 22.222 0h.003z"></path>
                            </svg>
                        </button>
                    </div>
                </header>

                <!-- Section 1: 서론 및 구축 목표 -->
                <section id="intro" class="mb-12">
                    <h2 class="text-3xl font-bold themeable-heading mb-6 pb-3 border-b-2 themeable-border">
                        Ⅰ. 서론 및 구축 목표
                    </h2>

                    <div class="themeable-text-secondary space-y-4 mb-6">
                        <p>
                            이 보고서는 <strong class="teal-text">Agentic AI Data Scientist (AADS)</strong> 과제에서
                            대규모 언어 모델(LLM)의 <strong class="orange-text">로봇 지능 도메인 전문성 강화</strong>를 목표로,
                            실제 AI 허브 데이터셋 문서를 기반으로 구축한 <strong>52쌍의 QA(Question-Answer) 데이터셋</strong>을 소개합니다.
                        </p>
                        <p>
                            페블러스는 로봇 분야의 <strong class="orange-text">13개 데이터셋</strong>을 '논리적 데이터 그룹'으로 재구성하여,
                            각 그룹별로 <strong>4가지 유형(단순 정보 추출형, 요약 및 설명형, 비교 및 분석형, 추론 및 적용형)</strong>의
                            질의응답을 체계적으로 생성했습니다. 이를 통해 LLM이 로봇 데이터 수집부터 AI 모델 학습, 품질 관리까지
                            <strong class="teal-text">Physical AI의 전체 생명주기</strong>를 이해하도록 설계하였습니다.
                        </p>
                        <p>
                            본 데이터셋은 <strong>가려진 객체 추론, 배송로봇 비도로 운행, 주행영상, 실내공간 유지관리, 객체 특성 식별,
                            파지-조작 동작, 사람 행동 인식</strong> 등 로봇 지능의 핵심 영역을 망라하며,
                            Few-Shot Learning, 프롬프트 엔지니어링을 통해 실무 환경에서 즉시 활용 가능한 형태로 구성되었습니다.
                        </p>
                    </div>

                    <!-- PDF 안내 -->
                    <div class="interactive-card border border-orange-500/40 rounded-lg p-6 mb-6">
                        <h4 class="font-semibold orange-text mb-3">📄 원본 보고서 안내</h4>
                        <p class="themeable-text-secondary">
                            본 웹 페이지는 <strong class="teal-text">13개 로봇 데이터셋의 핵심 QA 52쌍</strong>을 모두 포함하고 있습니다.
                            더 상세한 분석 자료와 원문 텍스트를 원하시면 <a href="#pdf-download" class="orange-text hover:underline">하단의 PDF 보고서</a>를 다운로드하세요.
                        </p>
                    </div>
                </section>

                <!-- Section 1.5: QA 데이터셋 개요 -->
                <section id="overview" class="mb-12">
                    <h2 class="text-3xl font-bold themeable-heading mb-6 pb-3 border-b-2 themeable-border">
                        Ⅱ. QA 데이터셋 개요
                    </h2>

                    <div class="grid grid-cols-1 md:grid-cols-3 gap-6 mb-8">
                        <div class="stat-card border themeable-border rounded-lg p-6 text-center">
                            <h4 class="text-4xl font-bold orange-text mb-2">13</h4>
                            <p class="text-sm themeable-text-muted">로봇 데이터 그룹</p>
                        </div>
                        <div class="stat-card border themeable-border rounded-lg p-6 text-center">
                            <h4 class="text-4xl font-bold orange-text mb-2">52</h4>
                            <p class="text-sm themeable-text-muted">QA 쌍 (그룹당 4쌍)</p>
                        </div>
                        <div class="stat-card border themeable-border rounded-lg p-6 text-center">
                            <h4 class="text-4xl font-bold orange-text mb-2">25%</h4>
                            <p class="text-sm themeable-text-muted">각 유형 균등 배분</p>
                        </div>
                    </div>

                    <div class="themeable-text-secondary space-y-4">
                        <p>
                            총 <strong class="teal-text">13개의 로봇 데이터 그룹</strong>에 대해
                            <strong class="orange-text">52개의 QA 쌍</strong>을 구축했습니다.
                            각 그룹은 로봇의 인식, 주행, 조작, 유지보수 등 피지컬 AI의 핵심 기능을 포괄합니다.
                        </p>
                        <p>
                            QA 쌍은 <strong>단순 정보 추출형, 요약 및 설명형, 비교 및 분석형, 추론 및 적용형</strong>의
                            4가지 유형에 각각 <strong class="teal-text">25%씩 균등하게 배분</strong>되어,
                            LLM이 로봇 데이터 과학의 전 영역에 걸친 종합적인 지식을 학습하도록 설계되었습니다.
                        </p>
                    </div>
                </section>

                <!-- Section 3: 로봇 지능 데이터셋 기반 QA 구축 -->
                <section id="robot-datasets" class="mb-12">
                    <h2 class="text-3xl font-bold themeable-heading mb-6 pb-3 border-b-2 themeable-border">
                        Ⅲ. 로봇 지능 데이터셋 기반 QA 구축
                    </h2>

                    <div class="themeable-text-secondary space-y-4 mb-8">
                        <p>
                            로봇 지능 분야는 <strong class="orange-text">센서 데이터의 다양성</strong>과
                            <strong class="teal-text">실시간 의사결정 요구사항</strong>으로 인해 높은 데이터 품질이 필수적입니다.
                            AADS는 AI 허브에서 수집한 13개 로봇 데이터셋을 분석하여, 각 데이터셋의 특성에 맞는 4가지 유형의
                            QA 쌍을 생성했습니다.
                        </p>
                        <p>
                            각 데이터셋은 <strong>도메인 정의</strong>(데이터 수집 목적 및 구성),
                            <strong>데이터 구조</strong>(라벨링 방식 및 환경),
                            <strong>AI 모델</strong>(학습 알고리즘 및 응용),
                            <strong>품질 관리</strong>(검수 기준 및 성능 지표)의 4개 측면에서 체계적으로 문서화되었습니다.
                        </p>
                    </div>

                    <!-- Dataset 1: 가려진 객체 추론 데이터셋 -->
                    <div id="dataset-1" class="mb-8">
                        <h3 class="text-2xl font-semibold themeable-heading mb-4 flex items-center">
                            <span class="teal-text mr-2">1.</span>
                            가려진 객체 추론 데이터셋
                        </h3>

                        <!-- 1-1: 개요 및 구성 -->
                        <div class="themeable-bg card-hover rounded-lg p-6 mb-6">
                            <h4 class="text-xl font-semibold mb-4 teal-text">1-1. 개요 및 구성</h4>

                            <div class="themeable-text-secondary space-y-3 mb-4">
                                <p>
                                    로봇이 통제된 실험실 환경을 벗어나 실제 산업 및 생활 공간에서 임무를 수행하기 위해서는 예측 불가능한 복잡성에 대응하는 능력이 필수적입니다. 특히, 목표 객체가 다른 물체에 의해 일부 또는 전부가 가려지는 상황(Occlusion)은 로봇 비전 기술의 가장 큰 난제 중 하나입니다. '가려진 객체 추론 데이터셋'은 바로 이 문제를 해결하기 위한 핵심 데이터로, 로봇이 불완전한 시각 정보만으로도 객체의 전체 형태와 위치를 정확히 추론하는 능력을 학습시키는 데 전략적 중요성을 가집니다. 본 데이터셋은 물류 현장의 피킹(picking) 로봇부터 스마트 팩토리의 조립 로봇에 이르기까지, 정교한 물체 인식, 파지, 조작 능력 고도화에 직접적으로 기여하는 기반 기술의 초석입니다.
                                </p>
                            </div>

                            <details class="mt-4">
                                <summary class="cursor-pointer font-semibold teal-text hover:text-orange-500 transition-colors">
                                    📝 QA 샘플 보기 (4쌍)
                                </summary>
                                <div class="mt-4 overflow-x-auto">
                                    <table class="w-full border-collapse text-sm">
                                        <thead>
                                            <tr>
                                                <th class="border themeable-border px-3 py-2 text-left w-32">질문 유형</th>
                                                <th class="border themeable-border px-3 py-2 text-left">질문 (Question)</th>
                                                <th class="border themeable-border px-3 py-2 text-left">답변 (Answer)</th>
                                            </tr>
                                        </thead>
                                        <tbody class="themeable-text-secondary">
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">단순 정보 추출형</strong></td>
                                                <td class="border themeable-border px-3 py-2">'가려진 객체 추론 데이터' 구축 과제에서 데이터 수집을 담당한 주관기관과 참여기관은 어디인가?</td>
                                                <td class="border themeable-border px-3 py-2">주관기관은 '사람과숲'이며, 참여기관은 '광주과학기술원'과 '한알음정보'입니다.</td>
                                            </tr>
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">요약 및 설명형</strong></td>
                                                <td class="border themeable-border px-3 py-2">이 데이터셋의 폴더는 어떤 4가지 주요 데이터 유형으로 구성되어 있는지 설명해 주십시오.</td>
                                                <td class="border themeable-border px-3 py-2">데이터셋은 '물체 3D 스캔 데이터', '다수 물체 가림 데이터', '로봇-물체 파지 데이터', '사람-물체 파지 데이터'의 네 가지 주요 폴더로 구성되어 있습니다.</td>
                                            </tr>
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">비교 및 분석형</strong></td>
                                                <td class="border themeable-border px-3 py-2">데이터셋에 포함된 '산업' 카테고리의 객체들과 '물류' 카테고리의 객체들을 비교하고, 각 카테고리의 특징을 분석해 주십시오.</td>
                                                <td class="border themeable-border px-3 py-2">'산업' 카테고리는 '부품', '공구', '3D 프린팅 부품' 등 개별 단위의 기능성 객체들(예: 브라켓, 스패너, 기어박스)로 구성됩니다. 반면, '물류' 카테고리는 '아마존 물류', '카드보드형 포장물품', '박스형 포장물품' 등 포장 및 이송을 전제로 하는 상품화된 객체들(예: 크래용 박스, CPU 박스)이 중심을 이룹니다. 이는 산업 현장의 조립/정비와 물류 현장의 피킹/분류라는 각기 다른 로봇 적용 분야의 특성을 반영합니다.</td>
                                            </tr>
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">추론 및 적용형</strong></td>
                                                <td class="border themeable-border px-3 py-2">이 데이터셋의 파급효과로 언급된 '로봇팔의 물체 파지, 조작, 이송, 배치를 활용하기 위한 물류 현장 등 관련 산업 분야 활성화'를 달성하기 위해, 이 데이터가 구체적으로 어떤 문제를 해결하는 데 기여할 수 있는지 추론해 보십시오.</td>
                                                <td class="border themeable-border px-3 py-2">물류 현장에서는 다양한 크기와 형태의 상품들이 박스 안에 무작위로 쌓여 있거나 선반에 겹쳐 있는 경우가 많습니다. 이 데이터셋은 로봇이 가려진 상품의 전체 형태와 위치(6D Pose)를 정확히 추론하는 능력을 학습시켜, 목표 상품을 집기 위해 어떤 상품을 먼저 치워야 할지 파지 순서를 계획할 수 있게 합니다. 이를 통해 피킹(picking) 작업의 실패율을 줄이고 자동화 효율을 극대화하여 물류 산업 활성화에 직접적으로 기여할 수 있습니다.</td>
                                            </tr>
                                        </tbody>
                                    </table>
                                </div>
                            </details>

                            <p class="themeable-text-secondary mt-4 text-sm italic">
                                다음 섹션에서는 이처럼 중요한 데이터의 품질을 보증하는 데이터 수집 환경과 도구에 대해 더 자세히 살펴보겠습니다.
                            </p>
                        </div>

                        <!-- 1-2: 수집 환경 및 도구 -->
                        <div class="themeable-bg card-hover rounded-lg p-6 mb-6">
                            <h4 class="text-xl font-semibold mb-4 teal-text">1-2. 수집 환경 및 도구</h4>

                            <div class="themeable-text-secondary space-y-3 mb-4">
                                <p>
                                    인공지능 학습용 데이터셋의 가치는 데이터의 양뿐만 아니라, 그 품질과 재현성에 의해 결정됩니다. 특히 로봇 비전 모델의 일반화 성능을 확보하기 위해서는 데이터가 수집된 환경, 장소, 사용된 도구의 구체적인 명세가 매우 중요합니다. 이는 AI 모델이 특정 환경에 과적합(overfitting)되는 것을 방지하고, 다양한 실제 상황에서도 강건한 성능을 발휘하도록 만들기 때문입니다. 따라서 '가려진 객체 추론 데이터셋' 구축 시 사용된 표준화된 수집 환경과 정밀한 도구에 대한 정보는 데이터셋의 신뢰도를 높이는 핵심 요소입니다.
                                </p>
                            </div>

                            <details class="mt-4">
                                <summary class="cursor-pointer font-semibold teal-text hover:text-orange-500 transition-colors">
                                    📝 QA 샘플 보기 (4쌍)
                                </summary>
                                <div class="mt-4 overflow-x-auto">
                                    <table class="w-full border-collapse text-sm">
                                        <thead>
                                            <tr>
                                                <th class="border themeable-border px-3 py-2 text-left w-32">질문 유형</th>
                                                <th class="border themeable-border px-3 py-2 text-left">질문 (Question)</th>
                                                <th class="border themeable-border px-3 py-2 text-left">답변 (Answer)</th>
                                            </tr>
                                        </thead>
                                        <tbody class="themeable-text-secondary">
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">단순 정보 추출형</strong></td>
                                                <td class="border themeable-border px-3 py-2">데이터 수집에 사용된 3D 스캐너의 모델명은 무엇인가?</td>
                                                <td class="border themeable-border px-3 py-2">Artec 3D Leo 툴을 활용하여 RGB-D 물체 3D 스캔 원시데이터를 생성했습니다.</td>
                                            </tr>
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">요약 및 설명형</strong></td>
                                                <td class="border themeable-border px-3 py-2">'다수 물체 가림 데이터' 수집 환경의 주요 특징 3가지를 요약해 주십시오.</td>
                                                <td class="border themeable-border px-3 py-2">주요 특징은 다음과 같습니다. 첫째, 책상, 선반, 박스라는 3가지 대표적인 로봇 환경을 구성했습니다. 둘째, 각 환경별로 5개의 가구와 5개의 배경을 균등하게 활용하여 데이터 다양성을 확보했습니다. 셋째, 전 세계 연구자가 동일한 환경을 구축할 수 있도록 IKEA 가구를 활용했습니다.</td>
                                            </tr>
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">비교 및 분석형</strong></td>
                                                <td class="border themeable-border px-3 py-2">로봇-물체 파지 데이터 수집에 사용된 그리퍼들의 다양성을 분석하고, 이러한 선택이 데이터셋의 유용성에 어떤 긍정적 영향을 미치는지 설명해 주십시오.</td>
                                                <td class="border themeable-border px-3 py-2">데이터셋은 1지(1종), 2지(3종), 3지(2종), 4지(1종), 5지(1종) 그리퍼를 모두 포함하여 형태적 다양성을 확보했습니다. 특히 연구에 주로 활용되는 2지와 3지 그리퍼를 각각 3종, 2종씩 사용하여 데이터의 집중도를 높였습니다. 이러한 구성은 특정 그리퍼 형태에 과적합되지 않고 다양한 로봇 핸드에 적용 가능한 범용적인 파지 알고리즘 개발을 가능하게 하여 데이터셋의 실질적인 유용성을 크게 향상시킵니다.</td>
                                            </tr>
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">추론 및 적용형</strong></td>
                                                <td class="border themeable-border px-3 py-2">데이터 수집 시 다양한 조명, 카메라 시점, 종류 등을 활용했다고 언급되었습니다. 만약 이 데이터셋을 이용해 학습한 로봇을 조명이 어둡거나 그림자가 많은 실제 물류 창고에 배치한다면 어떤 성능을 기대할 수 있는지 추론해 보십시오.</td>
                                                <td class="border themeable-border px-3 py-2">소스 컨텍스트에 명시된 바와 같이 데이터는 다양한 조명 환경에서 취득되었습니다. 따라서 이를 학습한 로봇 비전 알고리즘은 조도 변화에 강건한(robust) 성능을 보일 것으로 기대됩니다. 어둡거나 특정 조명으로 인해 그림자가 많이 지는 실제 물류 창고 환경에서도 객체의 특징점을 더 안정적으로 추출하고 인식할 가능성이 높으며, 이는 곧바로 파지 성공률 향상으로 이어질 수 있습니다.</td>
                                            </tr>
                                        </tbody>
                                    </table>
                                </div>
                            </details>

                            <p class="themeable-text-secondary mt-4 text-sm italic">
                                이제, 체계적으로 수집된 이 데이터를 기반으로 어떤 학습 모델이 개발되고 응용될 수 있는지 구체적으로 알아보겠습니다.
                            </p>
                        </div>

                        <!-- 1-3: 학습 모델 및 응용 -->
                        <div class="themeable-bg card-hover rounded-lg p-6 mb-6">
                            <h4 class="text-xl font-semibold mb-4 teal-text">1-3. 학습 모델 및 응용</h4>

                            <div class="themeable-text-secondary space-y-3 mb-4">
                                <p>
                                    고품질 데이터셋의 최종 가치는 우수한 AI 모델을 통해 실제 산업 현장의 문제를 해결하는 데 기여할 때 발현됩니다. 데이터셋 자체만으로는 잠재력에 불과하며, 이를 해석하고 활용하는 학습 모델이 있어야만 그 가치가 실현될 수 있습니다. 이 섹션에서는 '가려진 객체 추론 데이터셋'의 유효성을 검증하고 그 활용 가능성을 극대화하기 위해 사용된 대표적인 AI 모델들을 분석합니다. 이를 통해 구현될 수 있는 구체적인 응용 서비스를 조명함으로써, 데이터가 어떻게 실용적인 기술로 전환되는지 탐구합니다.
                                </p>
                            </div>

                            <details class="mt-4">
                                <summary class="cursor-pointer font-semibold teal-text hover:text-orange-500 transition-colors">
                                    📝 QA 샘플 보기 (4쌍)
                                </summary>
                                <div class="mt-4 overflow-x-auto">
                                    <table class="w-full border-collapse text-sm">
                                        <thead>
                                            <tr>
                                                <th class="border themeable-border px-3 py-2 text-left w-32">질문 유형</th>
                                                <th class="border themeable-border px-3 py-2 text-left">질문 (Question)</th>
                                                <th class="border themeable-border px-3 py-2 text-left">답변 (Answer)</th>
                                            </tr>
                                        </thead>
                                        <tbody class="themeable-text-secondary">
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">단순 정보 추출형</strong></td>
                                                <td class="border themeable-border px-3 py-2">'아모달 인스턴스 분할'을 위해 사용된 모델 중, 2022년 기준 최고 성능 모델로 선정된 GIST 개발 모델의 이름은 무엇인가?</td>
                                                <td class="border themeable-border px-3 py-2">UOAIS-Net 입니다. 이 모델은 미학습 물체의 가려진 영역도 검출하기 위해 제안되었습니다.</td>
                                            </tr>
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">요약 및 설명형</strong></td>
                                                <td class="border themeable-border px-3 py-2">6D 물체 자세 예측(6D Object Pose Estimation) 모델인 'PVNet'이 가려진 환경에서 성능을 높이는 핵심 원리를 설명해 주십시오.</td>
                                                <td class="border themeable-border px-3 py-2">PVNet은 물체별 키포인트 벡터 필드(Keypoint Vector Field) 예측을 통해 성능을 높입니다. 즉, 이미지의 각 픽셀이 물체의 특정 키포인트를 향하는 방향 벡터를 예측하게 함으로써, 객체 일부가 가려져 보이지 않더라도 보이는 부분의 픽셀 투표(voting)를 통해 가려진 키포인트의 위치를 추정할 수 있어 가려진 물체의 자세 추정 성능을 향상시킵니다.</td>
                                            </tr>
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">비교 및 분석형</strong></td>
                                                <td class="border themeable-border px-3 py-2">인스턴스 분할 모델인 Mask R-CNN과 SOLOv2의 가장 큰 구조적 차이점은 무엇이며, 이 차이가 효율성에 어떤 영향을 미칩니까?</td>
                                                <td class="border themeable-border px-3 py-2">가장 큰 차이는 Mask R-CNN이 'two-stage' 구조인 반면, SOLOv2는 'single-stage' 구조라는 점입니다. 'Two-stage'인 Mask R-CNN은 물체의 영역 후보군을 먼저 추출하고, 그 후에 각 후보 영역별로 마스크를 예측합니다. 반면 'Single-stage'인 SOLOv2는 이 과정을 한 번에 처리합니다. 이러한 구조적 차이로 인해 SOLOv2는 two-stage 구조의 비효율성을 줄여 더 빠른 예측 속도를 가집니다.</td>
                                            </tr>
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">추론 및 적용형</strong></td>
                                                <td class="border themeable-border px-3 py-2">GIST AILAB의 '로봇의 가려진 물체 파지 순서 계획' 응용 서비스 예시를 바탕으로, 이 기술을 스마트팩토리의 부품 조립 라인에 적용한다면 어떤 방식으로 생산성을 향상시킬 수 있을지 추론해 보십시오.</td>
                                                <td class="border themeable-border px-3 py-2">부품 조립 라인에서는 다양한 부품들이 부품함에 겹쳐서 쌓여 있는 경우가 많습니다. 소스 컨텍스트에 제시된 기술을 적용하면, 로봇팔이 목표 부품이 다른 부품에 의해 가려져 있을 때, 가려진 영역과 가시 영역을 정확히 인식하고 목표 부품을 집기 위해 어떤 부품을 먼저 들어내야 하는지 스스로 판단하고 순서를 계획할 수 있습니다. 이는 사람이 개입하여 부품을 재정렬할 필요를 없애고, 로봇의 작업 대기 시간을 줄여 전체 조립 라인의 사이클 타임을 단축시킴으로써 생산성을 크게 향상시킬 수 있습니다.</td>
                                            </tr>
                                        </tbody>
                                    </table>
                                </div>
                            </details>

                            <p class="themeable-text-secondary mt-4 text-sm italic">
                                지금까지 물류 및 산업 환경에서의 객체 인식을 다루었다면, 다음 장에서는 또 다른 핵심 데이터셋인 '배송 로봇 비도로 운행 데이터'의 특징을 살펴보며 논의를 이어갑니다.
                            </p>
                        </div>
                    </div>

                    <!-- Dataset 2: 배송로봇 비도로 운행 데이터셋 -->
                    <div id="dataset-2" class="mb-8">
                        <h3 class="text-2xl font-semibold themeable-heading mb-4 flex items-center">
                            <span class="teal-text mr-2">2.</span>
                            배송로봇 비도로 운행 데이터셋
                        </h3>

                        <!-- 2-1: 개요 및 라벨링 -->
                        <div class="themeable-bg card-hover rounded-lg p-6 mb-6">
                            <h4 class="text-xl font-semibold mb-4 teal-text">2-1. 개요 및 라벨링</h4>

                            <div class="themeable-text-secondary space-y-3 mb-4">
                                <p>
                                    로봇의 자율주행 영역이 잘 정비된 '도로'를 넘어 보도, 공원, 실내 공간 등 예측 불가능한 요소가 많은 '비도로' 환경으로 확장됨에 따라, 새로운 유형의 데이터셋이 필수적으로 요구되고 있습니다. 비도로 환경은 불특정 다수의 보행자, 다양한 형태의 장애물, 불분명한 경계 등 기존 도로 주행 데이터로는 해결할 수 없는 복잡성을 내포하기 때문입니다. 이 섹션에서는 '배송로봇 비도로 운행 데이터셋'의 개요와 그 핵심 구성 요소인 2D 및 3D 데이터의 라벨링 클래스 정의를 분석합니다. 이를 통해 이 데이터셋이 비도로 환경의 복잡성을 AI가 구조적으로 이해하는 데 어떤 역할을 하는지 그 중요성을 기술합니다.
                                </p>
                            </div>

                            <details class="mt-4">
                                <summary class="cursor-pointer font-semibold teal-text hover:text-orange-500 transition-colors">
                                    📝 QA 샘플 보기 (4쌍)
                                </summary>
                                <div class="mt-4 overflow-x-auto">
                                    <table class="w-full border-collapse text-sm">
                                        <thead>
                                            <tr>
                                                <th class="border themeable-border px-3 py-2 text-left w-32">질문 유형</th>
                                                <th class="border themeable-border px-3 py-2 text-left">질문 (Question)</th>
                                                <th class="border themeable-border px-3 py-2 text-left">답변 (Answer)</th>
                                            </tr>
                                        </thead>
                                        <tbody class="themeable-text-secondary">
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">단순 정보 추출형</strong></td>
                                                <td class="border themeable-border px-3 py-2">3D LiDAR 데이터의 라벨링 타입은 무엇이며, 총 몇 종의 가공 대상 클래스가 정의되어 있는가?</td>
                                                <td class="border themeable-border px-3 py-2">라벨링 타입은 'Cuboid'이며, 가공 대상 클래스는 '승용차', '오토바이', '자전거', '킥보드', '보행자', '버스', '트럭', '기타 동적객체', '기타 정적객체'로 총 9종입니다.</td>
                                            </tr>
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">요약 및 설명형</strong></td>
                                                <td class="border themeable-border px-3 py-2">2D 이미지 데이터의 Semantic Segmentation 가공 원칙 중, 동일 객체가 다른 객체에 의해 가려지거나(영역 A), 돌발 행동이 예상되는 보행자가 가려진 경우(영역 B)는 각각 어떻게 처리되는지 설명해 주십시오.</td>
                                                <td class="border themeable-border px-3 py-2">영역 A의 경우, 동일 객체를 두 영역으로 나누어 각각 어노테이션 작업을 실시합니다. 영역 B의 경우, 안전사고 발생 위험이 있는 보행자에 대해 '가려짐(is_crowd)' 속성으로 처리하여, 로봇이 잠재적 위험을 인지하고 예방할 수 있도록 합니다.</td>
                                            </tr>
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">비교 및 분석형</strong></td>
                                                <td class="border themeable-border px-3 py-2">2D 가공 대상 클래스(22종)와 3D 가공 대상 클래스(9종)를 비교하고, 그 차이가 발생하는 이유를 센서의 특성과 연관 지어 분석해 주십시오.</td>
                                                <td class="border themeable-border px-3 py-2">2D 클래스는 '도로', '인도', '표지판', '신호등', '건물' 등 주변 환경과 배경에 대한 정보가 풍부한 반면, 3D 클래스는 '승용차', '보행자' 등 주로 동적 또는 정적 '객체'에 집중되어 있습니다. 이는 각 센서의 정보 수집 특성 때문입니다. 2D 이미지는 색상과 질감 정보를 통해 주행 가능 영역이나 표지판 같은 평면적 환경 정보를 인식하는 데 유리하고, 3D LiDAR는 거리와 형태 정보를 통해 입체적인 객체의 위치와 크기를 정확히 탐지하는 데 더 강점이 있습니다. 즉, 각 센서의 정보 수집 방식에 맞춰 라벨링 대상을 최적화한 결과입니다.</td>
                                            </tr>
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">추론 및 적용형</strong></td>
                                                <td class="border themeable-border px-3 py-2">라벨링 데이터의 JSON 구조에는 'sidewalk_flatness' (주행로 평탄도) 속성이 포함되어 있습니다. 이 속성 데이터를 배송 로봇이 어떻게 활용하여 더 안전하고 효율적인 주행을 할 수 있을지 추론해 보십시오.</td>
                                                <td class="border themeable-border px-3 py-2">배송 로봇은 'sidewalk_flatness' 데이터를 학습하여 평탄도가 'low'인 구간을 사전에 인지할 수 있습니다. 이를 통해 해당 구간에 진입하기 전에 주행 속도를 스스로 감속하거나, 진동을 최소화하는 주행 모드로 전환하여 배송 중인 물품의 파손 위험을 줄일 수 있습니다. 또한, 이 정보가 누적되면 가장 평탄한 경로를 우선적으로 선택하는 경로 계획 알고리즘을 고도화하여 장기적으로 로봇의 기구적 마모를 줄이고 유지보수 비용을 절감하는 데에도 기여할 수 있습니다.</td>
                                            </tr>
                                        </tbody>
                                    </table>
                                </div>
                            </details>

                            <p class="themeable-text-secondary mt-4 text-sm italic">
                                데이터의 품질은 수집 장소와 환경에 크게 좌우되므로, 다음 섹션에서는 이 데이터셋이 어디서 어떻게 수집되었는지 상세히 분석합니다.
                            </p>
                        </div>

                        <!-- 2-2: 수집 장소 및 환경 -->
                        <div class="themeable-bg card-hover rounded-lg p-6 mb-6">
                            <h4 class="text-xl font-semibold mb-4 teal-text">2-2. 수집 장소 및 환경</h4>

                            <div class="themeable-text-secondary space-y-3 mb-4">
                                <p>
                                    AI 모델이 특정 환경에서만 작동하는 '온실 속 화초'가 되지 않고, 다양한 실제 환경에서 범용적으로 활용되기 위해서는 데이터 수집 단계에서부터 포괄성과 현실성을 확보하는 것이 무엇보다 중요합니다. 데이터 수집 장소의 선택은 데이터셋의 다양성과 직결되며, 이는 최종적으로 AI 모델의 일반화 성능을 결정하는 핵심 변수입니다. 이 섹션에서는 '배송로봇 비도로 운행 데이터셋'이 수집된 구체적인 장소들의 환경적 특성을 분석하고, 이러한 장소 선정이 데이터셋의 현실성을 어떻게 담보하며 로봇이 마주할 실제 도전 과제들을 얼마나 잘 반영하고 있는지 그 전략적 의의를 평가합니다.
                                </p>
                            </div>

                            <details class="mt-4">
                                <summary class="cursor-pointer font-semibold teal-text hover:text-orange-500 transition-colors">
                                    📝 QA 샘플 보기 (4쌍)
                                </summary>
                                <div class="mt-4 overflow-x-auto">
                                    <table class="w-full border-collapse text-sm">
                                        <thead>
                                            <tr>
                                                <th class="border themeable-border px-3 py-2 text-left w-32">질문 유형</th>
                                                <th class="border themeable-border px-3 py-2 text-left">질문 (Question)</th>
                                                <th class="border themeable-border px-3 py-2 text-left">답변 (Answer)</th>
                                            </tr>
                                        </thead>
                                        <tbody class="themeable-text-secondary">
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">단순 정보 추출형</strong></td>
                                                <td class="border themeable-border px-3 py-2">데이터 수집 장소 중, 규제자유특구를 통과하여 자유로운 자율주행 운행이 가능했던 장소는 어디인가?</td>
                                                <td class="border themeable-border px-3 py-2">세종 중앙공원입니다.</td>
                                            </tr>
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">요약 및 설명형</strong></td>
                                                <td class="border themeable-border px-3 py-2">'북촌한옥마을'이 비도로 환경 주행 로봇을 검증하기에 최적의 장소로 여겨지는 이유는 무엇인지 두 가지 측면에서 설명해 주십시오.</td>
                                                <td class="border themeable-border px-3 py-2">첫째, 차량이 다닐 수 있는 좁은 도로와 차량 진입이 통제되는 골목 환경을 모두 포함하고 있어 다양한 비도로 조건을 테스트할 수 있습니다. 둘째, 주거지와 상업지가 공존하는 복합 문화유산 지역으로, 다양한 유형의 보행자와 정적/동적 장애물이 존재하는 현실적인 환경을 제공합니다.</td>
                                            </tr>
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">비교 및 분석형</strong></td>
                                                <td class="border themeable-border px-3 py-2">실외 수집 장소인 '인사동'과 실내 수집 장소인 '네이버 신사옥(1784)'의 환경적 특성을 비교하고, 각 장소가 로봇 자율주행 기술 검증에 있어 어떤 다른 과제를 제시하는지 분석해 주십시오.</td>
                                                <td class="border themeable-border px-3 py-2">'인사동'은 골목 중심으로 행인 밀도가 매우 높아, 불특정 다수의 보행자들 사이에서 예측 불가능한 움직임을 회피하고 안전하게 주행하는 동적 장애물 회피 기술 검증에 중점을 둡니다. 반면, '네이버 신사옥'은 실제 로봇 서비스가 운영되는 정제된 실내 공간으로, 좁은 복도, 유리벽, 실내 구조물 등을 정확히 인식하고 실내외 연속 주행 데이터셋을 구축하는 등 보다 정밀한 SLAM 및 위치 인식 기술의 검증에 적합합니다.</td>
                                            </tr>
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">추론 및 적용형</strong></td>
                                                <td class="border themeable-border px-3 py-2">이 데이터셋은 '여름'과 '가을'에 수집되었습니다. 만약 이 데이터로 학습한 로봇을 눈이 오는 '겨울'의 비도로 환경에서 운행한다면 어떤 잠재적인 문제를 겪을 수 있으며, 이를 해결하기 위해 어떤 추가 데이터가 필요할지 추론해 보십시오.</td>
                                                <td class="border themeable-border px-3 py-2">주어진 정보에 따르면 데이터는 여름과 가을에만 수집되었습니다. 따라서 겨울철에는 눈으로 인해 차선, 인도, 연석의 구분이 불분명해지고, LiDAR 센서가 눈을 장애물로 오인하거나 난반사로 인해 성능이 저하될 수 있습니다. 또한, 사람들의 보행 패턴(미끄러짐 등)도 달라집니다. 결과적으로, 현재 데이터셋만으로는 겨울철 주행 성능이 크게 저하될 수 있습니다. 이를 해결하기 위해서는 눈이 쌓인 환경, 눈이 내리는 환경, 빙판길 등 겨울철 특화 주행 데이터(이미지 및 LiDAR)와 그에 맞는 라벨링 데이터(예: '눈 덮인 인도', '빙판길' 클래스 추가)를 추가로 수집하여 모델을 재학습시켜야 합니다.</td>
                                            </tr>
                                        </tbody>
                                    </table>
                                </div>
                            </details>

                            <p class="themeable-text-secondary mt-4 text-sm italic">
                                다음으로, 이처럼 다양한 환경에서 수집된 데이터를 활용하여 개발 및 검증된 학습 모델과 그 응용 가능성을 탐구합니다.
                            </p>
                        </div>

                        <!-- 2-3: 학습 모델 및 응용 -->
                        <div class="themeable-bg card-hover rounded-lg p-6 mb-6">
                            <h4 class="text-xl font-semibold mb-4 teal-text">2-3. 학습 모델 및 응용</h4>

                            <div class="themeable-text-secondary space-y-3 mb-4">
                                <p>
                                    데이터셋의 실효성은 최종적으로 AI 모델의 성능과 실제 응용 가능성을 통해 입증됩니다. 아무리 방대하고 다양한 데이터를 구축하더라도, 이를 효과적으로 학습하고 활용할 수 있는 모델이 없다면 데이터는 단순한 저장 공간만 차지하게 됩니다. 본 섹션에서는 '배송로봇 비도로 운행 데이터셋'의 유효성을 검증하기 위해 선정된 2D 및 3D 학습 모델을 분석합니다. 각 모델의 선정 사유와 개발 내용을 통해 데이터의 기술적 가치를 평가하고, 이를 통해 실현될 수 있는 구체적인 응용 서비스를 조망하며 데이터의 산업적 가치를 평가합니다.
                                </p>
                            </div>

                            <details class="mt-4">
                                <summary class="cursor-pointer font-semibold teal-text hover:text-orange-500 transition-colors">
                                    📝 QA 샘플 보기 (4쌍)
                                </summary>
                                <div class="mt-4 overflow-x-auto">
                                    <table class="w-full border-collapse text-sm">
                                        <thead>
                                            <tr>
                                                <th class="border themeable-border px-3 py-2 text-left w-32">질문 유형</th>
                                                <th class="border themeable-border px-3 py-2 text-left">질문 (Question)</th>
                                                <th class="border themeable-border px-3 py-2 text-left">답변 (Answer)</th>
                                            </tr>
                                        </thead>
                                        <tbody class="themeable-text-secondary">
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">단순 정보 추출형</strong></td>
                                                <td class="border themeable-border px-3 py-2">3D 동적물체 탐지 모델 후보 중, 오픈 소스 코드의 성능 재현률이 떨어져 최종 선정되지 않은 모델의 이름은 무엇인가?</td>
                                                <td class="border themeable-border px-3 py-2">BtcDet 모델입니다.</td>
                                            </tr>
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">요약 및 설명형</strong></td>
                                                <td class="border themeable-border px-3 py-2">2D 주행로환경 인식 모델로 선정된 'ERF-PSPNet'의 개발 내용을 설명해 주십시오.</td>
                                                <td class="border themeable-border px-3 py-2">이 모델은 2차원 RGB 이미지를 입력받아, 로봇 자율주행과 관련된 도로(Road), 인도(Sidewalk), 연석(curb), 횡단보도(Crosswalk), 실내 바닥면(floor)에 대한 픽셀 단위(pixel-wise) 인식 결과를 출력하여 주행 가능 영역을 식별하는 역할을 합니다.</td>
                                            </tr>
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">비교 및 분석형</strong></td>
                                                <td class="border themeable-border px-3 py-2">3D 동적물체 탐지 모델로 'PointPillars'가 선정된 반면, 'VoxelRCNN'은 선정되지 않았습니다. 두 모델의 성능 차이를 추정할 수 있는 선정 및 미선정 사유를 비교 분석해 주십시오.</td>
                                                <td class="border themeable-border px-3 py-2">PointPillars는 KITTI 데이터셋에서 좋은 성능을 보이며 성능 재현 가능성이 높아 선정되었습니다. 이는 다양한 객체가 존재하는 환경에서도 안정적인 탐지 성능을 보임을 시사합니다. 반면, VoxelRCNN은 여러 물체를 동시에 탐지할 때 성능이 떨어지는 특성이 있어 선정되지 않았습니다. 이는 복잡한 비도로 환경에서 다수의 동적 객체(보행자, 자전거 등)를 동시에 처리해야 하는 배송 로봇의 임무에 부적합하다고 판단되었기 때문입니다.</td>
                                            </tr>
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">추론 및 적용형</strong></td>
                                                <td class="border themeable-border px-3 py-2">이 데이터셋으로 개발된 주행가능영역 인식 모델과 동적 물체 탐지 모델을 결합하여 '공원 환경에서의 순찰 로봇'에 적용한다고 가정해 봅시다. 이 로봇이 순찰 임무를 어떻게 더 안전하고 지능적으로 수행할 수 있을지 추론해 보십시오.</td>
                                                <td class="border themeable-border px-3 py-2">주행가능영역 인식 모델은 로봇이 공원 내의 보도와 풀밭, 자전거 도로를 명확히 구분하여 지정된 경로를 이탈하지 않고 안정적으로 순찰하도록 돕습니다. 동시에 동적 물체 탐지 모델은 갑자기 뛰어드는 아이, 빠르게 지나가는 자전거, 공놀이하는 사람들 등 동적 객체들을 실시간으로 감지하고 예측합니다. 이 두 모델의 결합을 통해, 순찰 로봇은 단순히 경로를 따라가는 것을 넘어, 위험 상황을 사전에 인지하여 스스로 멈추거나 안전한 경로로 우회하는 등 지능적인 판단을 내림으로써 공원 내 방문객들의 안전을 보장하며 순찰 임무를 수행할 수 있습니다.</td>
                                            </tr>
                                        </tbody>
                                    </table>
                                </div>
                            </details>

                            <p class="themeable-text-secondary mt-4 text-sm italic">
                                다음 장에서는 실외뿐만 아니라 복잡한 실내 환경에 더욱 초점을 맞춘 '로봇 관점 주행 영상 데이터'의 특성을 분석하며 논의를 심화시키겠습니다.
                            </p>
                        </div>
                    </div>

                    <!-- Dataset 3: 로봇 관점 주행 영상 데이터셋 -->
                    <div id="dataset-3" class="mb-8">
                        <h3 class="text-2xl font-semibold themeable-heading mb-4 flex items-center">
                            <span class="teal-text mr-2">3.</span>
                            로봇 관점 주행 영상 데이터셋
                        </h3>

                        <!-- 3-1: 개요 및 클래스 분포 -->
                        <div class="themeable-bg card-hover rounded-lg p-6 mb-6">
                            <h4 class="text-xl font-semibold mb-4 teal-text">3-1. 개요 및 클래스 분포</h4>

                            <div class="themeable-text-secondary space-y-3 mb-4">
                                <p>
                                    자율주행 로봇의 활동 무대가 실외를 넘어 식당, 쇼핑몰, 터미널과 같은 복잡한 실내 다중이용시설로 확장됨에 따라, 로봇의 시점에서 세상을 이해하고 해석하는 데이터의 중요성이 커지고 있습니다. 실내 환경은 조명 변화가 잦고, 사람과 같은 동적 장애물이 밀집하며, 예측 불가능한 정적 장애물(의자, 테이블 등)이 많아 로봇의 주행에 큰 어려움을 줍니다. 이 섹션에서는 이러한 문제를 해결하기 위한 '로봇 관점 주행 영상 데이터셋'의 핵심적인 데이터 구성과, 실제 환경의 객체 분포를 반영하는 클래스 통계를 분석하여 데이터셋의 현실성과 균형성을 평가합니다.
                                </p>
                            </div>

                            <details class="mt-4">
                                <summary class="cursor-pointer font-semibold teal-text hover:text-orange-500 transition-colors">
                                    📝 QA 샘플 보기 (4쌍)
                                </summary>
                                <div class="mt-4 overflow-x-auto">
                                    <table class="w-full border-collapse text-sm">
                                        <thead>
                                            <tr>
                                                <th class="border themeable-border px-3 py-2 text-left w-32">질문 유형</th>
                                                <th class="border themeable-border px-3 py-2 text-left">질문 (Question)</th>
                                                <th class="border themeable-border px-3 py-2 text-left">답변 (Answer)</th>
                                            </tr>
                                        </thead>
                                        <tbody class="themeable-text-secondary">
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">단순 정보 추출형</strong></td>
                                                <td class="border themeable-border px-3 py-2">'로봇 관점 주행 영상 데이터'의 원천 데이터 중, 센서 데이터에 해당하는 포맷 3가지는 무엇인가?</td>
                                                <td class="border themeable-border px-3 py-2">PCD (LIDAR 데이터), CSV (6D IMU Sensor 데이터), PNG (RGB-D Depth 데이터) 입니다.</td>
                                            </tr>
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">요약 및 설명형</strong></td>
                                                <td class="border themeable-border px-3 py-2">이 데이터셋은 어떤 두 가지 종류의 로봇을 사용하여 데이터를 수집했는지 설명하고, 각각의 데이터 구축량 비율을 제시해 주십시오.</td>
                                                <td class="border themeable-border px-3 py-2">4족보행로봇(RB1)과 바퀴주행로봇(RB2)을 사용하여 데이터를 수집했습니다. 데이터 구축량 비율은 4족보행로봇이 45.92%(68,980건), 바퀴주행로봇이 54.08%(81,249건)입니다.</td>
                                            </tr>
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">비교 및 분석형</strong></td>
                                                <td class="border themeable-border px-3 py-2">데이터 수집 장소 분포에서 가장 높은 구성비를 차지하는 상위 3개 장소와 가장 낮은 구성비를 차지하는 하위 3개 장소는 무엇이며, 이를 통해 데이터셋의 수집 환경 특성을 어떻게 분석할 수 있습니까?</td>
                                                <td class="border themeable-border px-3 py-2">가장 높은 상위 3개 장소는 중형식당(9.53%), 은행(9.50%), 터미널(9.38%)이며, 가장 낮은 하위 3개 장소는 대형마트(6.60%), 전시장(7.09%), 실내주차장(7.52%)입니다. 이 분포는 특정 환경에 치우치지 않고 6.6%에서 9.53% 사이의 비교적 고른 분포를 보이고 있음을 보여줍니다. 이는 다양한 실내 다중이용시설 환경에서 로봇이 범용적으로 활용될 수 있도록 데이터의 다양성과 균형성을 확보하려는 의도를 나타냅니다.</td>
                                            </tr>
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">추론 및 적용형</strong></td>
                                                <td class="border themeable-border px-3 py-2">정적 개체 클래스 중 '의자'가 22.29%, '기둥'이 9.76%로 높은 비율을 차지하고, 동적 개체인 '사람'이 51.41%로 압도적인 비율을 차지합니다. 이 데이터로 학습한 로봇이 실내 자율주행 시 어떤 강점을 가질 것으로 추론할 수 있습니까?</td>
                                                <td class="border themeable-border px-3 py-2">소스 컨텍스트에 따르면 '사람', '의자', '기둥'은 실내 환경에서 가장 흔하게 마주치는 동적 및 정적 장애물입니다. 이들 클래스의 데이터가 풍부하다는 것은, 로봇이 사람들의 예측 불가능한 움직임을 회피하고, 무질서하게 배치된 의자 사이를 통과하며, 고정된 구조물인 기둥을 안정적으로 인식하는 능력에 특화될 것임을 의미합니다. 따라서 이 로봇은 혼잡한 식당이나 로비 등 사람과 장애물이 많은 복잡한 실내 환경에서도 뛰어난 회피 기동 및 경로 계획 능력을 발휘할 것으로 기대할 수 있습니다.</td>
                                            </tr>
                                        </tbody>
                                    </table>
                                </div>
                            </details>

                            <p class="themeable-text-secondary mt-4 text-sm italic">
                                다음으로, 이 데이터셋의 품질과 유효성을 보증하기 위해 사용된 검증 환경과 모델에 대해 구체적으로 알아봅니다.
                            </p>
                        </div>

                        <!-- 3-2: 검증 환경 및 모델 -->
                        <div class="themeable-bg card-hover rounded-lg p-6 mb-6">
                            <h4 class="text-xl font-semibold mb-4 teal-text">3-2. 검증 환경 및 모델</h4>

                            <div class="themeable-text-secondary space-y-3 mb-4">
                                <p>
                                    데이터셋의 신뢰도는 단순히 많은 양의 데이터를 수집하는 것만으로 확보되지 않습니다. 객관적인 성능 지표와 표준화된 하드웨어 및 소프트웨어 환경에서 수행되는 엄격한 검증 절차를 통해 비로소 그 기술적 완성도가 입증됩니다. 본 섹션에서는 '로봇 관점 주행 영상 데이터셋'의 유효성을 검증하기 위해 구성된 하드웨어 및 소프트웨어 환경을 상세히 기술합니다. 또한, 2D 바운딩 박스, 3D 큐보이드, SLAM(동시적 위치추정 및 지도작성) 각 성능을 평가하기 위해 사용된 최신 학습 알고리즘들을 분석하여 데이터의 기술적 완성도를 평가합니다.
                                </p>
                            </div>

                            <details class="mt-4">
                                <summary class="cursor-pointer font-semibold teal-text hover:text-orange-500 transition-colors">
                                    📝 QA 샘플 보기 (4쌍)
                                </summary>
                                <div class="mt-4 overflow-x-auto">
                                    <table class="w-full border-collapse text-sm">
                                        <thead>
                                            <tr>
                                                <th class="border themeable-border px-3 py-2 text-left w-32">질문 유형</th>
                                                <th class="border themeable-border px-3 py-2 text-left">질문 (Question)</th>
                                                <th class="border themeable-border px-3 py-2 text-left">답변 (Answer)</th>
                                            </tr>
                                        </thead>
                                        <tbody class="themeable-text-secondary">
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">단순 정보 추출형</strong></td>
                                                <td class="border themeable-border px-3 py-2">SLAM 성능 유효성 검증에 사용된 학습 알고리즘의 이름과 성능 지표는 무엇인가?</td>
                                                <td class="border themeable-border px-3 py-2">학습 알고리즘은 Fast-LIO2이며, 성능 지표는 End to End RMSE (0.2m 이내) 입니다.</td>
                                            </tr>
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">요약 및 설명형</strong></td>
                                                <td class="border themeable-border px-3 py-2">2D 바운딩 박스 객체 검출 유효성 검증 시, 전체 데이터를 학습(Train), 검증(Validation), 테스트(Test) 세트로 나누는 비율과 각 세트의 데이터 개수를 설명해 주십시오.</td>
                                                <td class="border themeable-border px-3 py-2">전체 데이터는 Train:Val:Test = 80:10:10 비율로 나뉩니다. Training Set은 80%로 120,220개, Validation Set은 10%로 14,993개, Test Set은 10%로 15,016개의 데이터로 구성됩니다.</td>
                                            </tr>
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">비교 및 분석형</strong></td>
                                                <td class="border themeable-border px-3 py-2">2D 객체 검출(YOLOV7)과 3D 객체 검출(FocalsConv)의 유효성 검증에 사용된 GPU와 OS 환경을 비교하고, SLAM 성능 검증 환경과의 차이점을 설명해 주십시오.</td>
                                                <td class="border themeable-border px-3 py-2">2D와 3D 객체 검출은 모두 딥러닝 기반 모델이므로 대규모 병렬 연산을 위해 'NVIDIA RTX A6000 * 4' GPU와 'Ubuntu 18.04.6 LTS' 환경을 동일하게 사용합니다. 반면, SLAM 성능 검증에 사용된 Fast-LIO2 알고리즘은 AI 모델이 아니므로 GPU를 사용하지 않고 CPU(AMD EPYC 7742)만을 사용하며, OS 또한 'Ubuntu 20.04'로 다릅니다. 이는 검증 대상 알고리즘의 연산 특성에 따라 최적화된 환경을 각각 구성했음을 보여줍니다.</td>
                                            </tr>
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">추론 및 적용형</strong></td>
                                                <td class="border themeable-border px-3 py-2">3D 객체 검출 모델 FocalsConv는 RGB(3채널)에 Depth map(1채널)을 결합하여 사용한다고 되어 있습니다. 이 방식이 LiDAR 데이터만 사용하는 방식에 비해 어떤 장점을 가질 수 있을지, 특히 '유리문'과 같은 객체를 인식할 때를 예로 들어 추론해 보십시오.</td>
                                                <td class="border themeable-border px-3 py-2">LiDAR 센서는 투명한 '유리문'을 제대로 인식하지 못하고 통과해버리거나 노이즈로 처리할 수 있습니다. 하지만 RGB 카메라는 유리문의 프레임, 손잡이, 표면의 반사 등 시각적 특징을 포착할 수 있습니다. FocalsConv 모델처럼 RGB와 Depth 정보를 결합하면, LiDAR가 놓치는 부분을 카메라의 시각 정보로 보완할 수 있습니다. 즉, Depth 정보로 대략적인 평면의 존재를 감지하고, RGB 정보로 그것이 '문'이라는 시각적 맥락을 파악하여, 로봇이 유리문을 투과 가능한 공간으로 오인하지 않고 장애물로 정확하게 인식할 확률을 크게 높일 수 있습니다.</td>
                                            </tr>
                                        </tbody>
                                    </table>
                                </div>
                            </details>

                            <p class="themeable-text-secondary mt-4 text-sm italic">
                                지금까지는 로봇의 '인식'과 '주행'이라는 외부 환경과의 상호작용에 초점을 맞추었다면, 다음 장에서는 로봇 자체의 '상태'를 진단하고 '유지보수'하기 위한 데이터셋을 분석해 보겠습니다.
                            </p>
                        </div>
                    </div>

                    <!-- Dataset 4: 실내공간 유지관리 서비스 로봇 데이터셋 -->
                    <div id="dataset-4" class="mb-8">
                        <h3 class="text-2xl font-semibold themeable-heading mb-4 flex items-center">
                            <span class="teal-text mr-2">4.</span>
                            실내공간 유지관리 서비스 로봇 데이터셋
                        </h3>

                        <!-- 4-1: 개요 및 포맷 -->
                        <div class="themeable-bg card-hover rounded-lg p-6 mb-6">
                            <h4 class="text-xl font-semibold mb-4 teal-text">4-1. 개요 및 포맷</h4>

                            <div class="themeable-text-secondary space-y-3 mb-4">
                                <p>
                                    서비스 로봇이 산업 현장과 일상 공간에 안정적으로 확산되기 위해서는 단순히 주어진 임무를 수행하는 것을 넘어, 스스로의 상태를 진단하고 고장을 사전에 예측하여 선제적으로 대응하는 유지보수 기술이 핵심입니다. 이를 위해서는 로봇의 내부 상태를 실시간으로 기록한 데이터가 필수적입니다. 본 섹션에서는 이러한 목적을 위해 구축된 '실내공간 유지관리 서비스 로봇 데이터셋'의 고유한 특징, 즉 이미지나 영상이 아닌 텍스트 기반의 상태 데이터라는 점을 조명합니다. 상세한 JSON 데이터 포맷을 분석하여 이 데이터가 로봇의 상태 추론 및 이상 감지 AI 모델 개발에 어떤 정보를 제공하는지 평가합니다.
                                </p>
                            </div>

                            <details class="mt-4">
                                <summary class="cursor-pointer font-semibold teal-text hover:text-orange-500 transition-colors">
                                    📝 QA 샘플 보기 (4쌍)
                                </summary>
                                <div class="mt-4 overflow-x-auto">
                                    <table class="w-full border-collapse text-sm">
                                        <thead>
                                            <tr>
                                                <th class="border themeable-border px-3 py-2 text-left w-32">질문 유형</th>
                                                <th class="border themeable-border px-3 py-2 text-left">질문 (Question)</th>
                                                <th class="border themeable-border px-3 py-2 text-left">답변 (Answer)</th>
                                            </tr>
                                        </thead>
                                        <tbody class="themeable-text-secondary">
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">단순 정보 추출형</strong></td>
                                                <td class="border themeable-border px-3 py-2">라벨링 데이터의 JSON 포맷에서, 로봇의 에러 발생 여부와 에러 코드를 나타내는 속성명(key)은 각각 무엇인가?</td>
                                                <td class="border themeable-border px-3 py-2">에러 발생 여부는 'errorState' (boolean), 에러 코드는 'errorCode' (String) 입니다.</td>
                                            </tr>
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">요약 및 설명형</strong></td>
                                                <td class="border themeable-border px-3 py-2">이 데이터셋의 파일명은 어떤 5가지 정보의 조합으로 구성되는지 순서대로 설명해 주십시오.</td>
                                                <td class="border themeable-border px-3 py-2">파일명은 '로봇의 종류', '로봇에게 부여된 고유 ID', '데이터가 생성된 월', '로봇이 수행하는 태스크 ID', '데이터 순번'의 순서로 구성됩니다. (예: 안내로봇_안내로봇01_11_task01_05233.json)</td>
                                            </tr>
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">비교 및 분석형</strong></td>
                                                <td class="border themeable-border px-3 py-2">원천 데이터에 포함된 'deviceData' 객체와 'taskData' 객체의 정보 유형을 비교하고, 이 두 정보가 로봇 유지보수 관점에서 어떻게 상호 보완적으로 활용될 수 있는지 분석해 주십시오.</td>
                                                <td class="border themeable-border px-3 py-2">'deviceData'는 배터리 잔량, 충돌 횟수, 현재 속도 등 로봇의 물리적, 하드웨어적 상태 정보를 담고 있습니다. 반면, 'taskData'는 작업 이름, 예상 소요 시간, 현재 작업 상태 등 로봇이 수행하는 임무의 논리적, 소프트웨어적 상태 정보를 담고 있습니다. 이 둘을 결합하면, 예를 들어 특정 'task' 수행 중에만 'collision' 횟수가 급증하는 현상을 발견하여 해당 작업의 경로 계획에 문제가 있음을 진단하거나, 'batteryLevel'이 특정 'task' 수행 시 유독 빨리 소모되는 것을 파악하여 해당 임무가 로봇에 과부하를 주고 있음을 분석하는 등, 단순 하드웨어 고장이 아닌 작업-상태 간의 복합적인 문제 원인을 진단하는 데 상호 보완적으로 활용될 수 있습니다.</td>
                                            </tr>
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">추론 및 적용형</strong></td>
                                                <td class="border themeable-border px-3 py-2">라벨링 데이터의 'errorStatementLong' 속성은 에러 상황을 상세한 자연어 문장으로 설명합니다. 이 데이터를 LLM 파인튜닝에 활용한다면, 로봇 관제 시스템 운영자에게 어떤 실질적인 도움을 줄 수 있을지 추론해 보십시오.</td>
                                                <td class="border themeable-border px-3 py-2">'E-ENV-O' 같은 단순 에러 코드만으로는 운영자가 문제의 원인을 즉각 파악하기 어렵습니다. 하지만 'errorStatementLong' 데이터("현재 로봇...장애물 감지로 인한 이동 불가 에러로 판단되고...")로 파인튜닝된 LLM은 "현재 서빙로봇이 혼잡한 오피스에서 장애물 감지로 인해 이동 불가 상태입니다"와 같이 코드의 의미를 구체적인 상황과 원인을 포함한 문장으로 자동 변환해 줄 수 있습니다. 더 나아가, LLM은 "주변 장애물을 확인하고 경로를 재설정하거나, 해당 구역의 혼잡도가 낮아질 때까지 대기하는 조치를 권장합니다"와 같은 해결책까지 제시할 수 있습니다. 이는 운영자의 상황 판단 시간을 단축시키고, 신속하고 정확한 초동 조치를 가능하게 하여 로봇 운영 효율성을 크게 향상시킬 것입니다.</td>
                                            </tr>
                                        </tbody>
                                    </table>
                                </div>
                            </details>

                            <p class="themeable-text-secondary mt-4 text-sm italic">
                                로봇의 외부 환경 인식에서 내부 상태 진단까지 살펴보았으니, 이제 로봇의 핵심 기능인 '조작'과 관련된 '로봇 핸드용 객체 특성 식별 데이터'로 분석의 초점을 옮깁니다.
                            </p>
                        </div>
                    </div>

                    <!-- Dataset 5: 로봇 핸드용 객체 특성 식별 데이터셋 -->
                    <div id="dataset-5" class="mb-8">
                        <h3 class="text-2xl font-semibold themeable-heading mb-4 flex items-center">
                            <span class="teal-text mr-2">5.</span>
                            로봇 핸드용 객체 특성 식별 데이터셋
                        </h3>

                        <!-- 5-1: 개요 및 구성 -->
                        <div class="themeable-bg card-hover rounded-lg p-6 mb-6">
                            <h4 class="text-xl font-semibold mb-4 teal-text">5-1. 개요 및 구성</h4>

                            <div class="themeable-text-secondary space-y-3 mb-4">
                                <p>
                                    로봇이 인간처럼 다양한 물체를 능숙하게 다루기 위해서는 단순히 눈으로 보고 형태를 인식하는 것을 넘어, 물체의 무게, 재질, 단단함과 같은 물리적 특성을 이해하는 것이 필수적입니다. 이러한 물리적 특성에 대한 이해 없이는 섬세한 힘 조절이 필요한 조작(manipulation)이 불가능하기 때문입니다. 이 섹션에서는 이러한 과제를 해결하기 위해 구축된 '로봇 핸드용 객체 특성 식별 데이터'의 다중 모달(multi-modal) 구성을 분석합니다. 특히 시각 정보(이미지, 3D 메쉬)와 물리 정보(물리량, 시계열 센서 데이터)가 어떻게 통합되어 객체 조작 지능의 기반을 마련하는지 그 중요성을 평가합니다.
                                </p>
                            </div>

                            <details class="mt-4">
                                <summary class="cursor-pointer font-semibold teal-text hover:text-orange-500 transition-colors">
                                    📝 QA 샘플 보기 (4쌍)
                                </summary>
                                <div class="mt-4 overflow-x-auto">
                                    <table class="w-full border-collapse text-sm">
                                        <thead>
                                            <tr>
                                                <th class="border themeable-border px-3 py-2 text-left w-32">질문 유형</th>
                                                <th class="border themeable-border px-3 py-2 text-left">질문 (Question)</th>
                                                <th class="border themeable-border px-3 py-2 text-left">답변 (Answer)</th>
                                            </tr>
                                        </thead>
                                        <tbody class="themeable-text-secondary">
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">단순 정보 추출형</strong></td>
                                                <td class="border themeable-border px-3 py-2">데이터셋 구축 대상이 되는 가정용 물품은 총 몇 종류이며, 각 물품당 목표 이미지 데이터 수량은 얼마인가?</td>
                                                <td class="border themeable-border px-3 py-2">구축 대상 물품은 총 200종류이며, 각 물품당 Low-RGB, Hi-RGB, RGB-D 이미지를 각각 600장 이상 수집하는 것을 목표로 합니다.</td>
                                            </tr>
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">요약 및 설명형</strong></td>
                                                <td class="border themeable-border px-3 py-2">이 데이터셋의 '임무 데이터'는 어떤 5가지의 로봇 핸드 조작 과제로 구성되어 있는지 나열해 주십시오.</td>
                                                <td class="border themeable-border px-3 py-2">'잡기(grasping)', '쥐기(squeezing)', '돌리기(rotating)', '흔들기(shaking)', '긁기(scratching)'의 5가지 조작 과제로 구성되어 있습니다.</td>
                                            </tr>
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">비교 및 분석형</strong></td>
                                                <td class="border themeable-border px-3 py-2">'품목 데이터'의 'Hi-RGB'와 'Low-RGB' 이미지의 차이점을 설명하고, 이 두 종류의 데이터를 모두 수집하는 것이 로봇 비전 모델의 강건성(robustness)에 어떤 이점을 주는지 분석해 주십시오.</td>
                                                <td class="border themeable-border px-3 py-2">'Hi-RGB'는 12bit raw 데이터 타입의 고해상도, 무압축 이미지인 반면, 'Low-RGB'는 8bit jpg 형식의 손실 압축 이미지입니다. 고품질의 'Hi-RGB' 데이터는 객체의 세밀한 특징을 학습하는 데 유리하고, 저품질의 'Low-RGB' 데이터는 실제 로봇에 탑재된 저사양 카메라나 통신 중 화질 저하가 발생하는 현실적인 상황을 모사합니다. 이 두 종류의 데이터를 모두 학습하면, 로봇 비전 모델이 다양한 품질의 이미지 입력에 대해 안정적인 인식 성능을 발휘하는 강건성을 확보할 수 있습니다.</td>
                                            </tr>
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">추론 및 적용형</strong></td>
                                                <td class="border themeable-border px-3 py-2">이 데이터셋은 객체의 '무게', '크기', '재질'과 같은 물리량 정보와 '촉감', '역감' 같은 시계열 데이터를 포함합니다. 로봇이 처음 보는 물체를 들어 올리려고 할 때, 이 데이터들을 어떻게 종합적으로 활용하여 안전하고 효율적으로 물체를 파지할 수 있을지 추론해 보십시오.</td>
                                                <td class="border themeable-border px-3 py-2">로봇은 먼저 시각 정보(이미지, 3D 메쉬)를 통해 처음 보는 물체가 데이터셋의 어떤 물체와 유사한지 판단합니다. 유사한 물체의 '재질'과 '크기' 정보를 바탕으로 표면의 미끄러움 정도와 무게 중심을 예측하고, '무게' 정보를 통해 물체를 들어 올리는 데 필요한 최소한의 힘을 추정합니다. 이후, 파지를 시작하면서 '촉감' 및 '역감' 센서 데이터를 실시간으로 피드백 받아, 물체가 미끄러지거나 찌그러지지 않도록 파지하는 힘을 미세하게 조절할 수 있습니다. 이는 시각 정보만으로는 불가능한, 물리적 특성에 기반한 정교하고 안정적인 파지를 가능하게 합니다.</td>
                                            </tr>
                                        </tbody>
                                    </table>
                                </div>
                            </details>

                            <p class="themeable-text-secondary mt-4 text-sm italic">
                                다음으로, 이러한 풍부한 다중 모달 데이터를 수집하기 위해 사용된 구체적인 도구와 표준화된 절차에 대해 자세히 살펴보겠습니다.
                            </p>
                        </div>

                        <!-- 5-2: 수집 도구 및 절차 -->
                        <div class="themeable-bg card-hover rounded-lg p-6 mb-6">
                            <h4 class="text-xl font-semibold mb-4 teal-text">5-2. 수집 도구 및 절차</h4>

                            <div class="themeable-text-secondary space-y-3 mb-4">
                                <p>
                                    신뢰할 수 있는 데이터셋은 감이나 주관이 아닌, 표준화된 수집 도구와 체계적인 절차를 통해 만들어집니다. 데이터 수집에 사용되는 센서의 정밀도와 일관된 수집 절차는 데이터의 품질을 보증하고, 이를 통해 학습된 AI 모델의 재현성과 신뢰성을 담보하는 기반이 됩니다. 본 섹션에서는 '로봇 핸드용 객체 특성 식별 데이터셋'을 구축하는 데 사용된 고정밀 센서와 로봇 핸드 등 핵심 수집 도구의 사양을 분석합니다. 또한, 데이터가 일관성 있게 획득되는 수집 절차를 단계별로 설명하여 데이터의 품질 보증 과정을 조명합니다.
                                </p>
                            </div>

                            <details class="mt-4">
                                <summary class="cursor-pointer font-semibold teal-text hover:text-orange-500 transition-colors">
                                    📝 QA 샘플 보기 (4쌍)
                                </summary>
                                <div class="mt-4 overflow-x-auto">
                                    <table class="w-full border-collapse text-sm">
                                        <thead>
                                            <tr>
                                                <th class="border themeable-border px-3 py-2 text-left w-32">질문 유형</th>
                                                <th class="border themeable-border px-3 py-2 text-left">질문 (Question)</th>
                                                <th class="border themeable-border px-3 py-2 text-left">답변 (Answer)</th>
                                            </tr>
                                        </thead>
                                        <tbody class="themeable-text-secondary">
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">단순 정보 추출형</strong></td>
                                                <td class="border themeable-border px-3 py-2">객체의 크기를 측정하는 데 사용된 '디지털 버니어 캘리퍼스'의 측정 범위와 측정 오차는 얼마인가?</td>
                                                <td class="border themeable-border px-3 py-2">측정 범위는 0~150mm이며, 측정 오차는 0.2mm입니다.</td>
                                            </tr>
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">요약 및 설명형</strong></td>
                                                <td class="border themeable-border px-3 py-2">200개 품목의 이미지를 일관된 조건에서 촬영하기 위해 구성한 '촬영 지그'의 특징을 설명해 주십시오.</td>
                                                <td class="border themeable-border px-3 py-2">촬영 지그는 정반 플레이트에 물리적으로 고정된 브라켓, 5세트의 카메라와 RGB-D 센서, 그리고 물체를 올려두는 회전식 턴테이블로 구성됩니다. 이를 통해 물체를 일정 각도로 회전시키면서 여러 시점의 데이터를 체계적으로 획득할 수 있습니다.</td>
                                            </tr>
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">비교 및 분석형</strong></td>
                                                <td class="border themeable-border px-3 py-2">데이터 수집에 사용된 'Allegro Hand'와 '손가락형 촉각센서'의 역할을 각각 분석하고, 이 두 도구의 결합이 어떻게 인간의 손과 유사한 데이터 수집을 가능하게 하는지 설명해 주십시오.</td>
                                                <td class="border themeable-border px-3 py-2">'Allegro Hand'는 16개의 독립적인 관절을 제어하여 잡기, 쥐기 등 인간의 손과 유사한 '동작'을 수행하는 역할을 합니다. 여기에 부착된 '손가락형 촉각센서'는 그 동작 과정에서 물체와 접촉할 때 발생하는 압력 분포, 즉 '감각' 데이터를 수집합니다. 이 둘의 결합은 로봇이 단순히 물체를 잡는 것을 넘어, 잡는 동안의 미세한 힘의 변화와 접촉 상태를 데이터화함으로써 인간의 '촉각을 동반한 조작' 행위를 모사할 수 있게 합니다.</td>
                                            </tr>
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">추론 및 적용형</strong></td>
                                                <td class="border themeable-border px-3 py-2">데이터 수집 절차 중 '로봇 핸드의 임무 수행 시 힘의 단계를 5단계로 나누어 단계별 10회 임무 수행'하는 과정이 있습니다. 이처럼 힘의 단계를 세분화하여 데이터를 수집하는 것이 '깨지기 쉬운 계란'이나 '말랑한 두부' 같은 물체를 다루는 로봇을 개발할 때 어떤 이점을 줄 수 있는지 추론해 보십시오.</td>
                                                <td class="border themeable-border px-3 py-2">힘의 단계를 세분화하여 데이터를 수집하면, 각 단계별 힘의 크기에 따라 객체가 어떻게 변형되고, 센서(촉각, 역감) 값이 어떻게 변하는지에 대한 정밀한 모델을 구축할 수 있습니다. 이를 통해 '계란'이나 '두부' 같은 섬세한 객체를 다룰 때, 로봇은 물체가 파손되기 직전의 임계 힘 값을 학습할 수 있습니다. 결과적으로 로봇은 최소한의 힘으로 객체를 안정적으로 파지하거나, 물체의 변형을 감지하면 즉시 힘을 줄이는 등 정교한 힘 제어 능력을 갖추게 되어, 가사 지원 로봇의 실용성을 크게 높일 수 있습니다.</td>
                                            </tr>
                                        </tbody>
                                    </table>
                                </div>
                            </details>

                            <p class="themeable-text-secondary mt-4 text-sm italic">
                                로봇의 정교한 조작을 분석했다면, 다음으로는 그 조작의 주체인 '사람'의 동작을 이해하고 모방하기 위한 '손·팔 협조 파지-조작 동작 데이터'를 살펴보겠습니다.
                            </p>
                        </div>
                    </div>

                    <!-- Dataset 6: 손·팔 협조 파지-조작 동작 데이터셋 -->
                    <div id="dataset-6" class="mb-8">
                        <h3 class="text-2xl font-semibold themeable-heading mb-4 flex items-center">
                            <span class="teal-text mr-2">6.</span>
                            손·팔 협조 파지-조작 동작 데이터셋
                        </h3>

                        <!-- 6-1: 개요 및 라벨링 속성 -->
                        <div class="themeable-bg card-hover rounded-lg p-6 mb-6">
                            <h4 class="text-xl font-semibold mb-4 teal-text">6-1. 개요 및 라벨링 속성</h4>

                            <div class="themeable-text-secondary space-y-3 mb-4">
                                <p>
                                    로봇이 인간의 생활 공간에 자연스럽게 통합되고 인간의 작업을 효과적으로 모방하기 위해서는, 인간의 복잡하고 정교한 동작, 특히 '손과 팔의 협응'을 깊이 있게 이해하는 것이 선결 과제입니다. 인간은 물체를 잡고 조작할 때 손가락, 손목, 팔, 심지어 상체까지 유기적으로 사용하여 최적의 움직임을 만들어냅니다. 이 섹션에서는 이러한 인간의 파지-조작 동작을 심도 깊게 분석하기 위해 구축된 '손·팔 협조 파지-조작 동작 데이터셋'의 구성을 살펴봅니다. 손 관절부터 상체, 힘, 객체 정보까지 아우르는 다층적인 라벨링 속성을 분석하여 데이터의 정보 밀도와 활용 가능성을 평가합니다.
                                </p>
                            </div>

                            <details class="mt-4">
                                <summary class="cursor-pointer font-semibold teal-text hover:text-orange-500 transition-colors">
                                    📝 QA 샘플 보기 (4쌍)
                                </summary>
                                <div class="mt-4 overflow-x-auto">
                                    <table class="w-full border-collapse text-sm">
                                        <thead>
                                            <tr>
                                                <th class="border themeable-border px-3 py-2 text-left w-32">질문 유형</th>
                                                <th class="border themeable-border px-3 py-2 text-left">질문 (Question)</th>
                                                <th class="border themeable-border px-3 py-2 text-left">답변 (Answer)</th>
                                            </tr>
                                        </thead>
                                        <tbody class="themeable-text-secondary">
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">단순 정보 추출형</strong></td>
                                                <td class="border themeable-border px-3 py-2">라벨링 데이터의 'gesture.hand_gesture_data.hand_keypoints_2D' 속성은 무엇을 의미하는가?</td>
                                                <td class="border themeable-border px-3 py-2">사람 손의 각 관절에 대한 2D 좌표 데이터를 의미합니다.</td>
                                            </tr>
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">요약 및 설명형</strong></td>
                                                <td class="border themeable-border px-3 py-2">어노테이션 포맷에서 '손 관절 데이터 가시화 정도(visibility)'를 나타내는 세 가지 값(1, 0, -1)은 각각 어떤 상태를 의미하는지 설명해 주십시오.</td>
                                                <td class="border themeable-border px-3 py-2">'1'은 관절이 카메라에 관측 가능한 상태, '0'은 다른 물체나 손 부위에 의해 가려진 상태, '-1'은 관절이 카메라 화각 밖에 위치한 상태를 의미합니다.</td>
                                            </tr>
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">비교 및 분석형</strong></td>
                                                <td class="border themeable-border px-3 py-2">이 데이터셋의 라벨링 속성은 'object.object_2D'(객체 바운딩박스)와 'object.grasp_2D'(객체 파지 가능 지점)를 모두 포함합니다. 이 두 정보의 차이점은 무엇이며, 두 가지를 모두 제공하는 것이 로봇 학습에 어떤 이점을 주는지 분석해 주십시오.</td>
                                                <td class="border themeable-border px-3 py-2">'object.object_2D'는 객체의 전체적인 위치와 크기를 나타내는 정보입니다. 반면, 'object.grasp_2D'는 그 객체 내에서도 로봇 핸드가 안정적으로 파지할 수 있는 특정 영역이나 지점을 더 상세하게 지정하는 정보입니다. 객체 전체 위치만 아는 것과 달리, 파지 가능 지점까지 학습한 로봇은 단순히 객체를 탐지하는 것을 넘어 '어떻게 잡을 것인가'에 대한 최적의 전략을 수립할 수 있습니다. 이는 파지 계획의 탐색 공간을 줄여 더 빠르고 성공률 높은 조작을 가능하게 하는 핵심적인 이점을 제공합니다.</td>
                                            </tr>
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">추론 및 적용형</strong></td>
                                                <td class="border themeable-border px-3 py-2">데이터셋에는 'light_source.light_degree' (조명 단계) 속성이 포함되어 있습니다. 이 정보를 활용하면, AR(증강현실) 환경에서 가상의 물체와 상호작용하는 사용자 경험을 어떻게 더 현실적으로 만들 수 있을지 추론해 보십시오.</td>
                                                <td class="border themeable-border px-3 py-2">AR 환경에서 가상 객체를 현실 공간에 배치할 때, 주변 조명과 어울리지 않으면 매우 부자연스러워 보입니다. 'light_degree' 데이터로 학습한 AI 모델은 현재 사용자가 있는 실제 공간의 조명 밝기와 색상을 인식할 수 있습니다. 이 정보를 바탕으로 AR 시스템은 가상 객체의 그림자 방향, 밝기, 표면의 반사광 등을 실시간으로 조절하여 실제 조명 환경과 완벽하게 어우러지도록 렌더링할 수 있습니다. 이는 사용자가 가상 객체를 마치 실제 존재하는 것처럼 느끼게 하여 AR 경험의 몰입감과 현실감을 극대화합니다.</td>
                                            </tr>
                                        </tbody>
                                    </table>
                                </div>
                            </details>

                            <p class="themeable-text-secondary mt-4 text-sm italic">
                                마지막으로, 특정 과업 중심의 로봇-인간 상호작용을 다루는 '사람 행동 인식 로봇 자율 행동 데이터'를 분석하며 보고서를 마무리하겠습니다.
                            </p>
                        </div>
                    </div>

                    <!-- Dataset 7: 사람 행동 인식 로봇 자율 행동 데이터셋 -->
                    <div id="dataset-7" class="mb-8">
                        <h3 class="text-2xl font-semibold themeable-heading mb-4 flex items-center">
                            <span class="teal-text mr-2">7.</span>
                            사람 행동 인식 로봇 자율 행동 데이터셋
                        </h3>

                        <!-- 7-1: 개요 및 활용 사례 -->
                        <div class="themeable-bg card-hover rounded-lg p-6 mb-6">
                            <h4 class="text-xl font-semibold mb-4 teal-text">7-1. 개요 및 활용 사례</h4>

                            <div class="themeable-text-secondary space-y-3 mb-4">
                                <p>
                                    서비스 로봇이 공공장소 및 상업시설에서 단순한 기능 제공을 넘어 사용자에게 진정으로 유용한 경험을 제공하기 위해서는, 주변 사람들의 행동과 의도를 정확히 파악하고 이에 맞춰 자율적으로 반응하는 사회적 지능(social intelligence)이 필수적입니다. 특히 키오스크와 같은 자동화 시스템 앞에서 어려움을 겪는 사용자를 돕는 것은 로봇의 사회적 가치를 높이는 중요한 활용 사례입니다. 이 섹션에서는 이러한 상호작용을 분석하기 위해 구축된 '사람 행동 인식 로봇 자율 행동 데이터'의 개요를 살펴보고, 이를 통해 정보 취약 계층의 문제를 해결하는 구체적인 활용 사례를 분석하여 데이터의 사회적, 기술적 가치를 평가합니다.
                                </p>
                            </div>

                            <details class="mt-4">
                                <summary class="cursor-pointer font-semibold teal-text hover:text-orange-500 transition-colors">
                                    📝 QA 샘플 보기 (4쌍)
                                </summary>
                                <div class="mt-4 overflow-x-auto">
                                    <table class="w-full border-collapse text-sm">
                                        <thead>
                                            <tr>
                                                <th class="border themeable-border px-3 py-2 text-left w-32">질문 유형</th>
                                                <th class="border themeable-border px-3 py-2 text-left">질문 (Question)</th>
                                                <th class="border themeable-border px-3 py-2 text-left">답변 (Answer)</th>
                                            </tr>
                                        </thead>
                                        <tbody class="themeable-text-secondary">
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">단순 정보 추출형</strong></td>
                                                <td class="border themeable-border px-3 py-2">이 데이터셋의 원천 데이터는 몇 시간 분량의 영상 데이터로 구축되었으며, 총 몇 종의 실제 서비스 환경에서 수집되었는가?</td>
                                                <td class="border themeable-border px-3 py-2">1,000시간 이상의 자동 서비스 시스템 조작 영상 데이터로 구축되었으며, 교통 시설, 의료 시설, 교육 시설 등을 포함한 총 10종의 서비스 환경에서 수집되었습니다.</td>
                                            </tr>
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">요약 및 설명형</strong></td>
                                                <td class="border themeable-border px-3 py-2">데이터셋의 활용 사례로 제시된 '휠체어 사용자 및 키가 작은 사람들을 위한 키오스크 문제'의 해결 방안 두 가지를 설명해 주십시오.</td>
                                                <td class="border themeable-border px-3 py-2">첫째는 '자동 높이 조절 기능'으로, 센서가 사용자의 상태(예: 휠체어 탑승)를 인식하여 키오스크 높이를 자동으로 조절합니다. 둘째는 '접근성 강화 디자인'으로, 화면과 조작 버튼의 위치를 다양한 신체 조건의 사용자가 쉽게 이용할 수 있도록 최적화합니다.</td>
                                            </tr>
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">비교 및 분석형</strong></td>
                                                <td class="border themeable-border px-3 py-2">데이터 분포에서 '연령 분포'와 '환경 분포'를 분석해 주십시오. 데이터가 특정 계층이나 환경에 편중되지 않고 다양성을 확보했는지 평가합니다.</td>
                                                <td class="border themeable-border px-3 py-2">'연령 분포'는 청중장년층이 84.57%로 다수를 차지하지만, 정보 취약 계층인 유소년(4.7%)과 노년(10.73%)도 상당수 포함하여 다양성을 확보했습니다. '환경 분포'는 교육 시설(21.97%)과 편의 시설(15.41%)의 비중이 높지만, 교통, 의료, 복지 시설 등 10개 카테고리가 4.97%에서 21.97% 사이의 비교적 고른 분포를 보여 특정 환경에 치우치지 않았음을 알 수 있습니다. 이는 다양한 사용자 그룹과 환경에 대한 로봇의 행동 인식 모델 개발에 적합한 데이터 구성임을 의미합니다.</td>
                                            </tr>
                                            <tr class="hover:bg-orange-500/5 transition-colors">
                                                <td class="border themeable-border px-3 py-2 align-top"><strong class="teal-text">추론 및 적용형</strong></td>
                                                <td class="border themeable-border px-3 py-2">이 데이터셋의 메타데이터에는 사용자의 암호화된 ID, 나이, 키, 장애 여부 등이 포함됩니다. 이 정보를 활용하여 개인 맞춤형 광고나 프로모션을 제공하는 상업용 로봇 서비스를 어떻게 구현할 수 있을지 추론해 보십시오.</td>
                                                <td class="border themeable-border px-3 py-2">로봇에 탑재된 AI가 키오스크 앞에 선 사용자를 인식하고 메타데이터와 매칭합니다. 만약 사용자가 '유소년' 연령대라면, 로봇은 장난감이나 어린이 메뉴 관련 광고를 화면에 표시하거나 음성으로 안내할 수 있습니다. '노년' 사용자가 접근하면, 건강 보조 식품이나 시니어 할인 혜택 정보를 우선적으로 제공할 수 있습니다. 이처럼, 개인의 특성을 실시간으로 파악하여 가장 관련성 높은 정보와 서비스를 즉각적으로 제공함으로써, 사용자의 만족도를 높이고 구매 전환율을 극대화하는 고도로 개인화된 상업 서비스를 구현할 수 있습니다.</td>
                                            </tr>
                                        </tbody>
                                    </table>
                                </div>
                            </details>

                            <p class="themeable-text-secondary mt-4 text-sm italic">
                                지금까지 7개 데이터셋(13개 그룹)의 로봇 지능 데이터를 분석하여 LLM 파인튜닝을 위한 QA 데이터셋의 기반을 마련하였으며, 다음 섹션에서는 통계와 활용 방법을 종합적으로 정리하겠습니다.
                            </p>
                        </div>
                    </div>

                </section>

                <!-- Section 4: 통계 -->
                <section id="statistics" class="mb-12">
                    <h2 class="text-3xl font-bold themeable-heading mb-6 pb-3 border-b-2 themeable-border">
                        Ⅳ. 질의-응답 유형 최종 통계
                    </h2>

                    <p class="themeable-text-secondary mb-6">
                        LLM 학습 데이터 생성을 위해 총 <strong class="teal-text">13개</strong>의 로봇 데이터 그룹에 대해
                        <strong class="orange-text">52개</strong>의 질의응답 쌍을 구성했습니다.
                        로봇 지능의 피지컬 AI 특성을 반영한 질의 유형 통계는 다음과 같습니다.
                    </p>

                    <div class="overflow-x-auto mb-8">
                        <table class="w-full border-collapse">
                            <thead>
                                <tr class="bg-slate-800/80">
                                    <th class="border themeable-border px-4 py-3 text-left themeable-text-primary">질의 유형</th>
                                    <th class="border themeable-border px-4 py-3 text-left themeable-text-primary">정의</th>
                                    <th class="border themeable-border px-4 py-3 text-center themeable-text-primary">사용 횟수</th>
                                    <th class="border themeable-border px-4 py-3 text-center themeable-text-primary">비율</th>
                                </tr>
                            </thead>
                            <tbody class="themeable-text-secondary">
                                <tr class="hover:bg-orange-500/5 transition-colors">
                                    <td class="border themeable-border px-4 py-3"><strong class="teal-text">단순 정보 추출형</strong></td>
                                    <td class="border themeable-border px-4 py-3">문서에 명시된 사실적 정보를 직접 추출</td>
                                    <td class="border themeable-border px-4 py-3 text-center font-bold">13회</td>
                                    <td class="border themeable-border px-4 py-3 text-center font-bold">25.0%</td>
                                </tr>
                                <tr class="hover:bg-orange-500/5 transition-colors">
                                    <td class="border themeable-border px-4 py-3"><strong class="teal-text">요약 및 설명형</strong></td>
                                    <td class="border themeable-border px-4 py-3">특정 개념이나 프로세스에 대한 종합적 요약</td>
                                    <td class="border themeable-border px-4 py-3 text-center font-bold">13회</td>
                                    <td class="border themeable-border px-4 py-3 text-center font-bold">25.0%</td>
                                </tr>
                                <tr class="hover:bg-orange-500/5 transition-colors">
                                    <td class="border themeable-border px-4 py-3"><strong class="teal-text">비교 및 분석형</strong></td>
                                    <td class="border themeable-border px-4 py-3">둘 이상의 개념을 비교하여 차이점 분석</td>
                                    <td class="border themeable-border px-4 py-3 text-center font-bold">13회</td>
                                    <td class="border themeable-border px-4 py-3 text-center font-bold">25.0%</td>
                                </tr>
                                <tr class="hover:bg-orange-500/5 transition-colors">
                                    <td class="border themeable-border px-4 py-3"><strong class="teal-text">추론 및 적용형</strong></td>
                                    <td class="border themeable-border px-4 py-3">주어진 정보를 바탕으로 특정 상황의 결과 추론</td>
                                    <td class="border themeable-border px-4 py-3 text-center font-bold">13회</td>
                                    <td class="border themeable-border px-4 py-3 text-center font-bold">25.0%</td>
                                </tr>
                                <tr class="bg-orange-100/50">
                                    <td class="border themeable-border px-4 py-3 font-bold themeable-text-primary" colspan="2">총합</td>
                                    <td class="border themeable-border px-4 py-3 text-center font-bold orange-text">52회</td>
                                    <td class="border themeable-border px-4 py-3 text-center font-bold orange-text">100.0%</td>
                                </tr>
                            </tbody>
                        </table>
                    </div>

                    <div class="interactive-card border border-teal-500/40 rounded-lg p-6">
                        <h4 class="font-semibold teal-text mb-3">💡 핵심 특징</h4>
                        <p class="themeable-text-secondary">
                            로봇 지능 데이터의 핵심 요소인 <strong>정보 추출, 개념 설명, 비교 분석, 응용 추론</strong>
                            영역에 대해 <strong class="orange-text">균등하게 질문을 배분</strong>하여
                            LLM이 전 영역에 걸친 <strong class="teal-text">종합적인 지식</strong>을 학습하도록 설계되었습니다.
                        </p>
                    </div>
                </section>

                <!-- Section 5: 프롬프트 템플릿 -->
                <section id="prompt-template" class="mb-12">
                    <h2 class="text-3xl font-bold themeable-heading mb-6 pb-3 border-b-2 themeable-border">
                        Ⅴ. 도메인 LLM 보고서 생성을 위한 프롬프트 템플릿
                    </h2>

                    <p class="themeable-text-secondary mb-6">
                        이 프롬프트는 다른 도메인(예: 제조, 헬스케어, 자율주행 등)의 학습 데이터 문서가 주어졌을 때,
                        해당 도메인의 전문 지식을 LLM이 학습할 수 있도록
                        <strong class="teal-text">구조화된 QA 데이터셋 보고서</strong>를 생성하는 데 사용될 수 있습니다.
                    </p>

                    <div class="interactive-card border themeable-border rounded-lg p-6">
                        <h3 class="text-lg font-bold themeable-heading mb-4">Report Generation Prompt Template</h3>

                        <div class="bg-slate-900 text-slate-100 rounded-lg p-6 overflow-x-auto mb-4">
                            <pre class="text-sm leading-relaxed"><code>[지시사항]
당신은 Agentic AI Data Scientist (AADS) 과제에서 대규모 언어 모델(LLM) 파인튜닝을 위한
전문 QA 데이터셋을 구축하는 전문가입니다. 아래에 제시된 [INPUT: 분석 대상 문서]의 내용을 분석하여,
**'논리적 데이터 그룹'** 단위로 묶어 QA 보고서를 생성해야 합니다.

**[보고서 구성 요소]**
1. **보고서 제목:** 도메인 및 목적에 맞게 작성하십시오.
2. **논리적 데이터 그룹 식별:** 문서 내에서 동일한 프로젝트나 목표를 공유하는 문서들을
   하나의 '논리적 그룹'으로 묶습니다.
3. **QA 쌍 생성:** 각 논리적 그룹별로 **4개**의 질의응답(QA) 쌍을 생성해야 합니다.
4. **질의 유형 분류:** 생성된 QA 쌍은 다음 4가지 핵심 유형 중 하나로 분류되어야 합니다.
   * **단순 정보 추출형:** 문서에 명시된 사실적 정보를 직접 추출
   * **요약 및 설명형:** 특정 개념이나 프로세스에 대한 종합적 요약
   * **비교 및 분석형:** 둘 이상의 개념을 비교하여 차이점과 공통점 분석
   * **추론 및 적용형:** 주어진 정보를 바탕으로 특정 상황의 결과나 응용 추론
5. **출처 표기:** 응답의 모든 문장은 원본 문서의 출처를 명확하게 표기해야 합니다.
6. **최종 통계:** 생성된 모든 QA 쌍을 대상으로, 사용된 **4가지 유형의 최종 횟수와 비율**을
   정리해야 합니다.</code></pre>
                        </div>

                        <p class="text-sm themeable-text-muted">
                            <strong>활용 방법:</strong> 이 템플릿을 사용하여 다양한 도메인(헬스케어, 자율주행, 제조 등)의
                            학습 데이터 문서에서 고품질 QA 데이터셋을 자동으로 생성할 수 있습니다.
                        </p>
                    </div>
                </section>

                <!-- Section 6: 페블러스 관점 -->
                <section id="pebblous-perspective" class="mb-12">
                    <h2 class="text-3xl font-bold themeable-heading mb-6 pb-3 border-b-2 themeable-border">
                        페블러스 관점: 피지컬 AI 시대의 데이터 중심 접근법
                    </h2>

                    <div class="themeable-text-secondary space-y-6">
                        <p>
                            이번 AADS 과제에서 구축한 로봇 분야 QA 데이터셋은
                            <strong class="teal-text">피지컬 AI (Physical AI)</strong> 시대에
                            <strong class="orange-text">데이터 품질이 만드는 지능의 차이</strong>를 명확히 보여줍니다.
                        </p>

                        <div class="interactive-card border border-teal-500/40 rounded-lg p-6 mb-6">
                            <h3 class="text-xl font-bold themeable-heading mb-4">🎯 AADS의 차별화된 접근법</h3>
                            <ul class="space-y-3 themeable-text-secondary">
                                <li class="flex items-start">
                                    <span class="teal-text mr-2">•</span>
                                    <div>
                                        <strong class="themeable-text-primary">균형잡힌 지식 구조:</strong>
                                        4가지 질의 유형을 25%씩 균등 배분하여
                                        LLM이 편향되지 않은 전문성을 습득하도록 설계
                                    </div>
                                </li>
                                <li class="flex items-start">
                                    <span class="teal-text mr-2">•</span>
                                    <div>
                                        <strong class="themeable-text-primary">실무 중심 QA:</strong>
                                        13개 로봇 데이터셋의 실제 문서에서 추출한 사실 기반(Factual) 질의응답으로
                                        환각(Hallucination) 최소화
                                    </div>
                                </li>
                                <li class="flex items-start">
                                    <span class="teal-text mr-2">•</span>
                                    <div>
                                        <strong class="themeable-text-primary">확장 가능한 템플릿:</strong>
                                        프롬프트 템플릿을 통해 다른 도메인(헬스케어, 자율주행 등)으로
                                        즉시 확장 가능한 재사용 가능 구조
                                    </div>
                                </li>
                            </ul>
                        </div>

                        <div class="interactive-card border border-orange-500/40 rounded-lg p-6">
                            <h3 class="text-xl font-bold themeable-heading mb-4">🚀 Physical AI의 핵심: 센서 데이터 이해</h3>
                            <p class="themeable-text-secondary mb-3">
                                로봇 지능 데이터셋은 <strong>RGB-D 카메라, LiDAR, 촉각 센서, IMU</strong> 등
                                다양한 센서의 물리적 특성을 이해하는 것이 필수적입니다.
                            </p>
                            <p class="themeable-text-secondary">
                                AADS는 각 센서의 정보 수집 방식, 라벨링 클래스 차이, 모델 선택 기준을
                                <strong class="teal-text">비교 및 분석형 QA</strong>로 체계화하여,
                                LLM이 단순히 데이터를 나열하는 것을 넘어
                                <strong class="orange-text">"왜 이 센서가 이 작업에 적합한가?"</strong>를 추론하도록 설계했습니다.
                            </p>
                        </div>
                    </div>
                </section>

                <!-- Section 7: FAQ -->
                <section id="faq" class="mb-12">
                    <h2 class="text-3xl font-bold themeable-heading mb-6 pb-3 border-b-2 themeable-border">
                        자주 묻는 질문 (FAQ)
                    </h2>

                    <div class="space-y-4">
                        <!-- FAQ items will be rendered from config -->
                        <div class="interactive-card border themeable-border rounded-lg p-6">
                            <h3 class="text-xl font-semibold themeable-heading mb-3">
                                AADS가 로봇 분야 QA 데이터셋을 어떻게 구축하나요?
                            </h3>
                            <p class="themeable-text-secondary">
                                AADS는 AI 허브의 로봇 데이터셋 문서를 분석하여,
                                동일한 프로젝트 목표를 공유하는 문서들을 '논리적 데이터 그룹'으로 묶습니다.
                                각 그룹별로 4가지 질의 유형(정보 추출, 요약 설명, 비교 분석, 추론 적용)에서
                                각 1개씩 총 4개의 QA 쌍을 생성하여, 52개의 고품질 질의응답을 구축했습니다.
                            </p>
                        </div>

                        <div class="interactive-card border themeable-border rounded-lg p-6">
                            <h3 class="text-xl font-semibold themeable-heading mb-3">
                                로봇 데이터셋이 제조 데이터셋과 다른 점은 무엇인가요?
                            </h3>
                            <p class="themeable-text-secondary">
                                로봇 데이터셋은 <strong>센서 다양성</strong>(RGB-D, LiDAR, 촉각, IMU 등)과
                                <strong>실시간 의사결정</strong> 요구사항이 핵심입니다.
                                반면 제조 데이터셋은 품질 검사와 예지보전에 중점을 둡니다.
                                AADS는 각 도메인의 특성을 반영하여 QA 데이터셋을 구축합니다.
                            </p>
                        </div>

                        <div class="interactive-card border themeable-border rounded-lg p-6">
                            <h3 class="text-xl font-semibold themeable-heading mb-3">
                                가장 중요한 로봇 데이터셋은 무엇인가요?
                            </h3>
                            <p class="themeable-text-secondary">
                                <strong>가려진 객체 추론 데이터셋</strong>과 <strong>배송로봇 비도로 운행 데이터셋</strong>이
                                핵심입니다. 전자는 로봇의 인식 능력(occlusion handling), 후자는 주행 능력(비도로 환경)을
                                검증하는 데 필수적이며, 두 데이터셋 모두 멀티모달 센서 융합을 요구합니다.
                            </p>
                        </div>

                        <div class="interactive-card border themeable-border rounded-lg p-6">
                            <h3 class="text-xl font-semibold themeable-heading mb-3">
                                멀티모달 데이터가 LLM 파인튜닝에 어떤 이점을 제공하나요?
                            </h3>
                            <p class="themeable-text-secondary">
                                멀티모달 데이터(센서 값 + 이미지, LiDAR + RGB)는 LLM이 로봇의
                                <strong>물리적 현상을 종합적으로 이해</strong>하도록 돕습니다.
                                예를 들어 배송로봇 데이터는 2D 이미지(주행 가능 영역)와 3D LiDAR(동적 객체 탐지)를 결합하여,
                                LLM이 "평탄도가 낮은 인도에서는 속도를 줄여야 한다"와 같은
                                다차원 지식을 학습할 수 있게 합니다.
                            </p>
                        </div>

                        <div class="interactive-card border themeable-border rounded-lg p-6">
                            <h3 class="text-xl font-semibold themeable-heading mb-3">
                                AADS QA 데이터셋이 DataClinic과 어떻게 연계되나요?
                            </h3>
                            <p class="themeable-text-secondary">
                                AADS가 생성한 QA 데이터셋은 <strong>DataClinic의 데이터 품질 진단 파이프라인</strong>과 긴밀히 연계됩니다.
                                DataClinic이 센서 데이터의 이상치, 라벨링 오류, 불균형 분포 등을 자동 탐지하면,
                                AADS는 해당 품질 문제에 대한 QA 쌍을 학습하여
                                "이 LiDAR 데이터의 노이즈를 줄이려면 특정 필터를 적용해야 한다"와 같은
                                <strong>실무적 해결책</strong>을 제시할 수 있습니다.
                            </p>
                        </div>
                    </div>
                </section>

                <!-- Section 8: 데이터셋 출처 -->
                <section id="datasets-sources" class="mb-12">
                    <h2 class="text-3xl font-bold themeable-heading mb-6 pb-3 border-b-2 themeable-border">
                        관련 데이터셋 출처
                    </h2>

                    <p class="themeable-text-secondary mb-6">
                        이 보고서에서 분석한 13개 로봇 분야 데이터셋의 출처 정보입니다.
                    </p>

                    <ol class="space-y-3 text-sm themeable-text-secondary">
                        <li class="flex items-start">
                            <span class="font-bold teal-text mr-3 min-w-[2rem]">[1]</span>
                            <div>
                                <strong>가려진 객체 추론 데이터</strong><br>
                                <span class="text-xs themeable-text-muted">
                                    RGB-D 카메라 / 물체 3D 스캔, 다수 물체 가림, 파지 데이터
                                </span>
                            </div>
                        </li>
                        <li class="flex items-start">
                            <span class="font-bold teal-text mr-3 min-w-[2rem]">[2]</span>
                            <div>
                                <strong>배송로봇 비도로 운행 데이터</strong><br>
                                <span class="text-xs themeable-text-muted">
                                    2D 이미지 (Semantic Segmentation) + 3D LiDAR (Cuboid) / 9종 동적/정적 객체
                                </span>
                            </div>
                        </li>
                        <li class="flex items-start">
                            <span class="font-bold teal-text mr-3 min-w-[2rem]">[3]</span>
                            <div>
                                <strong>로봇 관점 주행 영상 데이터</strong><br>
                                <span class="text-xs themeable-text-muted">
                                    4족보행로봇(45.92%) + 바퀴주행로봇(54.08%) / RGB, PCD, CSV, Depth
                                </span>
                            </div>
                        </li>
                        <li class="flex items-start">
                            <span class="font-bold teal-text mr-3 min-w-[2rem]">[4]</span>
                            <div>
                                <strong>실내공간 유지관리 서비스 로봇 데이터</strong><br>
                                <span class="text-xs themeable-text-muted">
                                    JSON 기반 상태 데이터 / errorState, batteryLevel, collision 등
                                </span>
                            </div>
                        </li>
                        <li class="flex items-start">
                            <span class="font-bold teal-text mr-3 min-w-[2rem]">[5]</span>
                            <div>
                                <strong>로봇 핸드용 객체 특성 식별 데이터</strong><br>
                                <span class="text-xs themeable-text-muted">
                                    200종 가정용 물품 / Hi-RGB, Low-RGB, RGB-D / 무게, 크기, 재질, 촉감
                                </span>
                            </div>
                        </li>
                        <li class="flex items-start">
                            <span class="font-bold teal-text mr-3 min-w-[2rem]">[6]</span>
                            <div>
                                <strong>손·팔 협조 파지-조작 동작 데이터</strong><br>
                                <span class="text-xs themeable-text-muted">
                                    손 관절 2D/3D 좌표, 상체 포즈, 파지 가능 지점, 힘 센서 데이터
                                </span>
                            </div>
                        </li>
                        <li class="flex items-start">
                            <span class="font-bold teal-text mr-3 min-w-[2rem]">[7]</span>
                            <div>
                                <strong>사람 행동 인식 로봇 자율 행동 데이터</strong><br>
                                <span class="text-xs themeable-text-muted">
                                    1,000시간 이상 영상 / 10종 서비스 환경 / 키오스크 접근성 강화
                                </span>
                            </div>
                        </li>
                    </ol>
                </section>

                <!-- Section 9: 결론 -->
                <section id="conclusion" class="mb-12">
                    <h2 class="text-3xl font-bold themeable-heading mb-6 pb-3 border-b-2 themeable-border">
                        결론
                    </h2>

                    <div class="themeable-text-secondary space-y-4 mb-6">
                        <p>
                            본 보고서에서는 <strong class="orange-text">AADS 과제</strong>를 통해 구축한
                            <strong class="teal-text">로봇 지능 분야의 52쌍 QA 데이터셋</strong>을 소개했습니다.
                            13개 데이터셋은 <strong>가려진 객체 추론, 배송로봇 비도로 운행, 주행영상, 실내공간 유지관리,
                            객체 특성 식별, 파지-조작 동작, 사람 행동 인식</strong> 등 로봇 지능의 핵심 영역을 포괄합니다.
                        </p>
                        <p>
                            각 데이터셋은 <strong>도메인 정의, 데이터 구조, AI 모델, 품질 관리</strong>의 4가지 측면에서 체계적으로 분석되었으며,
                            <strong>단순 정보 추출형, 요약 및 설명형, 비교 및 분석형, 추론 및 적용형</strong>의 4가지 QA 유형으로 구성되어
                            LLM의 <strong class="teal-text">Physical AI 도메인 전문성</strong>을 강화합니다.
                        </p>
                        <p>
                            페블러스는 이 데이터셋을 통해 <strong class="orange-text">Few-Shot Learning</strong>과
                            <strong class="orange-text">프롬프트 엔지니어링</strong>을 적용하여,
                            실무 환경에서 LLM이 로봇 데이터 분석, 모델 선택, 품질 관리 등의 의사결정을 지원하도록 설계했습니다.
                        </p>
                        <p>
                            향후 AADS는 <strong>DataClinic</strong> 플랫폼과 연계하여,
                            로봇 지능 데이터셋의 자동 품질 진단 및 개선 권장사항을 제공하고,
                            도메인 전문가와 AI 모델 간의 협업을 강화할 계획입니다.
                        </p>
                    </div>
                </section>

                <!-- PDF Download Section -->
                <section id="pdf-download" class="mb-12">
                    <h2 class="text-3xl font-bold themeable-heading mb-6 pb-3 border-b-2 themeable-border">
                        📄 원본 보고서 다운로드
                    </h2>

                    <div class="themeable-bg card-hover rounded-lg p-8 text-center">
                        <div class="mb-4">
                            <svg class="w-16 h-16 mx-auto text-orange-500 mb-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                                <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z"></path>
                            </svg>
                            <h3 class="text-xl font-semibold themeable-heading mb-2">
                                AADS LLM 파인튜닝용 QA 데이터셋 구축: 로봇 분야
                            </h3>
                            <p class="themeable-text-secondary mb-4">
                                로봇 지능 분야의 13개 데이터셋과 52쌍의 QA 데이터를 상세히 담은 원본 보고서를 다운로드하세요.
                            </p>
                            <p class="text-sm themeable-text-muted mb-6">
                                <strong class="teal-text">웹 페이지의 모든 QA와 함께</strong> 추가 분석 자료 및 원문 텍스트가 포함되어 있습니다.
                            </p>
                        </div>

                        <a href="/project/AADS/source/AADS LLM 파인튜닝용 QA 데이터셋 구축_ 로봇 분야.pdf"
                           download="AADS_로봇분야_QA데이터셋_구축보고서.pdf"
                           class="inline-flex items-center gap-2 bg-orange-500 hover:bg-orange-600 text-white font-semibold px-6 py-3 rounded-lg transition-all transform hover:scale-105 shadow-lg hover:shadow-xl">
                            <svg class="w-5 h-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                                <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M12 10v6m0 0l-3-3m3 3l3-3m2 8H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"></path>
                            </svg>
                            <span>PDF 다운로드</span>
                        </a>

                        <div class="mt-6 text-sm themeable-text-muted">
                            <p>파일 형식: PDF | 작성일: 2025년 11월 30일 | 페블러스 데이터 커뮤니케이션 팀</p>
                        </div>
                    </div>
                </section>

            </main>
        </div>
    </div>

    <!-- Footer will be loaded by common-utils.js -->
    <div id="footer-placeholder"></div>

    <!-- Scripts -->
    <script src="/scripts/common-utils.js"></script>
    <script>
    document.addEventListener('DOMContentLoaded', async function() {
        // 공유 버튼 이벤트 핸들러
        const currentUrl = window.location.href;
        const pageTitle = document.title;

        document.getElementById('copy-url-btn').addEventListener('click', function() {
            navigator.clipboard.writeText(currentUrl).then(() => {
                const btn = this;
                const originalHTML = btn.innerHTML;
                btn.innerHTML = '<svg fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 13l4 4L19 7"></path></svg><span>복사됨!</span>';
                btn.classList.add('copied');
                setTimeout(() => {
                    btn.innerHTML = originalHTML;
                    btn.classList.remove('copied');
                }, 2000);
            });
        });

        document.getElementById('share-twitter-btn').addEventListener('click', function() {
            window.open(`https://twitter.com/intent/tweet?url=${encodeURIComponent(currentUrl)}&text=${encodeURIComponent(pageTitle)}`, '_blank');
        });

        document.getElementById('share-facebook-btn').addEventListener('click', function() {
            window.open(`https://www.facebook.com/sharer/sharer.php?u=${encodeURIComponent(currentUrl)}`, '_blank');
        });

        document.getElementById('share-linkedin-btn').addEventListener('click', function() {
            window.open(`https://www.linkedin.com/sharing/share-offsite/?url=${encodeURIComponent(currentUrl)}`, '_blank');
        });

        // PebblousPage initialization with config
        const config = {
            mainTitle: "로봇 분야 LLM 파인튜닝용 QA 데이터셋 구축: (1) 도메인 지식",
            subtitle: "AADS의 피지컬 AI 접근법",
            pageTitle: "로봇 분야 LLM 파인튜닝용 QA 데이터셋 구축: (1) 도메인 지식 - AADS의 피지컬 AI 접근법 | 페블러스",
            publishDate: "2025년 11월 30일",
            publisher: "페블러스 데이터 커뮤니케이션 팀",
            defaultTheme: "beige",
            category: "tech",
            articlePath: "project/AADS/robot-qa-dataset.html",
            tags: [
                "LLM 파인튜닝", "LLM Fine-tuning", "QA 데이터셋", "Question-Answer Dataset",
                "로봇 분야", "Robotics AI", "로봇 데이터", "Robot Data", "AADS", "Agentic AI Data Scientist",
                "피지컬 AI", "Physical AI", "데이터 품질", "Data Quality",
                "데이터 중심 AI", "Data-Centric AI", "멀티모달 데이터", "Multimodal Data",
                "도메인 지식", "Domain Knowledge", "가려진 객체 추론", "Occluded Object Detection",
                "배송로봇", "Delivery Robot", "비도로 운행", "Off-Road Navigation",
                "주행영상", "Driving Video", "실내공간 유지관리", "Indoor Maintenance",
                "서비스 로봇", "Service Robot", "객체 특성 식별", "Object Property Recognition",
                "로봇 핸드", "Robot Hand", "파지-조작 동작", "Grasp-Manipulation",
                "손·팔 협조", "Hand-Arm Coordination", "사람 행동 인식", "Human Activity Recognition",
                "로봇 자율 행동", "Robot Autonomous Behavior", "Few-Shot Learning", "퓨샷 러닝",
                "프롬프트 엔지니어링", "Prompt Engineering", "라벨링", "Labeling",
                "데이터 검수", "Data Validation", "mAP", "F1-score", "mIoU",
                "페블러스", "Pebblous", "DataClinic", "데이터클리닉"
            ],
            faqs: [
                {
                    question: "AADS가 로봇 분야 QA 데이터셋을 어떻게 구축하나요?",
                    answer: "AADS는 AI 허브의 로봇 지능 데이터셋 문서를 분석하여, 13개의 '논리적 데이터 그룹'으로 묶고, 각 그룹별로 도메인 정의, 데이터 구조, AI 모델, 품질 관리의 4가지 유형에서 QA 쌍을 생성하여 총 52개의 고품질 질의응답을 구축했습니다."
                },
                {
                    question: "로봇 데이터셋에서 멀티모달 데이터가 왜 중요한가요?",
                    answer: "로봇은 RGB 카메라, Depth 센서, LiDAR, IMU, GPS 등 다양한 센서를 통합하여 환경을 이해합니다. 멀티모달 데이터는 각 센서의 장점을 융합하여 강건한 인식 성능과 안전한 의사결정을 가능하게 합니다. 예를 들어, RGB는 색상을, Depth는 거리를, LiDAR는 3D 형상을 제공합니다."
                },
                {
                    question: "가려진 객체 추론 데이터셋의 실무 활용 사례는?",
                    answer: "물류 창고에서 로봇이 상자에 가려진 제품의 위치와 형상을 추론하여 효율적인 피킹 순서를 결정하거나, 자율주행 로봇이 부분적으로 보이는 장애물의 전체 형상을 예측하여 안전한 경로를 계획하는 데 활용됩니다."
                },
                {
                    question: "배송로봇 비도로 운행 데이터가 라스트마일 배송에 어떻게 기여하나요?",
                    answer: "보도, 공원, 주차장 등 다양한 비도로 환경에서 수집된 50,000장의 이미지와 날씨 조건(맑음, 비, 눈) 데이터를 학습하여, 배송로봇이 교통 혼잡 없이 안전하게 운행하며 배송 시간을 단축하고 비용을 절감합니다."
                },
                {
                    question: "로봇 핸드용 객체 특성 식별 데이터의 핵심 가치는?",
                    answer: "촉각 센서로 물체의 질감, 무게, 온도, 강성을 측정한 100,000회의 파지 데이터는 로봇이 적절한 파지력을 적용하여 파손을 방지하고, 식품 포장, 전자제품 조립, 의료 수술 등에서 정밀한 조작 능력을 발휘하도록 합니다."
                },
                {
                    question: "사람 행동 인식 데이터셋이 서비스 로봇에 어떻게 활용되나요?",
                    answer: "15,000개의 행동 영상(걷기, 앉기, 손 흔들기 등 20가지 클래스)과 Skeleton Keypoint 데이터를 통해, 안내 로봇이 사람의 손짓을 인식하여 방향을 안내하거나, 노약자의 낙상을 감지하여 도움을 제공하는 등 상황 인식 서비스를 구현합니다."
                },
                {
                    question: "AADS QA 데이터셋이 DataClinic과 어떻게 연계되나요?",
                    answer: "AADS의 로봇 QA 데이터셋은 DataClinic 플랫폼과 연계하여, 로봇 데이터의 자동 품질 진단(라벨 일관성, 센서 캘리브레이션, 시나리오 균형)과 개선 권장사항을 제공하며, 도메인 전문가와 AI 모델 간 협업을 통해 Physical AI의 실무 적용을 가속화합니다."
                }
            ]
        };

        await PebblousPage.init(config);
    });
    </script>
</body>
</html>
