<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="author" content="Pebblous Data Communication Team">
    <meta name="language" content="Korean">
    <meta name="copyright" content="© 2025 Pebblous. All rights reserved.">
    <meta name="rating" content="general">
    <meta name="revisit-after" content="7 days">
    <meta name="distribution" content="global">
    <meta name="audience" content="AI Researchers, Data Scientists, Robotics Engineers, LLM Developers, Physical AI Researchers">
    <meta name="topic" content="LLM Fine-tuning, QA Dataset, Robotics AI, Physical AI, Data Quality">
    <meta http-equiv="content-language" content="ko">

    <!-- Google Tag Manager -->
    <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
    new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
    j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
    'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','GTM-57L9F58B');</script>
    <!-- End Google Tag Manager -->

    <!-- Favicon -->
    <link rel="icon" href="/image/favicon.ico" sizes="any">
    <link rel="icon" href="/image/Pebblous_BM_Orange_RGB.png" type="image/png">
    <link rel="apple-touch-icon" href="/image/Pebblous_BM_Orange_RGB.png">

    <!-- SEO Meta Tags -->
    <title id="page-title">로봇 분야 LLM 파인튜닝용 QA 데이터셋 구축: AADS의 피지컬 AI 접근법 | 페블러스</title>
    <meta id="meta-description" name="description" content="페블러스 AADS가 로봇 지능 분야의 13개 도메인(가려진 객체 추론, 배송로봇, 주행영상, 실내공간 유지관리, 객체 특성 식별 등)에서 구축한 52쌍 LLM 파인튜닝용 QA 데이터셋. 로봇 데이터 수집부터 AI 모델 학습, 품질 관리까지 아우르는 데이터 중심 Physical AI 접근법을 소개합니다.">
    <meta id="meta-keywords" name="keywords" content="LLM 파인튜닝, LLM Fine-tuning, QA 데이터셋, Question-Answer Dataset, 로봇 분야, Robotics AI, 로봇 데이터, Robot Data, AADS, Agentic AI Data Scientist, 피지컬 AI, Physical AI, 데이터 품질, Data Quality, 데이터 중심 AI, Data-Centric AI, 멀티모달 데이터, Multimodal Data, 도메인 지식, Domain Knowledge, 가려진 객체 추론, Occluded Object Detection, 배송로봇, Delivery Robot, 비도로 운행, Off-Road Navigation, 주행영상, Driving Video, 실내공간 유지관리, Indoor Maintenance, 서비스 로봇, Service Robot, 객체 특성 식별, Object Property Recognition, 로봇 핸드, Robot Hand, 파지-조작 동작, Grasp-Manipulation, 손·팔 협조, Hand-Arm Coordination, 사람 행동 인식, Human Activity Recognition, 로봇 자율 행동, Robot Autonomous Behavior, Few-Shot Learning, 퓨샷 러닝, 프롬프트 엔지니어링, Prompt Engineering, 라벨링, Labeling, 데이터 검수, Data Validation, mAP, F1-score, mIoU, 페블러스, Pebblous, DataClinic, 데이터클리닉">
    <meta name="robots" content="index, follow">

    <link id="hreflang-ko" rel="alternate" hreflang="ko" href="https://blog.pebblous.ai/project/AADS/robot-qa-dataset.html">
    <link id="hreflang-en" rel="alternate" hreflang="en" href="https://blog.pebblous.ai/project/AADS/robot-qa-dataset.html">
    <link id="hreflang-default" rel="alternate" hreflang="x-default" href="https://blog.pebblous.ai/project/AADS/robot-qa-dataset.html">

    <link id="canonical-url" rel="canonical" href="https://blog.pebblous.ai/project/AADS/robot-qa-dataset.html">

    <meta id="og-url" property="og:url" content="https://blog.pebblous.ai/project/AADS/robot-qa-dataset.html">
    <meta id="og-title" property="og:title" content="로봇 분야 LLM 파인튜닝용 QA 데이터셋 구축: AADS의 피지컬 AI 접근법 | 페블러스">
    <meta id="og-description" property="og:description" content="13개 로봇 지능 도메인에서 52쌍의 고품질 QA 데이터셋을 구축한 AADS의 실무 사례. 도메인 정의, 데이터 구조, AI 모델, 품질 관리를 균형있게 다룬 데이터 중심 Physical AI 전략을 공개합니다.">
    <meta id="og-image" property="og:image" content="https://blog.pebblous.ai/image/Pebblous_BM_Orange_RGB.png">
    <meta property="og:image:width" content="1200">
    <meta property="og:image:height" content="630">
    <meta property="og:image:alt" content="AADS 로봇 분야 LLM 파인튜닝 QA 데이터셋 - 페블러스">
    <meta property="og:type" content="article">
    <meta property="og:site_name" content="Pebblous Blog">
    <meta property="og:locale" content="ko_KR">
    <meta property="article:published_time" content="2025-11-30T09:00:00+09:00">
    <meta property="article:modified_time" content="2025-11-30T09:00:00+09:00">
    <meta property="article:author" content="Pebblous Data Communication Team">
    <meta property="article:section" content="Technology">
    <meta property="article:tag" content="LLM Fine-tuning">
    <meta property="article:tag" content="Robotics AI">
    <meta property="article:tag" content="Physical AI">
    <meta property="article:tag" content="AADS">

    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:site" content="@Pebblous">
    <meta name="twitter:creator" content="@pebblous">
    <meta name="twitter:title" content="로봇 분야 LLM 파인튜닝용 QA 데이터셋 구축: AADS의 피지컬 AI 접근법">
    <meta name="twitter:description" content="13개 로봇 지능 도메인에서 52쌍의 고품질 QA 데이터셋을 구축한 AADS의 실무 사례. 도메인 정의, 데이터 구조, AI 모델, 품질 관리를 균형있게 다룬 데이터 중심 Physical AI 전략.">
    <meta name="twitter:image" content="https://blog.pebblous.ai/image/Pebblous_BM_Orange_RGB.png">
    <meta name="twitter:image:alt" content="AADS 로봇 분야 LLM 파인튜닝 QA 데이터셋">
    <meta name="twitter:label1" content="읽는 시간">
    <meta name="twitter:data1" content="15분">
    <meta name="twitter:label2" content="난이도">
    <meta name="twitter:data2" content="중급">

    <!-- JSON-LD Structured Data for SEO -->
    <script type="application/ld+json">
    {
        "@context": "https://schema.org",
        "@type": "TechArticle",
        "headline": "로봇 분야 LLM 파인튜닝용 QA 데이터셋 구축: AADS의 피지컬 AI 접근법",
        "alternativeHeadline": "13개 로봇 지능 도메인에서 52쌍의 고품질 QA 데이터셋을 구축한 데이터 중심 Physical AI 전략",
        "description": "페블러스 AADS가 로봇 지능 분야의 13개 도메인(가려진 객체 추론, 배송로봇, 주행영상, 실내공간 유지관리, 객체 특성 식별 등)에서 구축한 52쌍 LLM 파인튜닝용 QA 데이터셋. 로봇 데이터 수집부터 AI 모델 학습, 품질 관리까지 아우르는 데이터 중심 Physical AI 접근법을 소개합니다.",
        "image": {
            "@type": "ImageObject",
            "url": "https://blog.pebblous.ai/image/Pebblous_BM_Orange_RGB.png",
            "width": 1200,
            "height": 630
        },
        "author": {
            "@type": "Organization",
            "name": "Pebblous",
            "url": "https://www.pebblous.ai",
            "logo": {
                "@type": "ImageObject",
                "url": "https://www.pebblous.ai/image/Pebblous_BM_Orange_RGB.png"
            },
            "description": "페블러스는 AI-Ready Data 솔루션을 제공하는 딥테크 기업입니다."
        },
        "publisher": {
            "@type": "Organization",
            "name": "Pebblous",
            "url": "https://www.pebblous.ai",
            "logo": {
                "@type": "ImageObject",
                "url": "https://www.pebblous.ai/image/Pebblous_BM_Orange_RGB.png",
                "width": 600,
                "height": 60
            },
            "sameAs": [
                "https://www.linkedin.com/company/pebblous",
                "https://github.com/pebblous"
            ]
        },
        "datePublished": "2025-11-30T09:00:00+09:00",
        "dateModified": "2025-11-30T09:00:00+09:00",
        "mainEntityOfPage": {
            "@type": "WebPage",
            "@id": "https://blog.pebblous.ai/project/AADS/robot-qa-dataset.html"
        },
        "keywords": "LLM 파인튜닝, QA 데이터셋, 로봇 분야, 로봇 데이터, AADS, 피지컬 AI, 데이터 품질, 데이터 중심 AI, 멀티모달 데이터, 도메인 지식, Robotics AI, Robot Data, Physical AI",
        "articleSection": "Technology",
        "inLanguage": "ko-KR",
        "isAccessibleForFree": true,
        "proficiencyLevel": "Intermediate"
    }
    </script>

    <!-- FAQ Schema is dynamically generated by common-utils.js from config.faqs -->

    <!-- Stylesheets -->
    <link rel="stylesheet" href="/styles/common-styles.css">
    <script src="https://cdn.tailwindcss.com"></script>

    <!-- Fonts -->
    <link rel="stylesheet" as="style" crossorigin
          href="https://cdn.jsdelivr.net/gh/orioncactus/pretendard@v1.3.9/dist/web/variable/pretendardvariable.min.css">

    <style>
        /* Theme Variables - Light as default */
        :root {
            --bg-primary: #F9FAFB;
            --bg-secondary: #F3F4F6;
            --bg-card: rgba(255, 255, 255, 0.95);
            --text-primary: #111827;
            --text-secondary: #4B5563;
            --text-muted: #6B7280;
            --heading-color: #111827;
            --border-color: #E5E7EB;
            --accent-color: #F86825;
            --teal-color: #0d9488;
        }

        [data-theme="dark"] {
            --bg-primary: #020617;
            --bg-secondary: #0f172a;
            --bg-card: rgba(30, 41, 59, 0.95);
            --text-primary: #F1F5F9;
            --text-secondary: #CBD5E1;
            --text-muted: #94A3B8;
            --heading-color: #F1F5F9;
            --border-color: #334155;
            --accent-color: #F86825;
            --teal-color: #14b8a6;
        }

        body {
            background-color: var(--bg-primary);
            color: var(--text-primary);
            font-family: 'Pretendard Variable', -apple-system, BlinkMacSystemFont, system-ui, Roboto, sans-serif;
            transition: background-color 0.3s ease, color 0.3s ease;
            line-height: 1.7;
        }

        .themeable-bg {
            background-color: var(--bg-card);
            transition: background-color 0.3s ease;
        }

        .themeable-text {
            color: var(--text-primary);
        }

        .themeable-text-secondary {
            color: var(--text-secondary);
        }

        .themeable-text-muted {
            color: var(--text-muted);
        }

        .themeable-heading {
            color: var(--heading-color);
        }

        .themeable-border {
            border-color: var(--border-color);
        }

        .orange-text {
            color: var(--accent-color);
        }

        .themeable-toc-border {
            border-color: var(--teal-color);
        }

        .teal-text {
            color: var(--teal-color);
        }

        .card-hover {
            transition: all 0.3s ease;
            border: 1px solid var(--border-color);
        }

        .card-hover:hover {
            border-color: var(--teal-color);
            box-shadow: 0 4px 12px rgba(20, 184, 166, 0.15);
        }

        /* Share buttons styling */
        .share-container {
            display: flex;
            align-items: center;
            justify-content: center;
            gap: 0.75rem;
            flex-wrap: wrap;
        }

        .share-label {
            font-size: 0.875rem;
            color: var(--text-secondary);
            font-weight: 500;
        }

        .share-btn {
            display: inline-flex;
            align-items: center;
            gap: 0.375rem;
            padding: 0.5rem 0.875rem;
            border-radius: 0.5rem;
            font-size: 0.875rem;
            font-weight: 500;
            transition: all 0.2s ease;
            border: 1px solid var(--border-color);
            background-color: var(--bg-card);
            color: var(--text-secondary);
            cursor: pointer;
        }

        .share-btn:hover {
            background-color: var(--accent-color);
            color: white;
            border-color: var(--accent-color);
            transform: translateY(-2px);
            box-shadow: 0 4px 8px rgba(248, 104, 37, 0.2);
        }

        .share-btn svg {
            width: 1rem;
            height: 1rem;
        }

        .share-btn.copied {
            background-color: var(--teal-color);
            color: white;
            border-color: var(--teal-color);
        }

        /* Table styling */
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 1.5rem 0;
            font-size: 0.9rem;
        }

        thead {
            background-color: rgba(248, 104, 37, 0.1);
        }

        th {
            padding: 1rem;
            text-align: left;
            font-weight: 600;
            color: var(--heading-color);
            border-bottom: 2px solid var(--border-color);
        }

        td {
            padding: 1rem;
            border-bottom: 1px solid var(--border-color);
            color: var(--text-secondary);
        }

        tbody tr:hover {
            background-color: rgba(248, 104, 37, 0.05);
        }

        /* Details/Summary styling */
        details {
            margin: 1rem 0;
            padding: 1rem;
            border: 1px solid var(--border-color);
            border-radius: 0.5rem;
            background-color: var(--bg-card);
        }

        details summary {
            cursor: pointer;
            font-weight: 600;
            color: var(--teal-color);
            padding: 0.5rem;
            user-select: none;
        }

        details summary:hover {
            color: var(--accent-color);
        }

        details[open] summary {
            margin-bottom: 1rem;
            padding-bottom: 0.5rem;
            border-bottom: 1px solid var(--border-color);
        }

        /* Code styling */
        code {
            background-color: rgba(248, 104, 37, 0.1);
            padding: 0.2rem 0.4rem;
            border-radius: 0.25rem;
            font-family: 'Courier New', monospace;
            font-size: 0.9em;
            color: var(--accent-color);
        }

        /* List styling */
        ul, ol {
            margin-left: 1.5rem;
            margin-bottom: 1.5rem;
        }

        li {
            margin-bottom: 0.75rem;
            line-height: 1.7;
        }

        /* Strong text */
        strong {
            font-weight: 600;
            color: var(--heading-color);
        }
    </style>
</head>
<body>
    <!-- Google Tag Manager (noscript) -->
    <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-57L9F58B"
    height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
    <!-- End Google Tag Manager (noscript) -->

    <!-- Header will be loaded by common-utils.js -->
    <div id="header-placeholder"></div>

    <!-- Main Content -->
    <div class="container mx-auto px-4 sm:px-6 lg:px-8 py-8 md:py-12 max-w-[1400px]">
        <div class="lg:flex lg:gap-8 lg:justify-center lg:items-start">

            <!-- TOC Sidebar -->
            <nav class="hidden lg:block lg:w-[240px] lg:shrink-0 sticky top-20 self-start">
                <h3 class="font-bold themeable-heading mb-4 text-lg">목차</h3>
                <ul id="toc-links" class="space-y-3 text-sm border-l-2 themeable-toc-border pl-4">
                    <li><a href="#intro" class="toc-link themeable-text-secondary hover:text-teal-500 transition-colors">서론 및 구축 목표</a></li>
                    <li><a href="#robot-datasets" class="toc-link themeable-text-secondary hover:text-teal-500 transition-colors">로봇 지능 데이터셋</a>
                        <ul class="ml-4 mt-2 space-y-2">
                            <li><a href="#dataset-1" class="toc-link themeable-text-secondary hover:text-teal-500 transition-colors text-xs">1. 가려진 객체 추론</a></li>
                            <li><a href="#dataset-2" class="toc-link themeable-text-secondary hover:text-teal-500 transition-colors text-xs">2. 배송로봇 비도로 운행</a></li>
                            <li><a href="#dataset-3" class="toc-link themeable-text-secondary hover:text-teal-500 transition-colors text-xs">3. 로봇 관점 주행 영상</a></li>
                            <li><a href="#dataset-4" class="toc-link themeable-text-secondary hover:text-teal-500 transition-colors text-xs">4. 실내공간 유지관리</a></li>
                            <li><a href="#dataset-5" class="toc-link themeable-text-secondary hover:text-teal-500 transition-colors text-xs">5. 로봇 핸드 객체 특성</a></li>
                            <li><a href="#dataset-6" class="toc-link themeable-text-secondary hover:text-teal-500 transition-colors text-xs">6. 손·팔 협조 파지-조작</a></li>
                            <li><a href="#dataset-7" class="toc-link themeable-text-secondary hover:text-teal-500 transition-colors text-xs">7. 사람 행동 인식</a></li>
                        </ul>
                    </li>
                    <li><a href="#conclusion" class="toc-link themeable-text-secondary hover:text-teal-500 transition-colors">결론</a></li>
                </ul>
            </nav>

            <!-- Main Article -->
            <main class="max-w-[800px] px-4 sm:px-6">

                <header class="text-center mb-16">
                    <h1 id="page-h1-title" class="text-4xl md:text-5xl font-bold themeable-heading mb-6 leading-tight" style="line-height: 1.4;">
                        로봇 분야 LLM 파인튜닝용 QA 데이터셋 구축
                    </h1>

                    <!-- 발행 정보 -->
                    <div class="flex flex-wrap justify-center items-center gap-2 text-sm text-slate-400 mb-8">
                        <span id="publish-date">작성일: 2025년 11월 30일</span>
                        <span class="text-slate-600">|</span>
                        <span id="publisher">기획: 페블러스 데이터 커뮤니케이션 팀</span>
                        <span class="text-slate-600">|</span>
                        <span>읽는 시간: 약 15분</span>
                    </div>

                    <!-- 공유하기 -->
                    <div class="share-container mb-8">
                        <span class="share-label">공유하기:</span>
                        <button id="copy-url-btn" class="share-btn" title="URL 복사">
                            <svg fill="none" stroke="currentColor" viewBox="0 0 24 24">
                                <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M8 16H6a2 2 0 01-2-2V6a2 2 0 012-2h8a2 2 0 012 2v2m-6 12h8a2 2 0 002-2v-8a2 2 0 00-2-2h-8a2 2 0 00-2 2v8a2 2 0 002 2z"></path>
                            </svg>
                            <span>URL</span>
                        </button>
                        <button id="share-twitter-btn" class="share-btn" title="Twitter에 공유">
                            <svg fill="currentColor" viewBox="0 0 24 24">
                                <path d="M18.244 2.25h3.308l-7.227 8.26 8.502 11.24H16.17l-5.214-6.817L4.99 21.75H1.68l7.73-8.835L1.254 2.25H8.08l4.713 6.231zm-1.161 17.52h1.833L7.084 4.126H5.117z"></path>
                            </svg>
                        </button>
                        <button id="share-facebook-btn" class="share-btn" title="Facebook에 공유">
                            <svg fill="currentColor" viewBox="0 0 24 24">
                                <path d="M24 12.073c0-6.627-5.373-12-12-12s-12 5.373-12 12c0 5.99 4.388 10.954 10.125 11.854v-8.385H7.078v-3.47h3.047V9.43c0-3.007 1.792-4.669 4.533-4.669 1.312 0 2.686.235 2.686.235v2.953H15.83c-1.491 0-1.956.925-1.956 1.874v2.25h3.328l-.532 3.47h-2.796v8.385C19.612 23.027 24 18.062 24 12.073z"></path>
                            </svg>
                        </button>
                        <button id="share-linkedin-btn" class="share-btn" title="LinkedIn에 공유">
                            <svg fill="currentColor" viewBox="0 0 24 24">
                                <path d="M20.447 20.452h-3.554v-5.569c0-1.328-.027-3.037-1.852-3.037-1.853 0-2.136 1.445-2.136 2.939v5.667H9.351V9h3.414v1.561h.046c.477-.9 1.637-1.85 3.37-1.85 3.601 0 4.267 2.37 4.267 5.455v6.286zM5.337 7.433c-1.144 0-2.063-.926-2.063-2.065 0-1.138.92-2.063 2.063-2.063 1.14 0 2.064.925 2.064 2.063 0 1.139-.925 2.065-2.064 2.065zm1.782 13.019H3.555V9h3.564v11.452zM22.225 0H1.771C.792 0 0 .774 0 1.729v20.542C0 23.227.792 24 1.771 24h20.451C23.2 24 24 23.227 24 22.271V1.729C24 .774 23.2 0 22.222 0h.003z"></path>
                            </svg>
                        </button>
                    </div>
                </header>

                <!-- Section 1: 서론 및 구축 목표 -->
                <section id="intro" class="mb-12">
                    <h2 class="text-3xl font-bold themeable-heading mb-6 pb-3 border-b-2 themeable-border">
                        Ⅰ. 서론 및 구축 목표
                    </h2>

                    <div class="themeable-text-secondary space-y-4 mb-6">
                        <p>
                            이 보고서는 <strong class="teal-text">Agentic AI Data Scientist (AADS)</strong> 과제에서
                            대규모 언어 모델(LLM)의 <strong class="orange-text">로봇 지능 도메인 전문성 강화</strong>를 목표로,
                            실제 AI 허브 데이터셋 문서를 기반으로 구축한 <strong>52쌍의 QA(Question-Answer) 데이터셋</strong>을 소개합니다.
                        </p>
                        <p>
                            페블러스는 로봇 분야의 <strong class="orange-text">13개 데이터셋</strong>을 '논리적 데이터 그룹'으로 재구성하여,
                            각 그룹별로 <strong>4가지 유형(단순 정보 추출형, 요약 및 설명형, 비교 및 분석형, 추론 및 적용형)</strong>의
                            질의응답을 체계적으로 생성했습니다. 이를 통해 LLM이 로봇 데이터 수집부터 AI 모델 학습, 품질 관리까지
                            <strong class="teal-text">Physical AI의 전체 생명주기</strong>를 이해하도록 설계하였습니다.
                        </p>
                        <p>
                            본 데이터셋은 <strong>가려진 객체 추론, 배송로봇 비도로 운행, 주행영상, 실내공간 유지관리, 객체 특성 식별,
                            파지-조작 동작, 사람 행동 인식</strong> 등 로봇 지능의 핵심 영역을 망라하며,
                            Few-Shot Learning, 프롬프트 엔지니어링을 통해 실무 환경에서 즉시 활용 가능한 형태로 구성되었습니다.
                        </p>
                    </div>
                </section>

                <!-- Section 2: 로봇 지능 데이터셋 기반 QA 구축 -->
                <section id="robot-datasets" class="mb-12">
                    <h2 class="text-3xl font-bold themeable-heading mb-6 pb-3 border-b-2 themeable-border">
                        Ⅱ. 로봇 지능 데이터셋 기반 QA 구축
                    </h2>

                    <div class="themeable-text-secondary space-y-4 mb-8">
                        <p>
                            로봇 지능 분야는 <strong class="orange-text">센서 데이터의 다양성</strong>과
                            <strong class="teal-text">실시간 의사결정 요구사항</strong>으로 인해 높은 데이터 품질이 필수적입니다.
                            AADS는 AI 허브에서 수집한 13개 로봇 데이터셋을 분석하여, 각 데이터셋의 특성에 맞는 4가지 유형의
                            QA 쌍을 생성했습니다.
                        </p>
                        <p>
                            각 데이터셋은 <strong>도메인 정의</strong>(데이터 수집 목적 및 구성),
                            <strong>데이터 구조</strong>(라벨링 방식 및 환경),
                            <strong>AI 모델</strong>(학습 알고리즘 및 응용),
                            <strong>품질 관리</strong>(검수 기준 및 성능 지표)의 4개 측면에서 체계적으로 문서화되었습니다.
                        </p>
                    </div>

                    <!-- Dataset 1: 가려진 객체 추론 데이터셋 -->
                    <div id="dataset-1" class="mb-8">
                        <h3 class="text-2xl font-semibold themeable-heading mb-4 flex items-center">
                            <span class="teal-text mr-2">1.</span>
                            가려진 객체 추론 데이터셋
                        </h3>

                        <!-- 1-1: 개요 및 구성 -->
                        <div class="themeable-bg card-hover rounded-lg p-6 mb-6">
                            <h4 class="text-xl font-semibold mb-4 teal-text">1-1. 개요 및 구성</h4>

                            <div class="themeable-text-secondary space-y-3 mb-4">
                                <p>
                                    <strong>가려진 객체 추론 데이터셋</strong>은 로봇이 시각적으로 가려진 물체의 3D 형상과 위치를 추론하도록 설계되었습니다.
                                    RGB-D 카메라로 촬영한 이미지에서 부분적으로 보이는 물체의 전체 형상을 예측하는 것이 핵심 과제입니다.
                                </p>
                                <p>
                                    데이터셋은 <strong>100,030장의 이미지</strong>로 구성되며, 각 이미지는 <code>원본이미지</code>,
                                    <code>RGB 이미지</code>, <code>Depth 이미지</code>, <code>Segmentation 이미지</code>,
                                    <code>3D Mesh</code>, <code>BBox 정보</code> 등을 포함합니다.
                                </p>
                            </div>

                            <details class="mt-4">
                                <summary>QA 예제 보기 (4쌍)</summary>
                                <div class="mt-4">
                                    <table>
                                        <thead>
                                            <tr>
                                                <th>유형</th>
                                                <th>질문</th>
                                                <th>답변</th>
                                            </tr>
                                        </thead>
                                        <tbody>
                                            <tr>
                                                <td>단순 정보 추출형</td>
                                                <td>가려진 객체 추론 데이터셋의 총 이미지 수는?</td>
                                                <td>100,030장입니다.</td>
                                            </tr>
                                            <tr>
                                                <td>요약 및 설명형</td>
                                                <td>이 데이터셋의 구성 요소를 설명해 주세요.</td>
                                                <td>원본이미지, RGB 이미지, Depth 이미지, Segmentation 이미지, 3D Mesh, BBox 정보로 구성됩니다.</td>
                                            </tr>
                                            <tr>
                                                <td>비교 및 분석형</td>
                                                <td>RGB-D 데이터의 장점은 무엇인가요?</td>
                                                <td>RGB 이미지는 색상 정보를, Depth 이미지는 거리 정보를 제공하여 3D 추론 성능을 향상시킵니다.</td>
                                            </tr>
                                            <tr>
                                                <td>추론 및 적용형</td>
                                                <td>이 데이터셋이 물류 로봇에 어떻게 활용될 수 있나요?</td>
                                                <td>상자에 가려진 물체의 위치와 형상을 추론하여 효율적인 피킹 순서를 결정할 수 있습니다.</td>
                                            </tr>
                                        </tbody>
                                    </table>
                                </div>
                            </details>
                        </div>

                        <!-- 1-2: 수집 환경 및 도구 -->
                        <div class="themeable-bg card-hover rounded-lg p-6 mb-6">
                            <h4 class="text-xl font-semibold mb-4 teal-text">1-2. 수집 환경 및 도구</h4>

                            <div class="themeable-text-secondary space-y-3 mb-4">
                                <p>
                                    데이터는 <strong>Realsense D435i</strong> RGB-D 카메라를 사용하여 다양한 조명 조건(자연광, 형광등)에서 수집되었습니다.
                                    촬영 각도는 <strong>정면, 측면, 상단</strong>의 3가지로 설정되어 물체의 가려짐 정도를 다양화했습니다.
                                </p>
                                <p>
                                    라벨링은 <strong>LabelMe</strong> 도구를 사용하여 객체의 경계 상자(BBox)와 세그멘테이션 마스크를 수동 주석했으며,
                                    3D Mesh는 <strong>Open3D</strong> 라이브러리로 생성되었습니다.
                                </p>
                            </div>

                            <details class="mt-4">
                                <summary>QA 예제 보기 (4쌍)</summary>
                                <div class="mt-4">
                                    <table>
                                        <thead>
                                            <tr>
                                                <th>유형</th>
                                                <th>질문</th>
                                                <th>답변</th>
                                            </tr>
                                        </thead>
                                        <tbody>
                                            <tr>
                                                <td>단순 정보 추출형</td>
                                                <td>데이터 수집에 사용된 카메라 모델은?</td>
                                                <td>Realsense D435i입니다.</td>
                                            </tr>
                                            <tr>
                                                <td>요약 및 설명형</td>
                                                <td>촬영 각도는 어떻게 설정되었나요?</td>
                                                <td>정면, 측면, 상단의 3가지 각도로 촬영하여 가려짐 정도를 다양화했습니다.</td>
                                            </tr>
                                            <tr>
                                                <td>비교 및 분석형</td>
                                                <td>LabelMe와 Open3D의 역할 차이는?</td>
                                                <td>LabelMe는 2D 주석(BBox, 세그멘테이션)을, Open3D는 3D Mesh 생성을 담당합니다.</td>
                                            </tr>
                                            <tr>
                                                <td>추론 및 적용형</td>
                                                <td>다양한 조명 조건이 모델 성능에 미치는 영향은?</td>
                                                <td>자연광과 형광등 환경에서 수집된 데이터로 학습하면 조명 변화에 강건한 모델을 만들 수 있습니다.</td>
                                            </tr>
                                        </tbody>
                                    </table>
                                </div>
                            </details>
                        </div>

                        <!-- 1-3: 학습 모델 및 응용 -->
                        <div class="themeable-bg card-hover rounded-lg p-6 mb-6">
                            <h4 class="text-xl font-semibold mb-4 teal-text">1-3. 학습 모델 및 응용</h4>

                            <div class="themeable-text-secondary space-y-3 mb-4">
                                <p>
                                    주요 학습 모델은 <strong>Mask R-CNN</strong> (세그멘테이션), <strong>PointNet++</strong> (3D 형상 추론),
                                    <strong>6D Pose Estimation</strong> (객체 자세 추정)입니다.
                                </p>
                                <p>
                                    이 데이터셋은 <strong>로봇 피킹 시스템</strong>에서 가려진 물체의 위치를 추론하고,
                                    <strong>자율주행 로봇</strong>이 부분적으로 보이는 장애물의 전체 형상을 예측하는 데 활용됩니다.
                                </p>
                            </div>

                            <details class="mt-4">
                                <summary>QA 예제 보기 (4쌍)</summary>
                                <div class="mt-4">
                                    <table>
                                        <thead>
                                            <tr>
                                                <th>유형</th>
                                                <th>질문</th>
                                                <th>답변</th>
                                            </tr>
                                        </thead>
                                        <tbody>
                                            <tr>
                                                <td>단순 정보 추출형</td>
                                                <td>세그멘테이션에 사용된 모델은?</td>
                                                <td>Mask R-CNN입니다.</td>
                                            </tr>
                                            <tr>
                                                <td>요약 및 설명형</td>
                                                <td>PointNet++의 역할은 무엇인가요?</td>
                                                <td>Point Cloud 데이터로부터 객체의 3D 형상을 추론합니다.</td>
                                            </tr>
                                            <tr>
                                                <td>비교 및 분석형</td>
                                                <td>6D Pose Estimation과 3D Shape Inference의 차이는?</td>
                                                <td>Pose Estimation은 객체의 위치와 방향을, Shape Inference는 전체 형상을 추론합니다.</td>
                                            </tr>
                                            <tr>
                                                <td>추론 및 적용형</td>
                                                <td>이 데이터셋이 창고 자동화에 어떻게 기여하나요?</td>
                                                <td>상자에 가려진 제품의 형상을 추론하여 로봇 피킹의 성공률을 높이고, 작업 시간을 단축합니다.</td>
                                            </tr>
                                        </tbody>
                                    </table>
                                </div>
                            </details>
                        </div>
                    </div>

                    <!-- Dataset 2: 배송로봇 비도로 운행 데이터셋 -->
                    <div id="dataset-2" class="mb-8">
                        <h3 class="text-2xl font-semibold themeable-heading mb-4 flex items-center">
                            <span class="teal-text mr-2">2.</span>
                            배송로봇 비도로 운행 데이터셋
                        </h3>

                        <!-- 2-1: 개요 및 라벨링 -->
                        <div class="themeable-bg card-hover rounded-lg p-6 mb-6">
                            <h4 class="text-xl font-semibold mb-4 teal-text">2-1. 개요 및 라벨링</h4>

                            <div class="themeable-text-secondary space-y-3 mb-4">
                                <p>
                                    <strong>배송로봇 비도로 운행 데이터셋</strong>은 야외 환경(보도, 공원, 주차장 등)에서 자율주행 배송로봇이
                                    안전하게 운행하도록 설계되었습니다. 총 <strong>50,000장의 이미지</strong>와 <strong>10,000개의 주행 시나리오</strong>로 구성됩니다.
                                </p>
                                <p>
                                    라벨링은 <strong>보행자, 차량, 장애물, 도로 표면</strong> 등 4가지 주요 클래스에 대해
                                    <strong>Bounding Box</strong>와 <strong>Semantic Segmentation</strong>으로 수행되었습니다.
                                </p>
                            </div>

                            <details class="mt-4">
                                <summary>QA 예제 보기 (4쌍)</summary>
                                <div class="mt-4">
                                    <table>
                                        <thead>
                                            <tr>
                                                <th>유형</th>
                                                <th>질문</th>
                                                <th>답변</th>
                                            </tr>
                                        </thead>
                                        <tbody>
                                            <tr>
                                                <td>단순 정보 추출형</td>
                                                <td>데이터셋의 총 이미지 수는?</td>
                                                <td>50,000장입니다.</td>
                                            </tr>
                                            <tr>
                                                <td>요약 및 설명형</td>
                                                <td>주요 라벨링 클래스를 나열해 주세요.</td>
                                                <td>보행자, 차량, 장애물, 도로 표면입니다.</td>
                                            </tr>
                                            <tr>
                                                <td>비교 및 분석형</td>
                                                <td>Bounding Box와 Semantic Segmentation의 차이는?</td>
                                                <td>Bounding Box는 객체 위치를, Segmentation은 픽셀 단위 클래스를 표현합니다.</td>
                                            </tr>
                                            <tr>
                                                <td>추론 및 적용형</td>
                                                <td>이 데이터셋이 배송 효율성을 어떻게 개선하나요?</td>
                                                <td>다양한 비도로 환경에서 안전한 경로 계획을 학습하여 배송 시간을 단축하고 사고를 예방합니다.</td>
                                            </tr>
                                        </tbody>
                                    </table>
                                </div>
                            </details>
                        </div>

                        <!-- 2-2: 수집 장소 및 환경 -->
                        <div class="themeable-bg card-hover rounded-lg p-6 mb-6">
                            <h4 class="text-xl font-semibold mb-4 teal-text">2-2. 수집 장소 및 환경</h4>

                            <div class="themeable-text-secondary space-y-3 mb-4">
                                <p>
                                    데이터는 <strong>보도, 공원, 주차장, 캠퍼스</strong> 등 4가지 비도로 환경에서 수집되었으며,
                                    <strong>맑음, 흐림, 비, 눈</strong>의 4가지 날씨 조건을 포함합니다.
                                </p>
                                <p>
                                    촬영 시간대는 <strong>오전(06:00-12:00), 오후(12:00-18:00), 저녁(18:00-24:00)</strong>으로 구분되어
                                    조명 변화에 강건한 모델 학습을 지원합니다.
                                </p>
                            </div>

                            <details class="mt-4">
                                <summary>QA 예제 보기 (4쌍)</summary>
                                <div class="mt-4">
                                    <table>
                                        <thead>
                                            <tr>
                                                <th>유형</th>
                                                <th>질문</th>
                                                <th>답변</th>
                                            </tr>
                                        </thead>
                                        <tbody>
                                            <tr>
                                                <td>단순 정보 추출형</td>
                                                <td>수집 장소의 종류는?</td>
                                                <td>보도, 공원, 주차장, 캠퍼스입니다.</td>
                                            </tr>
                                            <tr>
                                                <td>요약 및 설명형</td>
                                                <td>날씨 조건은 어떻게 구성되었나요?</td>
                                                <td>맑음, 흐림, 비, 눈의 4가지 조건으로 구성되었습니다.</td>
                                            </tr>
                                            <tr>
                                                <td>비교 및 분석형</td>
                                                <td>오전과 저녁 데이터의 조명 차이가 중요한 이유는?</td>
                                                <td>시간대별 조명 변화를 학습하면 배송로봇이 하루 종일 안정적으로 운행할 수 있습니다.</td>
                                            </tr>
                                            <tr>
                                                <td>추론 및 적용형</td>
                                                <td>비와 눈 환경 데이터가 실제 배송에 어떻게 기여하나요?</td>
                                                <td>악천후 조건에서도 안전한 주행을 학습하여 배송 서비스의 연속성을 보장합니다.</td>
                                            </tr>
                                        </tbody>
                                    </table>
                                </div>
                            </details>
                        </div>

                        <!-- 2-3: 학습 모델 및 응용 -->
                        <div class="themeable-bg card-hover rounded-lg p-6 mb-6">
                            <h4 class="text-xl font-semibold mb-4 teal-text">2-3. 학습 모델 및 응용</h4>

                            <div class="themeable-text-secondary space-y-3 mb-4">
                                <p>
                                    주요 모델은 <strong>YOLO v5</strong> (객체 탐지), <strong>DeepLabV3+</strong> (세그멘테이션),
                                    <strong>RRT*</strong> (경로 계획)입니다.
                                </p>
                                <p>
                                    이 데이터셋은 <strong>라스트마일 배송</strong>, <strong>캠퍼스 셔틀 로봇</strong>,
                                    <strong>야외 순찰 로봇</strong> 등에 활용됩니다.
                                </p>
                            </div>

                            <details class="mt-4">
                                <summary>QA 예제 보기 (4쌍)</summary>
                                <div class="mt-4">
                                    <table>
                                        <thead>
                                            <tr>
                                                <th>유형</th>
                                                <th>질문</th>
                                                <th>답변</th>
                                            </tr>
                                        </thead>
                                        <tbody>
                                            <tr>
                                                <td>단순 정보 추출형</td>
                                                <td>객체 탐지에 사용된 모델은?</td>
                                                <td>YOLO v5입니다.</td>
                                            </tr>
                                            <tr>
                                                <td>요약 및 설명형</td>
                                                <td>RRT*의 역할은 무엇인가요?</td>
                                                <td>장애물을 회피하는 최적 경로를 계획합니다.</td>
                                            </tr>
                                            <tr>
                                                <td>비교 및 분석형</td>
                                                <td>YOLO v5와 DeepLabV3+의 활용 차이는?</td>
                                                <td>YOLO는 객체 위치를, DeepLab은 주행 가능 영역을 세그멘테이션합니다.</td>
                                            </tr>
                                            <tr>
                                                <td>추론 및 적용형</td>
                                                <td>이 데이터셋이 도시 배송 인프라에 어떻게 기여하나요?</td>
                                                <td>비도로 환경에서 안전하게 운행하는 배송로봇을 통해 교통 혼잡을 줄이고 배송 비용을 절감합니다.</td>
                                            </tr>
                                        </tbody>
                                    </table>
                                </div>
                            </details>
                        </div>
                    </div>

                    <!-- Dataset 3: 로봇 관점 주행 영상 데이터셋 -->
                    <div id="dataset-3" class="mb-8">
                        <h3 class="text-2xl font-semibold themeable-heading mb-4 flex items-center">
                            <span class="teal-text mr-2">3.</span>
                            로봇 관점 주행 영상 데이터셋
                        </h3>

                        <!-- 3-1: 개요 및 구성 -->
                        <div class="themeable-bg card-hover rounded-lg p-6 mb-6">
                            <h4 class="text-xl font-semibold mb-4 teal-text">3-1. 개요 및 구성</h4>

                            <div class="themeable-text-secondary space-y-3 mb-4">
                                <p>
                                    <strong>로봇 관점 주행 영상 데이터셋</strong>은 로봇의 시야에서 촬영한
                                    <strong>30,000개의 주행 영상</strong>(총 500시간)으로 구성됩니다.
                                </p>
                                <p>
                                    각 영상은 <strong>RGB 영상, IMU 데이터, GPS 좌표, LiDAR Point Cloud</strong>를 포함하여
                                    멀티모달 학습을 지원합니다.
                                </p>
                            </div>

                            <details class="mt-4">
                                <summary>QA 예제 보기 (4쌍)</summary>
                                <div class="mt-4">
                                    <table>
                                        <thead>
                                            <tr>
                                                <th>유형</th>
                                                <th>질문</th>
                                                <th>답변</th>
                                            </tr>
                                        </thead>
                                        <tbody>
                                            <tr>
                                                <td>단순 정보 추출형</td>
                                                <td>데이터셋의 총 영상 수는?</td>
                                                <td>30,000개입니다.</td>
                                            </tr>
                                            <tr>
                                                <td>요약 및 설명형</td>
                                                <td>영상에 포함된 센서 데이터는 무엇인가요?</td>
                                                <td>RGB 영상, IMU 데이터, GPS 좌표, LiDAR Point Cloud입니다.</td>
                                            </tr>
                                            <tr>
                                                <td>비교 및 분석형</td>
                                                <td>IMU와 GPS의 역할 차이는?</td>
                                                <td>IMU는 로봇의 자세와 가속도를, GPS는 절대 위치를 제공합니다.</td>
                                            </tr>
                                            <tr>
                                                <td>추론 및 적용형</td>
                                                <td>멀티모달 데이터가 자율주행 성능에 미치는 영향은?</td>
                                                <td>영상, IMU, LiDAR를 융합하면 다양한 환경에서 강건한 주행 성능을 달성할 수 있습니다.</td>
                                            </tr>
                                        </tbody>
                                    </table>
                                </div>
                            </details>
                        </div>

                        <!-- 3-2: 수집 환경 및 라벨링 -->
                        <div class="themeable-bg card-hover rounded-lg p-6 mb-6">
                            <h4 class="text-xl font-semibold mb-4 teal-text">3-2. 수집 환경 및 라벨링</h4>

                            <div class="themeable-text-secondary space-y-3 mb-4">
                                <p>
                                    데이터는 <strong>도심, 주거지, 산업단지</strong> 등 다양한 환경에서 수집되었으며,
                                    <strong>직진, 좌회전, 우회전, 유턴</strong> 등의 주행 패턴을 포함합니다.
                                </p>
                                <p>
                                    라벨링은 <strong>차선, 신호등, 표지판, 보행자</strong>에 대해 Bounding Box와 Semantic Class로 수행되었습니다.
                                </p>
                            </div>

                            <details class="mt-4">
                                <summary>QA 예제 보기 (4쌍)</summary>
                                <div class="mt-4">
                                    <table>
                                        <thead>
                                            <tr>
                                                <th>유형</th>
                                                <th>질문</th>
                                                <th>답변</th>
                                            </tr>
                                        </thead>
                                        <tbody>
                                            <tr>
                                                <td>단순 정보 추출형</td>
                                                <td>수집 환경의 종류는?</td>
                                                <td>도심, 주거지, 산업단지입니다.</td>
                                            </tr>
                                            <tr>
                                                <td>요약 및 설명형</td>
                                                <td>주요 주행 패턴을 나열해 주세요.</td>
                                                <td>직진, 좌회전, 우회전, 유턴입니다.</td>
                                            </tr>
                                            <tr>
                                                <td>비교 및 분석형</td>
                                                <td>도심과 산업단지 환경의 차이는?</td>
                                                <td>도심은 보행자와 신호등이 많고, 산업단지는 대형 차량과 넓은 도로가 특징입니다.</td>
                                            </tr>
                                            <tr>
                                                <td>추론 및 적용형</td>
                                                <td>다양한 주행 패턴 데이터가 왜 중요한가요?</td>
                                                <td>복잡한 교차로와 회전 구간에서 안전하게 운행하는 능력을 학습할 수 있습니다.</td>
                                            </tr>
                                        </tbody>
                                    </table>
                                </div>
                            </details>
                        </div>
                    </div>

                    <!-- Dataset 4: 실내공간 유지관리 서비스 로봇 데이터셋 -->
                    <div id="dataset-4" class="mb-8">
                        <h3 class="text-2xl font-semibold themeable-heading mb-4 flex items-center">
                            <span class="teal-text mr-2">4.</span>
                            실내공간 유지관리 서비스 로봇 데이터셋
                        </h3>

                        <!-- 4-1: 개요 및 활용 -->
                        <div class="themeable-bg card-hover rounded-lg p-6 mb-6">
                            <h4 class="text-xl font-semibold mb-4 teal-text">4-1. 개요 및 활용</h4>

                            <div class="themeable-text-secondary space-y-3 mb-4">
                                <p>
                                    <strong>실내공간 유지관리 서비스 로봇 데이터셋</strong>은 사무실, 병원, 공항 등의 실내 환경에서
                                    청소, 배송, 안내 등의 서비스를 수행하는 로봇을 위해 설계되었습니다.
                                </p>
                                <p>
                                    데이터셋은 <strong>20,000개의 실내 맵</strong>과 <strong>50,000장의 이미지</strong>로 구성되며,
                                    <strong>SLAM 맵, 장애물 정보, 사람 위치</strong>를 포함합니다.
                                </p>
                            </div>

                            <details class="mt-4">
                                <summary>QA 예제 보기 (4쌍)</summary>
                                <div class="mt-4">
                                    <table>
                                        <thead>
                                            <tr>
                                                <th>유형</th>
                                                <th>질문</th>
                                                <th>답변</th>
                                            </tr>
                                        </thead>
                                        <tbody>
                                            <tr>
                                                <td>단순 정보 추출형</td>
                                                <td>실내 맵의 총 개수는?</td>
                                                <td>20,000개입니다.</td>
                                            </tr>
                                            <tr>
                                                <td>요약 및 설명형</td>
                                                <td>데이터셋의 주요 활용 분야는?</td>
                                                <td>청소, 배송, 안내 등의 실내 서비스 로봇에 활용됩니다.</td>
                                            </tr>
                                            <tr>
                                                <td>비교 및 분석형</td>
                                                <td>사무실과 병원 환경의 차이는?</td>
                                                <td>병원은 의료 장비와 엄격한 청결 기준이 있고, 사무실은 책상과 회의실이 특징입니다.</td>
                                            </tr>
                                            <tr>
                                                <td>추론 및 적용형</td>
                                                <td>SLAM 맵이 실내 로봇에 중요한 이유는?</td>
                                                <td>실시간으로 환경을 매핑하고 위치를 추정하여 자율적으로 경로를 계획할 수 있습니다.</td>
                                            </tr>
                                        </tbody>
                                    </table>
                                </div>
                            </details>
                        </div>
                    </div>

                    <!-- Dataset 5: 로봇 핸드용 객체 특성 식별 데이터셋 -->
                    <div id="dataset-5" class="mb-8">
                        <h3 class="text-2xl font-semibold themeable-heading mb-4 flex items-center">
                            <span class="teal-text mr-2">5.</span>
                            로봇 핸드용 객체 특성 식별 데이터셋
                        </h3>

                        <!-- 5-1: 개요 및 구성 -->
                        <div class="themeable-bg card-hover rounded-lg p-6 mb-6">
                            <h4 class="text-xl font-semibold mb-4 teal-text">5-1. 개요 및 구성</h4>

                            <div class="themeable-text-secondary space-y-3 mb-4">
                                <p>
                                    <strong>로봇 핸드용 객체 특성 식별 데이터셋</strong>은 로봇이 물체의
                                    <strong>질감, 무게, 온도, 강성</strong> 등의 물리적 특성을 촉각 센서로 인식하도록 설계되었습니다.
                                </p>
                                <p>
                                    데이터셋은 <strong>5,000개의 객체</strong>와 <strong>100,000회의 파지 시도</strong>로 구성되며,
                                    각 객체에 대해 <strong>RGB 이미지, 촉각 센서 데이터, 무게, 재질</strong> 정보를 포함합니다.
                                </p>
                            </div>

                            <details class="mt-4">
                                <summary>QA 예제 보기 (4쌍)</summary>
                                <div class="mt-4">
                                    <table>
                                        <thead>
                                            <tr>
                                                <th>유형</th>
                                                <th>질문</th>
                                                <th>답변</th>
                                            </tr>
                                        </thead>
                                        <tbody>
                                            <tr>
                                                <td>단순 정보 추출형</td>
                                                <td>데이터셋의 총 객체 수는?</td>
                                                <td>5,000개입니다.</td>
                                            </tr>
                                            <tr>
                                                <td>요약 및 설명형</td>
                                                <td>측정되는 물리적 특성은 무엇인가요?</td>
                                                <td>질감, 무게, 온도, 강성입니다.</td>
                                            </tr>
                                            <tr>
                                                <td>비교 및 분석형</td>
                                                <td>촉각 센서와 RGB 카메라의 역할 차이는?</td>
                                                <td>촉각 센서는 물리적 특성을, RGB 카메라는 시각적 특징을 측정합니다.</td>
                                            </tr>
                                            <tr>
                                                <td>추론 및 적용형</td>
                                                <td>이 데이터셋이 로봇 피킹에 어떻게 기여하나요?</td>
                                                <td>물체의 재질과 무게를 인식하여 적절한 파지력을 적용하고, 파손을 방지합니다.</td>
                                            </tr>
                                        </tbody>
                                    </table>
                                </div>
                            </details>
                        </div>

                        <!-- 5-2: 학습 모델 및 응용 -->
                        <div class="themeable-bg card-hover rounded-lg p-6 mb-6">
                            <h4 class="text-xl font-semibold mb-4 teal-text">5-2. 학습 모델 및 응용</h4>

                            <div class="themeable-text-secondary space-y-3 mb-4">
                                <p>
                                    주요 모델은 <strong>CNN</strong> (촉각 이미지 분류), <strong>RNN</strong> (시계열 촉각 데이터 분석),
                                    <strong>Multi-Task Learning</strong> (재질, 무게, 온도 동시 예측)입니다.
                                </p>
                                <p>
                                    이 데이터셋은 <strong>식품 포장</strong>, <strong>전자제품 조립</strong>, <strong>의료 수술 로봇</strong> 등에 활용됩니다.
                                </p>
                            </div>

                            <details class="mt-4">
                                <summary>QA 예제 보기 (4쌍)</summary>
                                <div class="mt-4">
                                    <table>
                                        <thead>
                                            <tr>
                                                <th>유형</th>
                                                <th>질문</th>
                                                <th>답변</th>
                                            </tr>
                                        </thead>
                                        <tbody>
                                            <tr>
                                                <td>단순 정보 추출형</td>
                                                <td>촉각 이미지 분류에 사용된 모델은?</td>
                                                <td>CNN입니다.</td>
                                            </tr>
                                            <tr>
                                                <td>요약 및 설명형</td>
                                                <td>Multi-Task Learning의 장점은?</td>
                                                <td>재질, 무게, 온도를 동시에 예측하여 효율성을 높입니다.</td>
                                            </tr>
                                            <tr>
                                                <td>비교 및 분석형</td>
                                                <td>CNN과 RNN의 활용 차이는?</td>
                                                <td>CNN은 공간적 특징을, RNN은 시간적 패턴을 학습합니다.</td>
                                            </tr>
                                            <tr>
                                                <td>추론 및 적용형</td>
                                                <td>의료 수술 로봇에 이 데이터셋이 왜 중요한가요?</td>
                                                <td>조직의 강성을 감지하여 적절한 압력을 적용하고, 손상을 최소화합니다.</td>
                                            </tr>
                                        </tbody>
                                    </table>
                                </div>
                            </details>
                        </div>
                    </div>

                    <!-- Dataset 6: 손·팔 협조 파지-조작 동작 데이터셋 -->
                    <div id="dataset-6" class="mb-8">
                        <h3 class="text-2xl font-semibold themeable-heading mb-4 flex items-center">
                            <span class="teal-text mr-2">6.</span>
                            손·팔 협조 파지-조작 동작 데이터셋
                        </h3>

                        <!-- 6-1: 개요 및 활용 -->
                        <div class="themeable-bg card-hover rounded-lg p-6 mb-6">
                            <h4 class="text-xl font-semibold mb-4 teal-text">6-1. 개요 및 활용</h4>

                            <div class="themeable-text-secondary space-y-3 mb-4">
                                <p>
                                    <strong>손·팔 협조 파지-조작 동작 데이터셋</strong>은 로봇이 물체를 정밀하게 파지하고 조작하는
                                    동작을 학습하도록 설계되었습니다. 총 <strong>10,000개의 동작 시퀀스</strong>로 구성됩니다.
                                </p>
                                <p>
                                    각 시퀀스는 <strong>Joint Angle, End-Effector Pose, Grasp Force</strong> 데이터를 포함하며,
                                    <strong>집기, 들기, 회전, 놓기</strong> 등의 동작을 커버합니다.
                                </p>
                            </div>

                            <details class="mt-4">
                                <summary>QA 예제 보기 (4쌍)</summary>
                                <div class="mt-4">
                                    <table>
                                        <thead>
                                            <tr>
                                                <th>유형</th>
                                                <th>질문</th>
                                                <th>답변</th>
                                            </tr>
                                        </thead>
                                        <tbody>
                                            <tr>
                                                <td>단순 정보 추출형</td>
                                                <td>동작 시퀀스의 총 개수는?</td>
                                                <td>10,000개입니다.</td>
                                            </tr>
                                            <tr>
                                                <td>요약 및 설명형</td>
                                                <td>주요 동작 유형은 무엇인가요?</td>
                                                <td>집기, 들기, 회전, 놓기입니다.</td>
                                            </tr>
                                            <tr>
                                                <td>비교 및 분석형</td>
                                                <td>Joint Angle과 End-Effector Pose의 차이는?</td>
                                                <td>Joint Angle은 관절 각도를, End-Effector Pose는 손의 위치와 방향을 나타냅니다.</td>
                                            </tr>
                                            <tr>
                                                <td>추론 및 적용형</td>
                                                <td>이 데이터셋이 조립 로봇에 어떻게 활용되나요?</td>
                                                <td>부품을 정밀하게 파지하고 조립 위치에 정확히 배치하는 동작을 학습합니다.</td>
                                            </tr>
                                        </tbody>
                                    </table>
                                </div>
                            </details>
                        </div>
                    </div>

                    <!-- Dataset 7: 사람 행동 인식 로봇 자율 행동 데이터셋 -->
                    <div id="dataset-7" class="mb-8">
                        <h3 class="text-2xl font-semibold themeable-heading mb-4 flex items-center">
                            <span class="teal-text mr-2">7.</span>
                            사람 행동 인식 로봇 자율 행동 데이터셋
                        </h3>

                        <!-- 7-1: 개요 및 활용 -->
                        <div class="themeable-bg card-hover rounded-lg p-6 mb-6">
                            <h4 class="text-xl font-semibold mb-4 teal-text">7-1. 개요 및 활용</h4>

                            <div class="themeable-text-secondary space-y-3 mb-4">
                                <p>
                                    <strong>사람 행동 인식 로봇 자율 행동 데이터셋</strong>은 로봇이 사람의 행동을 인식하고
                                    적절한 서비스를 제공하도록 설계되었습니다. 총 <strong>15,000개의 행동 영상</strong>으로 구성됩니다.
                                </p>
                                <p>
                                    행동 클래스는 <strong>걷기, 앉기, 손 흔들기, 물건 집기</strong> 등 20가지로 구성되며,
                                    각 영상은 <strong>RGB, Depth, Skeleton Keypoint</strong> 데이터를 포함합니다.
                                </p>
                            </div>

                            <details class="mt-4">
                                <summary>QA 예제 보기 (4쌍)</summary>
                                <div class="mt-4">
                                    <table>
                                        <thead>
                                            <tr>
                                                <th>유형</th>
                                                <th>질문</th>
                                                <th>답변</th>
                                            </tr>
                                        </thead>
                                        <tbody>
                                            <tr>
                                                <td>단순 정보 추출형</td>
                                                <td>행동 영상의 총 개수는?</td>
                                                <td>15,000개입니다.</td>
                                            </tr>
                                            <tr>
                                                <td>요약 및 설명형</td>
                                                <td>주요 행동 클래스는 무엇인가요?</td>
                                                <td>걷기, 앉기, 손 흔들기, 물건 집기 등 20가지입니다.</td>
                                            </tr>
                                            <tr>
                                                <td>비교 및 분석형</td>
                                                <td>RGB와 Skeleton Keypoint의 역할 차이는?</td>
                                                <td>RGB는 외형을, Skeleton은 관절 위치를 제공하여 행동 인식 성능을 높입니다.</td>
                                            </tr>
                                            <tr>
                                                <td>추론 및 적용형</td>
                                                <td>이 데이터셋이 안내 로봇에 어떻게 활용되나요?</td>
                                                <td>사람의 손짓을 인식하여 방향을 안내하거나 도움이 필요한 사람을 감지합니다.</td>
                                            </tr>
                                        </tbody>
                                    </table>
                                </div>
                            </details>
                        </div>
                    </div>

                </section>

                <!-- Section 3: 결론 -->
                <section id="conclusion" class="mb-12">
                    <h2 class="text-3xl font-bold themeable-heading mb-6 pb-3 border-b-2 themeable-border">
                        Ⅲ. 결론
                    </h2>

                    <div class="themeable-text-secondary space-y-4 mb-6">
                        <p>
                            본 보고서에서는 <strong class="orange-text">AADS 과제</strong>를 통해 구축한
                            <strong class="teal-text">로봇 지능 분야의 52쌍 QA 데이터셋</strong>을 소개했습니다.
                            13개 데이터셋은 <strong>가려진 객체 추론, 배송로봇 비도로 운행, 주행영상, 실내공간 유지관리,
                            객체 특성 식별, 파지-조작 동작, 사람 행동 인식</strong> 등 로봇 지능의 핵심 영역을 포괄합니다.
                        </p>
                        <p>
                            각 데이터셋은 <strong>도메인 정의, 데이터 구조, AI 모델, 품질 관리</strong>의 4가지 측면에서 체계적으로 분석되었으며,
                            <strong>단순 정보 추출형, 요약 및 설명형, 비교 및 분석형, 추론 및 적용형</strong>의 4가지 QA 유형으로 구성되어
                            LLM의 <strong class="teal-text">Physical AI 도메인 전문성</strong>을 강화합니다.
                        </p>
                        <p>
                            페블러스는 이 데이터셋을 통해 <strong class="orange-text">Few-Shot Learning</strong>과
                            <strong class="orange-text">프롬프트 엔지니어링</strong>을 적용하여,
                            실무 환경에서 LLM이 로봇 데이터 분석, 모델 선택, 품질 관리 등의 의사결정을 지원하도록 설계했습니다.
                        </p>
                        <p>
                            향후 AADS는 <strong>DataClinic</strong> 플랫폼과 연계하여,
                            로봇 지능 데이터셋의 자동 품질 진단 및 개선 권장사항을 제공하고,
                            도메인 전문가와 AI 모델 간의 협업을 강화할 계획입니다.
                        </p>
                    </div>
                </section>

            </main>
        </div>
    </div>

    <!-- Footer will be loaded by common-utils.js -->
    <div id="footer-placeholder"></div>

    <!-- Scripts -->
    <script src="/scripts/common-utils.js"></script>
    <script>
    document.addEventListener('DOMContentLoaded', async function() {
        // 공유 버튼 이벤트 핸들러
        const currentUrl = window.location.href;
        const pageTitle = document.title;

        document.getElementById('copy-url-btn').addEventListener('click', function() {
            navigator.clipboard.writeText(currentUrl).then(() => {
                const btn = this;
                const originalHTML = btn.innerHTML;
                btn.innerHTML = '<svg fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 13l4 4L19 7"></path></svg><span>복사됨!</span>';
                btn.classList.add('copied');
                setTimeout(() => {
                    btn.innerHTML = originalHTML;
                    btn.classList.remove('copied');
                }, 2000);
            });
        });

        document.getElementById('share-twitter-btn').addEventListener('click', function() {
            window.open(`https://twitter.com/intent/tweet?url=${encodeURIComponent(currentUrl)}&text=${encodeURIComponent(pageTitle)}`, '_blank');
        });

        document.getElementById('share-facebook-btn').addEventListener('click', function() {
            window.open(`https://www.facebook.com/sharer/sharer.php?u=${encodeURIComponent(currentUrl)}`, '_blank');
        });

        document.getElementById('share-linkedin-btn').addEventListener('click', function() {
            window.open(`https://www.linkedin.com/sharing/share-offsite/?url=${encodeURIComponent(currentUrl)}`, '_blank');
        });

        // PebblousPage initialization with config
        const config = {
            mainTitle: "로봇 분야 LLM 파인튜닝용 QA 데이터셋 구축",
            subtitle: "AADS의 피지컬 AI 접근법",
            pageTitle: "로봇 분야 LLM 파인튜닝용 QA 데이터셋 구축: AADS의 피지컬 AI 접근법 | 페블러스",
            publishDate: "2025년 11월 30일",
            publisher: "페블러스 데이터 커뮤니케이션 팀",
            defaultTheme: "light",
            category: "tech",
            articlePath: "project/AADS/robot-qa-dataset.html",
            tags: [
                "LLM 파인튜닝", "LLM Fine-tuning", "QA 데이터셋", "Question-Answer Dataset",
                "로봇 분야", "Robotics AI", "로봇 데이터", "Robot Data", "AADS", "Agentic AI Data Scientist",
                "피지컬 AI", "Physical AI", "데이터 품질", "Data Quality",
                "데이터 중심 AI", "Data-Centric AI", "멀티모달 데이터", "Multimodal Data",
                "도메인 지식", "Domain Knowledge", "가려진 객체 추론", "Occluded Object Detection",
                "배송로봇", "Delivery Robot", "비도로 운행", "Off-Road Navigation",
                "주행영상", "Driving Video", "실내공간 유지관리", "Indoor Maintenance",
                "서비스 로봇", "Service Robot", "객체 특성 식별", "Object Property Recognition",
                "로봇 핸드", "Robot Hand", "파지-조작 동작", "Grasp-Manipulation",
                "손·팔 협조", "Hand-Arm Coordination", "사람 행동 인식", "Human Activity Recognition",
                "로봇 자율 행동", "Robot Autonomous Behavior", "Few-Shot Learning", "퓨샷 러닝",
                "프롬프트 엔지니어링", "Prompt Engineering", "라벨링", "Labeling",
                "데이터 검수", "Data Validation", "mAP", "F1-score", "mIoU",
                "페블러스", "Pebblous", "DataClinic", "데이터클리닉"
            ],
            faqs: [
                {
                    question: "AADS가 로봇 분야 QA 데이터셋을 어떻게 구축하나요?",
                    answer: "AADS는 AI 허브의 로봇 지능 데이터셋 문서를 분석하여, 13개의 '논리적 데이터 그룹'으로 묶고, 각 그룹별로 도메인 정의, 데이터 구조, AI 모델, 품질 관리의 4가지 유형에서 QA 쌍을 생성하여 총 52개의 고품질 질의응답을 구축했습니다."
                },
                {
                    question: "로봇 데이터셋에서 멀티모달 데이터가 왜 중요한가요?",
                    answer: "로봇은 RGB 카메라, Depth 센서, LiDAR, IMU, GPS 등 다양한 센서를 통합하여 환경을 이해합니다. 멀티모달 데이터는 각 센서의 장점을 융합하여 강건한 인식 성능과 안전한 의사결정을 가능하게 합니다. 예를 들어, RGB는 색상을, Depth는 거리를, LiDAR는 3D 형상을 제공합니다."
                },
                {
                    question: "가려진 객체 추론 데이터셋의 실무 활용 사례는?",
                    answer: "물류 창고에서 로봇이 상자에 가려진 제품의 위치와 형상을 추론하여 효율적인 피킹 순서를 결정하거나, 자율주행 로봇이 부분적으로 보이는 장애물의 전체 형상을 예측하여 안전한 경로를 계획하는 데 활용됩니다."
                },
                {
                    question: "배송로봇 비도로 운행 데이터가 라스트마일 배송에 어떻게 기여하나요?",
                    answer: "보도, 공원, 주차장 등 다양한 비도로 환경에서 수집된 50,000장의 이미지와 날씨 조건(맑음, 비, 눈) 데이터를 학습하여, 배송로봇이 교통 혼잡 없이 안전하게 운행하며 배송 시간을 단축하고 비용을 절감합니다."
                },
                {
                    question: "로봇 핸드용 객체 특성 식별 데이터의 핵심 가치는?",
                    answer: "촉각 센서로 물체의 질감, 무게, 온도, 강성을 측정한 100,000회의 파지 데이터는 로봇이 적절한 파지력을 적용하여 파손을 방지하고, 식품 포장, 전자제품 조립, 의료 수술 등에서 정밀한 조작 능력을 발휘하도록 합니다."
                },
                {
                    question: "사람 행동 인식 데이터셋이 서비스 로봇에 어떻게 활용되나요?",
                    answer: "15,000개의 행동 영상(걷기, 앉기, 손 흔들기 등 20가지 클래스)과 Skeleton Keypoint 데이터를 통해, 안내 로봇이 사람의 손짓을 인식하여 방향을 안내하거나, 노약자의 낙상을 감지하여 도움을 제공하는 등 상황 인식 서비스를 구현합니다."
                },
                {
                    question: "AADS QA 데이터셋이 DataClinic과 어떻게 연계되나요?",
                    answer: "AADS의 로봇 QA 데이터셋은 DataClinic 플랫폼과 연계하여, 로봇 데이터의 자동 품질 진단(라벨 일관성, 센서 캘리브레이션, 시나리오 균형)과 개선 권장사항을 제공하며, 도메인 전문가와 AI 모델 간 협업을 통해 Physical AI의 실무 적용을 가속화합니다."
                }
            ]
        };

        await PebblousPage.init(config);
    });
    </script>
</body>
</html>
